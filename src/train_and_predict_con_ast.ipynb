{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from data_preprocessing import Get_and_process_data\n",
    "from datasets import Dataset, ClassLabel, Sequence, load_dataset, load_metric\n",
    "from transformers import (AutoModelForTokenClassification, \n",
    "                          AutoTokenizer, \n",
    "                          DataCollatorForTokenClassification,\n",
    "                          pipeline,\n",
    "                          TrainingArguments, \n",
    "                          Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"allenai/scibert_scivocab_uncased\"\n",
    "# model_checkpoint = \"giacomomiolo/electramed_base_scivocab_1M\"\n",
    "batch_size = 64\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading raw text: 100%|██████████| 170/170 [00:05<00:00, 31.92it/s]\n",
      "Processing raw text: 170it [00:00, 208.56it/s]\n",
      "Adding unlabeled lines: 8454it [00:00, 10016.73it/s]\n",
      "Formatting dataset: 16525it [00:02, 6059.84it/s]\n",
      "Using custom data configuration default-958a72c76e9cc4be\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/ubuntu/.cache/huggingface/datasets/json/default-958a72c76e9cc4be/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8534f5d65e4445eb953c51a954b69fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e0258847414b6f9437eefec5771a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/default-958a72c76e9cc4be/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773aef1a566f483188a9c225586e9f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca40112a602245799078df240f2bc572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15593 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608fc12516554d9cbcfcff703457c6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/821 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/json/default-958a72c76e9cc4be/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde/cache-97e9e0a61cbfadce.arrow\n",
      "Loading raw text for test: 100%|██████████| 128/128 [00:08<00:00, 14.79it/s]\n",
      "Formatting test data: 14146it [00:00, 20424.61it/s]\n",
      "Using custom data configuration default-c62dbd7f379fcfdc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/ubuntu/.cache/huggingface/datasets/json/default-c62dbd7f379fcfdc/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f68c4b83c3474eafaa45f38b2a828c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0820299c5c934570a270ba654739e36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/default-c62dbd7f379fcfdc/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d91de4f15e48eb9f1be3b6c90b89e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca81fd1de07d4b94bc3e2bad9e19f986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14146 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = Get_and_process_data(tokenizer, train_split=0.95, add_unlabeled=True)\n",
    "D = data_loader.get_dataset()\n",
    "label_list = data_loader.get_label_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForTokenClassification.from_pretrained(\"logs/model_60_warmup_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"training_logs/{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    # learning_rate=1e-5,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.05,\n",
    "    logging_steps=5,\n",
    "\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=D[\"train\"],\n",
    "    eval_dataset=D[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running training *****\n",
      "  Num examples = 15593\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4880\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4880' max='4880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4880/4880 20:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.518400</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.577297</td>\n",
       "      <td>0.581066</td>\n",
       "      <td>0.579176</td>\n",
       "      <td>0.881732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.260600</td>\n",
       "      <td>0.192800</td>\n",
       "      <td>0.701736</td>\n",
       "      <td>0.747552</td>\n",
       "      <td>0.723920</td>\n",
       "      <td>0.939671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.152530</td>\n",
       "      <td>0.769866</td>\n",
       "      <td>0.811752</td>\n",
       "      <td>0.790254</td>\n",
       "      <td>0.953509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.148893</td>\n",
       "      <td>0.793031</td>\n",
       "      <td>0.817193</td>\n",
       "      <td>0.804930</td>\n",
       "      <td>0.953808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.138764</td>\n",
       "      <td>0.825532</td>\n",
       "      <td>0.844396</td>\n",
       "      <td>0.834857</td>\n",
       "      <td>0.959881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.142105</td>\n",
       "      <td>0.827623</td>\n",
       "      <td>0.841132</td>\n",
       "      <td>0.834323</td>\n",
       "      <td>0.960279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.157457</td>\n",
       "      <td>0.825212</td>\n",
       "      <td>0.847661</td>\n",
       "      <td>0.836286</td>\n",
       "      <td>0.960876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.160283</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.850925</td>\n",
       "      <td>0.839957</td>\n",
       "      <td>0.960677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.171127</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.850925</td>\n",
       "      <td>0.839957</td>\n",
       "      <td>0.961872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.172824</td>\n",
       "      <td>0.841151</td>\n",
       "      <td>0.858542</td>\n",
       "      <td>0.849758</td>\n",
       "      <td>0.962867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.179362</td>\n",
       "      <td>0.831746</td>\n",
       "      <td>0.855277</td>\n",
       "      <td>0.843348</td>\n",
       "      <td>0.960577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.186725</td>\n",
       "      <td>0.839703</td>\n",
       "      <td>0.860718</td>\n",
       "      <td>0.850081</td>\n",
       "      <td>0.964161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.196036</td>\n",
       "      <td>0.840212</td>\n",
       "      <td>0.863983</td>\n",
       "      <td>0.851931</td>\n",
       "      <td>0.963763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.196721</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.861806</td>\n",
       "      <td>0.849330</td>\n",
       "      <td>0.962867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.210368</td>\n",
       "      <td>0.838919</td>\n",
       "      <td>0.844396</td>\n",
       "      <td>0.841649</td>\n",
       "      <td>0.959881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.215088</td>\n",
       "      <td>0.848812</td>\n",
       "      <td>0.855277</td>\n",
       "      <td>0.852033</td>\n",
       "      <td>0.962369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.214045</td>\n",
       "      <td>0.845905</td>\n",
       "      <td>0.854189</td>\n",
       "      <td>0.850027</td>\n",
       "      <td>0.960777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.211884</td>\n",
       "      <td>0.851133</td>\n",
       "      <td>0.858542</td>\n",
       "      <td>0.854821</td>\n",
       "      <td>0.961175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.215246</td>\n",
       "      <td>0.847476</td>\n",
       "      <td>0.858542</td>\n",
       "      <td>0.852973</td>\n",
       "      <td>0.961274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.214478</td>\n",
       "      <td>0.846237</td>\n",
       "      <td>0.856366</td>\n",
       "      <td>0.851271</td>\n",
       "      <td>0.961175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4880, training_loss=0.12683728897531868, metrics={'train_runtime': 1215.6668, 'train_samples_per_second': 256.534, 'train_steps_per_second': 4.014, 'total_flos': 9950340615467940.0, 'train_loss': 0.12683728897531868, 'epoch': 20.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model = AutoModelForTokenClassification.from_pretrained(\"./training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2500\", local_files_only=True)\n",
    "# trainer.model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performence on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: treatment, conditional_indices_end, associated_with_someone_else, absent, possible_indices_start, filename, hypothetical_indices_start, present, test_indices_start, associated_with_someone_else_indices_start, offset_mapping, treatment_indices_start, absent_indices_end, hypothetical_indices_end, treatment_indices_end, associated_with_someone_else_indices_end, possible, row, hypothetical, present_indices_start, present_indices_end, test_indices_end, test, absent_indices_start, conditional, text, conditional_indices_start, possible_indices_end.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='235' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ABSENT': {'precision': 0.9452054794520548,\n",
       "  'recall': 0.9078947368421053,\n",
       "  'f1': 0.9261744966442953,\n",
       "  'number': 76},\n",
       " 'CONDITIONAL': {'precision': 0.6666666666666666,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.8,\n",
       "  'number': 2},\n",
       " 'HYPOTHETICAL': {'precision': 1.0,\n",
       "  'recall': 0.8181818181818182,\n",
       "  'f1': 0.9,\n",
       "  'number': 11},\n",
       " 'POSSIBLE': {'precision': 0.6666666666666666,\n",
       "  'recall': 0.4444444444444444,\n",
       "  'f1': 0.5333333333333333,\n",
       "  'number': 18},\n",
       " 'PRESENT': {'precision': 0.7803030303030303,\n",
       "  'recall': 0.8408163265306122,\n",
       "  'f1': 0.8094302554027505,\n",
       "  'number': 245},\n",
       " 'TEST': {'precision': 0.9066666666666666,\n",
       "  'recall': 0.85,\n",
       "  'f1': 0.8774193548387097,\n",
       "  'number': 320},\n",
       " 'TREATMENT': {'precision': 0.8215613382899628,\n",
       "  'recall': 0.8947368421052632,\n",
       "  'f1': 0.8565891472868217,\n",
       "  'number': 247},\n",
       " 'overall_precision': 0.8462365591397849,\n",
       " 'overall_recall': 0.8563656147986942,\n",
       " 'overall_f1': 0.8512709572742022,\n",
       " 'overall_accuracy': 0.9611747137879542}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(D[\"val\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to logs/scibert_20_epochs_64_batch_95_train_split\n",
      "Configuration saved in logs/scibert_20_epochs_64_batch_95_train_split/config.json\n",
      "Model weights saved in logs/scibert_20_epochs_64_batch_95_train_split/pytorch_model.bin\n",
      "tokenizer config file saved in logs/scibert_20_epochs_64_batch_95_train_split/tokenizer_config.json\n",
      "Special tokens file saved in logs/scibert_20_epochs_64_batch_95_train_split/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"logs/scibert_20_epochs_64_batch_95_train_split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat to the initial format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, row, filename, text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 14146\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "# prediction = np.load(\"../data/prediction.npy\")\n",
    "# print(\"min :\", min(prediction.reshape(-1)))\n",
    "# print(\"max :\", max(prediction.reshape(-1)))\n",
    "# prediction = predictions_2\n",
    "test_data = D[\"test\"]\n",
    "prediction, _, _ = trainer.predict(D[\"test\"])\n",
    "prediction = np.argmax(prediction, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'o',\n",
       " 1: 'test',\n",
       " 2: 'test',\n",
       " 3: 'treatment',\n",
       " 4: 'treatment',\n",
       " 5: 'present',\n",
       " 6: 'present',\n",
       " 7: 'absent',\n",
       " 8: 'absent',\n",
       " 9: 'possible',\n",
       " 10: 'possible',\n",
       " 11: 'conditional',\n",
       " 12: 'conditional',\n",
       " 13: 'hypothetical',\n",
       " 14: 'hypothetical',\n",
       " 15: 'associated_with_someone_else',\n",
       " 16: 'associated_with_someone_else'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list\n",
    "token_to_label = {token: token.split(\"-\")[-1] for token in label_list}\n",
    "token_id_to_label = {i: token_to_label[token].lower() for i, token in enumerate(label_list)}\n",
    "token_id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_to_concept = {\n",
    "    \"test\" : \"test\",\n",
    "    \"treatment\" : \"treatment\",\n",
    "    \"present\" : \"problem\",\n",
    "    \"absent\" : \"problem\",\n",
    "    \"possible\" : \"problem\",\n",
    "    \"conditional\" : \"problem\",\n",
    "    \"hypothetical\" : \"problem\",\n",
    "    \"associated_with_someone_else\" : \"problem\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.save_predictions import save_predictions\n",
    "\n",
    "# save_predictions(test_data, prediction)\n",
    "save_predictions(test_data, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_ner_model = pipeline(task=\"ner\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_1',\n",
       "  'score': 0.99998295,\n",
       "  'index': 1,\n",
       "  'word': 'her',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.99998784,\n",
       "  'index': 2,\n",
       "  'word': 'coagulation',\n",
       "  'start': 4,\n",
       "  'end': 15},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9999907,\n",
       "  'index': 3,\n",
       "  'word': 'parameters',\n",
       "  'start': 16,\n",
       "  'end': 26},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9999912,\n",
       "  'index': 4,\n",
       "  'word': 'were',\n",
       "  'start': 27,\n",
       "  'end': 31},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.99999094,\n",
       "  'index': 5,\n",
       "  'word': 'normal',\n",
       "  'start': 32,\n",
       "  'end': 38},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9999906,\n",
       "  'index': 6,\n",
       "  'word': '.',\n",
       "  'start': 39,\n",
       "  'end': 40}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_ner_model(D[\"train\"][4][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "def visualize_entities(sentence):\n",
    "    tokens = effect_ner_model(sentence)\n",
    "    entities = []\n",
    "\n",
    "    for token in tokens:\n",
    "        label = int(token[\"entity\"][-1])\n",
    "        if label != 0:\n",
    "            token[\"label\"] = label_list[label]\n",
    "            entities.append(token)\n",
    "\n",
    "    params = [{\"text\": sentence, \"ents\": entities, \"title\": None}]\n",
    "\n",
    "    html = displacy.render(\n",
    "        params,\n",
    "        style=\"ent\",\n",
    "        manual=True,\n",
    "        jupyter=True,\n",
    "        options={\n",
    "            \"colors\": {\n",
    "                \"B-PROBLEM\": \"#f08080\",\n",
    "                \"I-PROBLEM\": \"#f08080\",\n",
    "                \"B-TEST\": \"#9bddff\",\n",
    "                \"I-TEST\": \"#9bddff\",\n",
    "                \"B-TREATMENT\": \"#ffdab9\",\n",
    "                \"I-TREATMENT\": \"#ffdab9\",\n",
    "            },\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">at osh , \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    sputum\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cultures\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    grew\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pseudomonas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    and\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    mrsa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    and\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    he\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    was\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    arte\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    d\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    on\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    zo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    syn\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    08\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    08\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    )\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    and\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    vancomycin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    08\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    09\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    )\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: at osh , sputum cultures grew pseudomonas and mrsa , and he was restarted on zosyn ( 08-08 ) and vancomycin ( 08-09 ).\n",
      "Tests: ['sputum cultures']\n",
      "Treatments: ['vancomycin', 'zosyn']\n",
      "Predent: ['mrsa', 'pseudomonas']\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">the liver was 11 cm by \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    perc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TEST</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ussion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: the liver was 11 cm by percussion .\n",
      "Tests: ['percussion']\n",
      "Treatments: []\n",
      "Predent: []\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    hepatitis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    35\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ago\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: hepatitis 35 years ago .\n",
      "Tests: []\n",
      "Treatments: []\n",
      "Predent: ['hepatitis']\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Fluid , electrolytes and nutritions :</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Fluid , electrolytes and nutritions :\n",
      "Tests: []\n",
      "Treatments: []\n",
      "Predent: []\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    top\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rol\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    xl\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    25\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    mg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    qd\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ay\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: toprol xl 25 mg qday\n",
      "Tests: []\n",
      "Treatments: ['toprol xl']\n",
      "Predent: []\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick 5 random sentences from the test set\n",
    "for i in range(5):\n",
    "    index = np.random.randint(0, len(D[\"train\"]))\n",
    "    visualize_entities(D[\"train\"][index][\"text\"])\n",
    "    print(f\"Text: {D['train'][index]['text']}\")\n",
    "    # print(f\"Problems: {D['D'][index]['problem']}\")\n",
    "    print(f\"Tests: {D['train'][index]['test']}\")\n",
    "    print(f\"Treatments: {D['train'][index]['treatment']}\")\n",
    "    print(f\"Predent: {D['train'][index]['present']}\")\n",
    "    print(f\"{'*' * 50}\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d650a3b94c458c273a536969f20832049d87778dafdd87df0ff4ffcada142abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
