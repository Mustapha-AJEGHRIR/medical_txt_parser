{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from data_preprocessing import Get_and_process_data\n",
    "from datasets import Dataset, ClassLabel, Sequence, load_dataset, load_metric\n",
    "from transformers import (AutoModelForTokenClassification, \n",
    "                          AutoTokenizer, \n",
    "                          DataCollatorForTokenClassification,\n",
    "                          pipeline,\n",
    "                          TrainingArguments, \n",
    "                          Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/ubuntu/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"allenai/scibert_scivocab_uncased\"\n",
    "# model_checkpoint = \"giacomomiolo/electramed_base_scivocab_1M\"\n",
    "batch_size = 64\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading raw text: 100%|██████████| 170/170 [00:05<00:00, 32.07it/s]\n",
      "Processing raw text: 170it [00:00, 205.25it/s]\n",
      "Adding unlabeled lines: 8454it [00:00, 9591.17it/s] \n",
      "Formatting dataset: 16525it [00:02, 5901.93it/s]\n",
      "Using custom data configuration default-72f20bdf7ea795cf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/ubuntu/.cache/huggingface/datasets/json/default-72f20bdf7ea795cf/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f6737256e04a1499286b77d58eb096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a9c9992f724fa2a11aeb7938bd7f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/default-72f20bdf7ea795cf/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5931c67f7c44e4ae6a0c649b1135d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab674e9682b405090b6da4391376d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16249 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33469dd512c94e40855fc4a934a804b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/json/default-72f20bdf7ea795cf/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde/cache-16858c9a1d555ced.arrow\n",
      "Loading raw text for test: 100%|██████████| 128/128 [00:08<00:00, 14.28it/s]\n",
      "Formatting test data: 14146it [00:00, 19796.17it/s]\n",
      "Using custom data configuration default-9f8c2ce736b7b09f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/ubuntu/.cache/huggingface/datasets/json/default-9f8c2ce736b7b09f/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2542186dd33346bf90848f051a073dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c5f18ec115412296a622ea75752b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/default-9f8c2ce736b7b09f/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7944eeb42643aea6df609a4e02a2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc7fc19950b4badad659c8eed2e5280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14146 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = Get_and_process_data(tokenizer, train_split=0.99, add_unlabeled=True)\n",
    "D = data_loader.get_dataset()\n",
    "label_list = data_loader.get_label_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForTokenClassification.from_pretrained(\"logs/model_60_warmup_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"training_logs/{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    # learning_rate=1e-5,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.05,\n",
    "    logging_steps=5,\n",
    "\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=D[\"train\"],\n",
    "    eval_dataset=D[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running training *****\n",
      "  Num examples = 16249\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5080' max='5080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5080/5080 20:53, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>0.434549</td>\n",
       "      <td>0.512953</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.530831</td>\n",
       "      <td>0.864234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.231900</td>\n",
       "      <td>0.226715</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.924574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.169400</td>\n",
       "      <td>0.182616</td>\n",
       "      <td>0.760638</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.777174</td>\n",
       "      <td>0.943066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.115300</td>\n",
       "      <td>0.184480</td>\n",
       "      <td>0.805405</td>\n",
       "      <td>0.827778</td>\n",
       "      <td>0.816438</td>\n",
       "      <td>0.947932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.181408</td>\n",
       "      <td>0.828877</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.844687</td>\n",
       "      <td>0.950852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.190531</td>\n",
       "      <td>0.798942</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.818428</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.194995</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.953771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.215880</td>\n",
       "      <td>0.836158</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.952311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.211023</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.952798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>0.230061</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.952311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.217276</td>\n",
       "      <td>0.872928</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.875346</td>\n",
       "      <td>0.952798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.226758</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.874652</td>\n",
       "      <td>0.955718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.228968</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.872928</td>\n",
       "      <td>0.952798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.243274</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.953771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.244537</td>\n",
       "      <td>0.861878</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.864266</td>\n",
       "      <td>0.953771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.249653</td>\n",
       "      <td>0.871508</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.869081</td>\n",
       "      <td>0.955231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.255837</td>\n",
       "      <td>0.887640</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.955231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.259469</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.880223</td>\n",
       "      <td>0.954258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.259584</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.880223</td>\n",
       "      <td>0.955231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.257470</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.880223</td>\n",
       "      <td>0.954745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5080, training_loss=0.11468148054775056, metrics={'train_runtime': 1254.063, 'train_samples_per_second': 259.142, 'train_steps_per_second': 4.051, 'total_flos': 1.0417602575221668e+16, 'train_loss': 0.11468148054775056, 'epoch': 20.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model = AutoModelForTokenClassification.from_pretrained(\"./training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2500\", local_files_only=True)\n",
    "# trainer.model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performence on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: hypothetical_indices_end, test_indices_end, absent_indices_end, hypothetical_indices_start, associated_with_someone_else_indices_end, text, filename, test_indices_start, present, associated_with_someone_else_indices_start, treatment_indices_end, treatment_indices_start, possible_indices_start, test, hypothetical, absent_indices_start, present_indices_start, absent, row, conditional_indices_start, associated_with_someone_else, treatment, present_indices_end, possible_indices_end, conditional, conditional_indices_end, possible, offset_mapping.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 165\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ABSENT': {'precision': 0.8571428571428571,\n",
       "  'recall': 0.9,\n",
       "  'f1': 0.8780487804878048,\n",
       "  'number': 20},\n",
       " 'CONDITIONAL': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'POSSIBLE': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'PRESENT': {'precision': 0.8679245283018868,\n",
       "  'recall': 0.7419354838709677,\n",
       "  'f1': 0.8,\n",
       "  'number': 62},\n",
       " 'TEST': {'precision': 0.9473684210526315,\n",
       "  'recall': 0.9642857142857143,\n",
       "  'f1': 0.9557522123893805,\n",
       "  'number': 56},\n",
       " 'TREATMENT': {'precision': 0.8222222222222222,\n",
       "  'recall': 0.9487179487179487,\n",
       "  'f1': 0.8809523809523809,\n",
       "  'number': 39},\n",
       " 'overall_precision': 0.88268156424581,\n",
       " 'overall_recall': 0.8777777777777778,\n",
       " 'overall_f1': 0.8802228412256267,\n",
       " 'overall_accuracy': 0.9547445255474453}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(D[\"val\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to logs/scibert_20_epochs_64_batch_99_train_split\n",
      "Configuration saved in logs/scibert_20_epochs_64_batch_99_train_split/config.json\n",
      "Model weights saved in logs/scibert_20_epochs_64_batch_99_train_split/pytorch_model.bin\n",
      "tokenizer config file saved in logs/scibert_20_epochs_64_batch_99_train_split/tokenizer_config.json\n",
      "Special tokens file saved in logs/scibert_20_epochs_64_batch_99_train_split/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"logs/scibert_20_epochs_64_batch_99_train_split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat to the initial format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: text, row, offset_mapping, filename.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 14146\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "# prediction = np.load(\"../data/prediction.npy\")\n",
    "# print(\"min :\", min(prediction.reshape(-1)))\n",
    "# print(\"max :\", max(prediction.reshape(-1)))\n",
    "# prediction = predictions_2\n",
    "test_data = D[\"test\"]\n",
    "prediction, _, _ = trainer.predict(D[\"test\"])\n",
    "prediction = np.argmax(prediction, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'o',\n",
       " 1: 'test',\n",
       " 2: 'test',\n",
       " 3: 'treatment',\n",
       " 4: 'treatment',\n",
       " 5: 'present',\n",
       " 6: 'present',\n",
       " 7: 'absent',\n",
       " 8: 'absent',\n",
       " 9: 'possible',\n",
       " 10: 'possible',\n",
       " 11: 'conditional',\n",
       " 12: 'conditional',\n",
       " 13: 'hypothetical',\n",
       " 14: 'hypothetical',\n",
       " 15: 'associated_with_someone_else',\n",
       " 16: 'associated_with_someone_else'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list\n",
    "token_to_label = {token: token.split(\"-\")[-1] for token in label_list}\n",
    "token_id_to_label = {i: token_to_label[token].lower() for i, token in enumerate(label_list)}\n",
    "token_id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_to_concept = {\n",
    "    \"test\" : \"test\",\n",
    "    \"treatment\" : \"treatment\",\n",
    "    \"present\" : \"problem\",\n",
    "    \"absent\" : \"problem\",\n",
    "    \"possible\" : \"problem\",\n",
    "    \"conditional\" : \"problem\",\n",
    "    \"hypothetical\" : \"problem\",\n",
    "    \"associated_with_someone_else\" : \"problem\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.save_predictions import save_predictions\n",
    "\n",
    "# save_predictions(test_data, prediction)\n",
    "save_predictions(test_data, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_ner_model = pipeline(task=\"ner\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_1',\n",
       "  'score': 0.99998295,\n",
       "  'index': 1,\n",
       "  'word': 'her',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.99998784,\n",
       "  'index': 2,\n",
       "  'word': 'coagulation',\n",
       "  'start': 4,\n",
       "  'end': 15},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9999907,\n",
       "  'index': 3,\n",
       "  'word': 'parameters',\n",
       "  'start': 16,\n",
       "  'end': 26},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9999912,\n",
       "  'index': 4,\n",
       "  'word': 'were',\n",
       "  'start': 27,\n",
       "  'end': 31},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.99999094,\n",
       "  'index': 5,\n",
       "  'word': 'normal',\n",
       "  'start': 32,\n",
       "  'end': 38},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9999906,\n",
       "  'index': 6,\n",
       "  'word': '.',\n",
       "  'start': 39,\n",
       "  'end': 40}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_ner_model(D[\"train\"][4][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "def visualize_entities(sentence):\n",
    "    tokens = effect_ner_model(sentence)\n",
    "    entities = []\n",
    "\n",
    "    for token in tokens:\n",
    "        label = int(token[\"entity\"][-1])\n",
    "        if label != 0:\n",
    "            token[\"label\"] = label_list[label]\n",
    "            entities.append(token)\n",
    "\n",
    "    params = [{\"text\": sentence, \"ents\": entities, \"title\": None}]\n",
    "\n",
    "    html = displacy.render(\n",
    "        params,\n",
    "        style=\"ent\",\n",
    "        manual=True,\n",
    "        jupyter=True,\n",
    "        options={\n",
    "            \"colors\": {\n",
    "                \"B-PROBLEM\": \"#f08080\",\n",
    "                \"I-PROBLEM\": \"#f08080\",\n",
    "                \"B-TEST\": \"#9bddff\",\n",
    "                \"I-TEST\": \"#9bddff\",\n",
    "                \"B-TREATMENT\": \"#ffdab9\",\n",
    "                \"I-TREATMENT\": \"#ffdab9\",\n",
    "            },\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">at osh , \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    sputum\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cultures\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    grew\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pseudomonas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    and\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    mrsa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    and\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    he\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    was\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    arte\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    d\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    on\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    zo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    syn\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    08\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    08\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    )\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    and\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    vancomycin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    08\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    09\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    )\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: at osh , sputum cultures grew pseudomonas and mrsa , and he was restarted on zosyn ( 08-08 ) and vancomycin ( 08-09 ).\n",
      "Tests: ['sputum cultures']\n",
      "Treatments: ['vancomycin', 'zosyn']\n",
      "Predent: ['mrsa', 'pseudomonas']\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">the liver was 11 cm by \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    perc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TEST</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ussion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: the liver was 11 cm by percussion .\n",
      "Tests: ['percussion']\n",
      "Treatments: []\n",
      "Predent: []\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    hepatitis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    35\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ago\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: hepatitis 35 years ago .\n",
      "Tests: []\n",
      "Treatments: []\n",
      "Predent: ['hepatitis']\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Fluid , electrolytes and nutritions :</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Fluid , electrolytes and nutritions :\n",
      "Tests: []\n",
      "Treatments: []\n",
      "Predent: []\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    top\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rol\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    xl\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    25\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    mg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    qd\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ay\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: toprol xl 25 mg qday\n",
      "Tests: []\n",
      "Treatments: ['toprol xl']\n",
      "Predent: []\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick 5 random sentences from the test set\n",
    "for i in range(5):\n",
    "    index = np.random.randint(0, len(D[\"train\"]))\n",
    "    visualize_entities(D[\"train\"][index][\"text\"])\n",
    "    print(f\"Text: {D['train'][index]['text']}\")\n",
    "    # print(f\"Problems: {D['D'][index]['problem']}\")\n",
    "    print(f\"Tests: {D['train'][index]['test']}\")\n",
    "    print(f\"Treatments: {D['train'][index]['treatment']}\")\n",
    "    print(f\"Predent: {D['train'][index]['present']}\")\n",
    "    print(f\"{'*' * 50}\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d650a3b94c458c273a536969f20832049d87778dafdd87df0ff4ffcada142abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
