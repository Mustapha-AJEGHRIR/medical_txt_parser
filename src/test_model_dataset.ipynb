{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from data_preprocessing import Get_and_process_data\n",
    "from datasets import Dataset, ClassLabel, Sequence, load_dataset, load_metric\n",
    "from transformers import (AutoModelForTokenClassification, \n",
    "                          AutoTokenizer, \n",
    "                          DataCollatorForTokenClassification,\n",
    "                          pipeline,\n",
    "                          TrainingArguments, \n",
    "                          Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/m5u9s00/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/m5u9s00/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/m5u9s00/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/m5u9s00/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"allenai/scibert_scivocab_uncased\"\n",
    "batch_size = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading raw text: 100%|██████████| 170/170 [00:07<00:00, 23.95it/s]\n",
      "Processing raw text: 170it [00:01, 165.02it/s]\n",
      "Adding unlabeled lines: 8454it [00:00, 10921.11it/s]\n",
      "Formatting dataset: 16525it [00:02, 5526.96it/s]\n",
      "Using custom data configuration default-16ea6d19d6627546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/m5u9s00/.cache/huggingface/datasets/json/default-16ea6d19d6627546/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44752f9cb6be4f0093ce3ff8f513700f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dec302111874adba54e8a342e537c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/m5u9s00/.cache/huggingface/datasets/json/default-16ea6d19d6627546/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabcf117356f4bc6a29e879bf220c2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8208066d2d2549879ec84d6aa8b069c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15593 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8175ae8d2f2342f48717f0a6d9ce5eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/821 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/m5u9s00/.cache/huggingface/datasets/json/default-16ea6d19d6627546/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde/cache-35f631c589824040.arrow\n",
      "Loading raw text for test: 100%|██████████| 128/128 [00:11<00:00, 11.35it/s]\n",
      "Formatting test data: 13617it [00:00, 24979.72it/s]\n",
      "Using custom data configuration default-4f491f08e66c6861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/m5u9s00/.cache/huggingface/datasets/json/default-4f491f08e66c6861/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb680877b80441c89e818d9c8795ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9386f119d248e1bbfdf787666e69d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/m5u9s00/.cache/huggingface/datasets/json/default-4f491f08e66c6861/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a1b7f0f11645b68790c3c8710f5e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a6c8a55bdf4f199cda5cdb3260ee68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16414 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'test', 'test_indices_start', 'test_indices_end', 'treatment', 'treatment_indices_start', 'treatment_indices_end', 'present', 'present_indices_start', 'present_indices_end', 'absent', 'absent_indices_start', 'absent_indices_end', 'possible', 'possible_indices_start', 'possible_indices_end', 'conditional', 'conditional_indices_start', 'conditional_indices_end', 'hypothetical', 'hypothetical_indices_start', 'hypothetical_indices_end', 'associated with someone else', 'associated with someone else_indices_start', 'associated with someone else_indices_end', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels'],\n",
       "        num_rows: 15593\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'test', 'test_indices_start', 'test_indices_end', 'treatment', 'treatment_indices_start', 'treatment_indices_end', 'present', 'present_indices_start', 'present_indices_end', 'absent', 'absent_indices_start', 'absent_indices_end', 'possible', 'possible_indices_start', 'possible_indices_end', 'conditional', 'conditional_indices_start', 'conditional_indices_end', 'hypothetical', 'hypothetical_indices_start', 'hypothetical_indices_end', 'associated with someone else', 'associated with someone else_indices_start', 'associated with someone else_indices_end', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'],\n",
       "        num_rows: 16414\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'test', 'test_indices_start', 'test_indices_end', 'treatment', 'treatment_indices_start', 'treatment_indices_end', 'present', 'present_indices_start', 'present_indices_end', 'absent', 'absent_indices_start', 'absent_indices_end', 'possible', 'possible_indices_start', 'possible_indices_end', 'conditional', 'conditional_indices_start', 'conditional_indices_end', 'hypothetical', 'hypothetical_indices_start', 'hypothetical_indices_end', 'associated with someone else', 'associated with someone else_indices_start', 'associated with someone else_indices_end', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels'],\n",
       "        num_rows: 821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = Get_and_process_data(tokenizer, train_split=0.95, add_unlabeled=False)\n",
    "D = data_loader.get_dataset()\n",
    "label_list = data_loader.get_label_list()\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/m5u9s00/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at /home/m5u9s00/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"training_logs/{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    # learning_rate=1e-5,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.05,\n",
    "    logging_steps=5,\n",
    "\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=D[\"train\"],\n",
    "    eval_dataset=D[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running training *****\n",
      "  Num examples = 15593\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9760\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9760' max='9760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9760/9760 48:00, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.600600</td>\n",
       "      <td>0.684513</td>\n",
       "      <td>0.269297</td>\n",
       "      <td>0.300191</td>\n",
       "      <td>0.283906</td>\n",
       "      <td>0.793004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.368500</td>\n",
       "      <td>0.410699</td>\n",
       "      <td>0.449927</td>\n",
       "      <td>0.592734</td>\n",
       "      <td>0.511551</td>\n",
       "      <td>0.870241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>0.339691</td>\n",
       "      <td>0.536474</td>\n",
       "      <td>0.674952</td>\n",
       "      <td>0.597798</td>\n",
       "      <td>0.894957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.277114</td>\n",
       "      <td>0.605307</td>\n",
       "      <td>0.697897</td>\n",
       "      <td>0.648313</td>\n",
       "      <td>0.912896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.089200</td>\n",
       "      <td>0.295672</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>0.736138</td>\n",
       "      <td>0.689346</td>\n",
       "      <td>0.913992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.326391</td>\n",
       "      <td>0.633224</td>\n",
       "      <td>0.736138</td>\n",
       "      <td>0.680813</td>\n",
       "      <td>0.914590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.326181</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.760994</td>\n",
       "      <td>0.710714</td>\n",
       "      <td>0.924357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.352940</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.757170</td>\n",
       "      <td>0.712871</td>\n",
       "      <td>0.922962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.074200</td>\n",
       "      <td>0.378488</td>\n",
       "      <td>0.708042</td>\n",
       "      <td>0.774379</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.926251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.393227</td>\n",
       "      <td>0.684838</td>\n",
       "      <td>0.768642</td>\n",
       "      <td>0.724324</td>\n",
       "      <td>0.925653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.417449</td>\n",
       "      <td>0.691379</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.727108</td>\n",
       "      <td>0.921666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.433513</td>\n",
       "      <td>0.702238</td>\n",
       "      <td>0.780115</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.925154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.440595</td>\n",
       "      <td>0.711268</td>\n",
       "      <td>0.772467</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.920371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.460799</td>\n",
       "      <td>0.727599</td>\n",
       "      <td>0.776291</td>\n",
       "      <td>0.751156</td>\n",
       "      <td>0.924357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>0.727110</td>\n",
       "      <td>0.774379</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.921766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.483001</td>\n",
       "      <td>0.700342</td>\n",
       "      <td>0.782027</td>\n",
       "      <td>0.738934</td>\n",
       "      <td>0.920470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.486230</td>\n",
       "      <td>0.731449</td>\n",
       "      <td>0.791587</td>\n",
       "      <td>0.760331</td>\n",
       "      <td>0.923460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.486618</td>\n",
       "      <td>0.714536</td>\n",
       "      <td>0.780115</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.925055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.487767</td>\n",
       "      <td>0.709059</td>\n",
       "      <td>0.778203</td>\n",
       "      <td>0.742024</td>\n",
       "      <td>0.925952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.485187</td>\n",
       "      <td>0.705373</td>\n",
       "      <td>0.778203</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.924855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-3500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-4500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-5500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-6000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-6000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-6000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-6500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-6500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-6500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-7000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-7000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-7000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-7500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-7500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-7500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-8000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-8000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-8000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-8500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-8500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-8500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-9000\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-9000/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-9000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-9500\n",
      "Configuration saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-9500/config.json\n",
      "Model weights saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/scibert_scivocab_uncased-finetuned-ner/checkpoint-9500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9760, training_loss=0.15571206227638168, metrics={'train_runtime': 2881.2935, 'train_samples_per_second': 108.236, 'train_steps_per_second': 3.387, 'total_flos': 8168461354174668.0, 'train_loss': 0.15571206227638168, 'epoch': 20.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 821\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='539' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 01:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ABSENT': {'precision': 0.7391304347826086,\n",
       "  'recall': 0.8225806451612904,\n",
       "  'f1': 0.7786259541984734,\n",
       "  'number': 62},\n",
       " 'CONDITIONAL': {'precision': 0.14285714285714285,\n",
       "  'recall': 0.25,\n",
       "  'f1': 0.18181818181818182,\n",
       "  'number': 4},\n",
       " 'HYPOTHETICAL': {'precision': 0.4,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.4444444444444445,\n",
       "  'number': 8},\n",
       " 'POSSIBLE': {'precision': 0.375,\n",
       "  'recall': 0.3333333333333333,\n",
       "  'f1': 0.35294117647058826,\n",
       "  'number': 9},\n",
       " 'PRESENT': {'precision': 0.6534090909090909,\n",
       "  'recall': 0.7467532467532467,\n",
       "  'f1': 0.696969696969697,\n",
       "  'number': 154},\n",
       " 'TEST': {'precision': 0.7111111111111111,\n",
       "  'recall': 0.7804878048780488,\n",
       "  'f1': 0.7441860465116279,\n",
       "  'number': 123},\n",
       " 'TREATMENT': {'precision': 0.7965116279069767,\n",
       "  'recall': 0.8404907975460123,\n",
       "  'f1': 0.8179104477611939,\n",
       "  'number': 163},\n",
       " 'overall_precision': 0.7053726169844021,\n",
       " 'overall_recall': 0.7782026768642447,\n",
       " 'overall_f1': 0.7400000000000001,\n",
       " 'overall_accuracy': 0.9248554913294798}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(D[\"val\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: absent_indices_end, hypothetical_indices_start, possible, hypothetical, offset_mapping, conditional_indices_start, associated with someone else_indices_end, test, present_indices_end, present_indices_start, test_indices_start, associated with someone else, absent_indices_start, treatment_indices_start, possible_indices_end, hypothetical_indices_end, text, treatment_indices_end, associated with someone else_indices_start, conditional_indices_end, possible_indices_start, treatment, conditional, present, absent, test_indices_end.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 16414\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "predictions_2, labels_2, _ = trainer.predict(D[\"test\"])\n",
    "predictions_2 = np.argmax(predictions_2, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u =  7481\n"
     ]
    }
   ],
   "source": [
    "x= 0\n",
    "u = 0\n",
    "for i in range(x, x+16000):\n",
    "    if sum(predictions_2[i])>0:\n",
    "        u+=1\n",
    "        # print(\"i = \", i, end=\"\\t -> \")\n",
    "        # print(sum(predictions_2[i]))\n",
    "print(\"u = \", u)\n",
    "\n",
    "# 7510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       0, 4, 0, 4, 4, 4, 4, 4, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_2[194]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in logs/model_50_warmup_epochs/config.json\n",
      "Model weights saved in logs/model_50_warmup_epochs/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# model.save_pretrained(\"logs/model_50_warmup_epochs\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d650a3b94c458c273a536969f20832049d87778dafdd87df0ff4ffcada142abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
