{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from data_preprocessing import Get_and_process_data\n",
    "from datasets import Dataset, ClassLabel, Sequence, load_dataset, load_metric\n",
    "from transformers import (AutoModelForTokenClassification, \n",
    "                          AutoTokenizer, \n",
    "                          DataCollatorForTokenClassification,\n",
    "                          pipeline,\n",
    "                          TrainingArguments, \n",
    "                          Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "# model_checkpoint = \"allenai/scibert_scivocab_uncased\"\n",
    "model_checkpoint = \"giacomomiolo/electramed_base_scivocab_1M\"\n",
    "batch_size = 32\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading raw text: 100%|██████████| 170/170 [00:07<00:00, 23.46it/s]\n",
      "Processing raw text: 170it [00:01, 156.88it/s]\n",
      "Adding unlabeled lines: 8454it [00:00, 10414.01it/s]\n",
      "Formatting dataset: 16525it [00:03, 5369.16it/s]\n",
      "Using custom data configuration default-710d6deeeb2c3cc3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/m5u9s00/.cache/huggingface/datasets/json/default-710d6deeeb2c3cc3/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da34e6325ee473795910f84e0ed2007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19892be0a1c94fcb839449d0c18c6c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/m5u9s00/.cache/huggingface/datasets/json/default-710d6deeeb2c3cc3/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bbfb874c1a40c1967d703327077f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449d5fdd73aa44308eff8e327b24830a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15593 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a6cfb21eb9402cb9299d2c0a8b2a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/821 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/m5u9s00/.cache/huggingface/datasets/json/default-710d6deeeb2c3cc3/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-3d9ceab58bec6d3a.arrow\n",
      "Loading raw text for test: 100%|██████████| 128/128 [00:11<00:00, 10.89it/s]\n",
      "Formatting test data: 13617it [00:00, 21519.79it/s]\n",
      "Using custom data configuration default-7d8433eb049b5d4a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/m5u9s00/.cache/huggingface/datasets/json/default-7d8433eb049b5d4a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839f55294552469689626ffb768a0bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6ad93cdc394fbba7a9bff7145a0714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/m5u9s00/.cache/huggingface/datasets/json/default-7d8433eb049b5d4a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805b0cf5cfb94c689787e3552f6cf347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b56f53c546b4b42ada44ee602145e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13617 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = Get_and_process_data(tokenizer, train_split=0.95, add_unlabeled=True)\n",
    "D = data_loader.get_dataset()\n",
    "label_list = data_loader.get_label_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForTokenClassification.from_pretrained(\"logs/model_60_warmup_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/giacomomiolo/electramed_base_scivocab_1M/resolve/main/config.json from cache at /home/m5u9s00/.cache/huggingface/transformers/c482a39d4ee91eb23933abaff8eba8ae535cd80563e3b66caa41934b37d52138.1c949c4a68e30225f35ee7db82195ffff60db502f0c9b37ed1cc866f8708614a\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"giacomomiolo/electramed_base_scivocab_1M\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/giacomomiolo/electramed_base_scivocab_1M/resolve/main/pytorch_model.bin from cache at /home/m5u9s00/.cache/huggingface/transformers/7cd3aa7a826f713e7f8bc89440b75c35ea619acee924e1bc577fa87beb7dab53.38801300cd5c23e2bbbdc1a904e80e12f224e9c15b7d08da192f658e2b50de0d\n",
      "Some weights of the model checkpoint at giacomomiolo/electramed_base_scivocab_1M were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at giacomomiolo/electramed_base_scivocab_1M and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"training_logs/{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    # learning_rate=1e-5,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.05,\n",
    "    logging_steps=5,\n",
    "\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# def masking(l):\n",
    "#     if l >= 5:\n",
    "#         if l % 2 == 0:\n",
    "#             return 6\n",
    "#         else :\n",
    "#             return 5\n",
    "#     else :\n",
    "#         return l\n",
    "        \n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=D[\"train\"],\n",
    "    eval_dataset=D[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running training *****\n",
      "  Num examples = 15593\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3660\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3660' max='3660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3660/3660 28:14, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.515800</td>\n",
       "      <td>0.474720</td>\n",
       "      <td>0.556845</td>\n",
       "      <td>0.583942</td>\n",
       "      <td>0.570071</td>\n",
       "      <td>0.882645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.229068</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.766423</td>\n",
       "      <td>0.733411</td>\n",
       "      <td>0.938635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.177100</td>\n",
       "      <td>0.202665</td>\n",
       "      <td>0.739679</td>\n",
       "      <td>0.784672</td>\n",
       "      <td>0.761511</td>\n",
       "      <td>0.942793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.185424</td>\n",
       "      <td>0.772780</td>\n",
       "      <td>0.815085</td>\n",
       "      <td>0.793369</td>\n",
       "      <td>0.949488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.182688</td>\n",
       "      <td>0.796253</td>\n",
       "      <td>0.827251</td>\n",
       "      <td>0.811456</td>\n",
       "      <td>0.953342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.180355</td>\n",
       "      <td>0.786866</td>\n",
       "      <td>0.830900</td>\n",
       "      <td>0.808284</td>\n",
       "      <td>0.954559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.192984</td>\n",
       "      <td>0.786127</td>\n",
       "      <td>0.827251</td>\n",
       "      <td>0.806165</td>\n",
       "      <td>0.952936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.201601</td>\n",
       "      <td>0.801854</td>\n",
       "      <td>0.841849</td>\n",
       "      <td>0.821365</td>\n",
       "      <td>0.953951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.204951</td>\n",
       "      <td>0.821262</td>\n",
       "      <td>0.855231</td>\n",
       "      <td>0.837902</td>\n",
       "      <td>0.956385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.199271</td>\n",
       "      <td>0.804907</td>\n",
       "      <td>0.838200</td>\n",
       "      <td>0.821216</td>\n",
       "      <td>0.955776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.205107</td>\n",
       "      <td>0.816374</td>\n",
       "      <td>0.849148</td>\n",
       "      <td>0.832439</td>\n",
       "      <td>0.957602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.217226</td>\n",
       "      <td>0.816589</td>\n",
       "      <td>0.850365</td>\n",
       "      <td>0.833135</td>\n",
       "      <td>0.955979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.216763</td>\n",
       "      <td>0.818396</td>\n",
       "      <td>0.844282</td>\n",
       "      <td>0.831138</td>\n",
       "      <td>0.956385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.215055</td>\n",
       "      <td>0.814035</td>\n",
       "      <td>0.846715</td>\n",
       "      <td>0.830054</td>\n",
       "      <td>0.956689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>0.217409</td>\n",
       "      <td>0.813084</td>\n",
       "      <td>0.846715</td>\n",
       "      <td>0.829559</td>\n",
       "      <td>0.956892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-500\n",
      "Configuration saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-500/config.json\n",
      "Model weights saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-1000\n",
      "Configuration saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-1000/config.json\n",
      "Model weights saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-1500\n",
      "Configuration saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-1500/config.json\n",
      "Model weights saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2000\n",
      "Configuration saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2000/config.json\n",
      "Model weights saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2500\n",
      "Configuration saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2500/config.json\n",
      "Model weights saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-3000\n",
      "Configuration saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-3000/config.json\n",
      "Model weights saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-3000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-3500\n",
      "Configuration saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-3500/config.json\n",
      "Model weights saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-3500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3660, training_loss=0.17354840086416795, metrics={'train_runtime': 1695.2192, 'train_samples_per_second': 137.973, 'train_steps_per_second': 2.159, 'total_flos': 7518366901071588.0, 'train_loss': 0.17354840086416795, 'epoch': 15.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model = AutoModelForTokenClassification.from_pretrained(\"./training_logs/electramed_base_scivocab_1M-finetuned-ner/checkpoint-2500\", local_files_only=True)\n",
    "# trainer.model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performence on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: associated with someone else_indices_start, treatment, conditional_indices_start, absent_indices_start, conditional_indices_end, hypothetical_indices_start, offset_mapping, test_indices_end, hypothetical, present_indices_end, text, possible_indices_start, absent, associated with someone else_indices_end, associated with someone else, possible, present_indices_start, test_indices_start, test, treatment_indices_end, possible_indices_end, conditional, row, absent_indices_end, present, treatment_indices_start, filename, hypothetical_indices_end.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 821\n",
      "  Batch size = 64\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ABSENT': {'precision': 0.819672131147541,\n",
       "  'recall': 0.8620689655172413,\n",
       "  'f1': 0.8403361344537814,\n",
       "  'number': 58},\n",
       " 'CONDITIONAL': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3},\n",
       " 'HYPOTHETICAL': {'precision': 0.625,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.7692307692307693,\n",
       "  'number': 5},\n",
       " 'POSSIBLE': {'precision': 0.6451612903225806,\n",
       "  'recall': 0.8,\n",
       "  'f1': 0.7142857142857142,\n",
       "  'number': 25},\n",
       " 'PRESENT': {'precision': 0.8294573643410853,\n",
       "  'recall': 0.8492063492063492,\n",
       "  'f1': 0.8392156862745098,\n",
       "  'number': 252},\n",
       " 'TEST': {'precision': 0.8084291187739464,\n",
       "  'recall': 0.8791666666666667,\n",
       "  'f1': 0.8423153692614771,\n",
       "  'number': 240},\n",
       " 'TREATMENT': {'precision': 0.8367346938775511,\n",
       "  'recall': 0.8577405857740585,\n",
       "  'f1': 0.8471074380165289,\n",
       "  'number': 239},\n",
       " 'overall_precision': 0.815028901734104,\n",
       " 'overall_recall': 0.8576642335766423,\n",
       " 'overall_f1': 0.8358032009484292,\n",
       " 'overall_accuracy': 0.9560807384116036}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(D[\"val\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to logs/electramed_15_epochs\n",
      "Configuration saved in logs/electramed_15_epochs/config.json\n",
      "Model weights saved in logs/electramed_15_epochs/pytorch_model.bin\n",
      "tokenizer config file saved in logs/electramed_15_epochs/tokenizer_config.json\n",
      "Special tokens file saved in logs/electramed_15_epochs/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"logs/electramed_15_epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat to the initial format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: offset_mapping, filename, row, text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 13617\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "# prediction = np.load(\"../data/prediction.npy\")\n",
    "# print(\"min :\", min(prediction.reshape(-1)))\n",
    "# print(\"max :\", max(prediction.reshape(-1)))\n",
    "# prediction = predictions_2\n",
    "test_data = D[\"test\"]\n",
    "prediction, _, _ = trainer.predict(D[\"test\"])\n",
    "prediction = np.argmax(prediction, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'o',\n",
       " 1: 'test',\n",
       " 2: 'test',\n",
       " 3: 'treatment',\n",
       " 4: 'treatment',\n",
       " 5: 'present',\n",
       " 6: 'present',\n",
       " 7: 'absent',\n",
       " 8: 'absent',\n",
       " 9: 'possible',\n",
       " 10: 'possible',\n",
       " 11: 'conditional',\n",
       " 12: 'conditional',\n",
       " 13: 'hypothetical',\n",
       " 14: 'hypothetical',\n",
       " 15: 'associated with someone else',\n",
       " 16: 'associated with someone else'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list\n",
    "token_to_label = {token: token.split(\"-\")[-1] for token in label_list}\n",
    "token_id_to_label = {i: token_to_label[token].lower() for i, token in enumerate(label_list)}\n",
    "token_id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_to_concept = {\n",
    "    \"test\" : \"test\",\n",
    "    \"treatment\" : \"treatment\",\n",
    "    \"present\" : \"problem\",\n",
    "    \"absent\" : \"problem\",\n",
    "    \"possible\" : \"problem\",\n",
    "    \"conditional\" : \"problem\",\n",
    "    \"hypothetical\" : \"problem\",\n",
    "    \"associated with someone else\" : \"problem\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.save_predictions import save_predictions\n",
    "\n",
    "# save_predictions(test_data, prediction)\n",
    "save_predictions(test_data, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_ner_model = pipeline(task=\"ner\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_1',\n",
       "  'score': 0.99998295,\n",
       "  'index': 1,\n",
       "  'word': 'her',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.99998784,\n",
       "  'index': 2,\n",
       "  'word': 'coagulation',\n",
       "  'start': 4,\n",
       "  'end': 15},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9999907,\n",
       "  'index': 3,\n",
       "  'word': 'parameters',\n",
       "  'start': 16,\n",
       "  'end': 26},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9999912,\n",
       "  'index': 4,\n",
       "  'word': 'were',\n",
       "  'start': 27,\n",
       "  'end': 31},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.99999094,\n",
       "  'index': 5,\n",
       "  'word': 'normal',\n",
       "  'start': 32,\n",
       "  'end': 38},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9999906,\n",
       "  'index': 6,\n",
       "  'word': '.',\n",
       "  'start': 39,\n",
       "  'end': 40}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_ner_model(D[\"train\"][4][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "def visualize_entities(sentence):\n",
    "    tokens = effect_ner_model(sentence)\n",
    "    entities = []\n",
    "\n",
    "    for token in tokens:\n",
    "        label = int(token[\"entity\"][-1])\n",
    "        if label != 0:\n",
    "            token[\"label\"] = label_list[label]\n",
    "            entities.append(token)\n",
    "\n",
    "    params = [{\"text\": sentence, \"ents\": entities, \"title\": None}]\n",
    "\n",
    "    html = displacy.render(\n",
    "        params,\n",
    "        style=\"ent\",\n",
    "        manual=True,\n",
    "        jupyter=True,\n",
    "        options={\n",
    "            \"colors\": {\n",
    "                \"B-PROBLEM\": \"#f08080\",\n",
    "                \"I-PROBLEM\": \"#f08080\",\n",
    "                \"B-TEST\": \"#9bddff\",\n",
    "                \"I-TEST\": \"#9bddff\",\n",
    "                \"B-TREATMENT\": \"#ffdab9\",\n",
    "                \"I-TREATMENT\": \"#ffdab9\",\n",
    "            },\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">at osh , \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    sputum\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cultures\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    grew\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pseudomonas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    and\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    mrsa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    and\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    he\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    was\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    arte\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    d\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    on\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    zo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    syn\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    08\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    08\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    )\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    and\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    vancomycin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    08\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    09\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    )\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: at osh , sputum cultures grew pseudomonas and mrsa , and he was restarted on zosyn ( 08-08 ) and vancomycin ( 08-09 ).\n",
      "Tests: ['sputum cultures']\n",
      "Treatments: ['vancomycin', 'zosyn']\n",
      "Predent: ['mrsa', 'pseudomonas']\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">the liver was 11 cm by \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    perc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TEST</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ussion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #9bddff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TEST</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: the liver was 11 cm by percussion .\n",
      "Tests: ['percussion']\n",
      "Treatments: []\n",
      "Predent: []\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    hepatitis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    35\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ago\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PRESENT</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: hepatitis 35 years ago .\n",
      "Tests: []\n",
      "Treatments: []\n",
      "Predent: ['hepatitis']\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Fluid , electrolytes and nutritions :</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Fluid , electrolytes and nutritions :\n",
      "Tests: []\n",
      "Treatments: []\n",
      "Predent: []\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    top\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rol\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    xl\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    25\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    mg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    qd\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ffdab9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ay\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-TREATMENT</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: toprol xl 25 mg qday\n",
      "Tests: []\n",
      "Treatments: ['toprol xl']\n",
      "Predent: []\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick 5 random sentences from the test set\n",
    "for i in range(5):\n",
    "    index = np.random.randint(0, len(D[\"train\"]))\n",
    "    visualize_entities(D[\"train\"][index][\"text\"])\n",
    "    print(f\"Text: {D['train'][index]['text']}\")\n",
    "    # print(f\"Problems: {D['D'][index]['problem']}\")\n",
    "    print(f\"Tests: {D['train'][index]['test']}\")\n",
    "    print(f\"Treatments: {D['train'][index]['treatment']}\")\n",
    "    print(f\"Predent: {D['train'][index]['present']}\")\n",
    "    print(f\"{'*' * 50}\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d650a3b94c458c273a536969f20832049d87778dafdd87df0ff4ffcada142abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
