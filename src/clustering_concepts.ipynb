{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Embedding for all the lines of the document\n",
    " <!-- - Embeddings for all concepts -->\n",
    " <!-- - Each concept has a list of neighboring concepts based on similarity (e.g. cosine similarity) -->\n",
    " <!-- - The searched term will be embedded and compared to all concepts -->\n",
    " - The searched term will be embedded and compared to all lines of the corpus (with hashing to accelerate)\n",
    " <!-- - Return patients having the neighboring concepts of the searched term -->\n",
    " - Return patients that have big similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -U sentence-transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- tech ----------------------------------- #\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------- Transformers and freinds ------------------------- #\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------- local ---------------------------------- #\n",
    "from data_preprocessing import Get_and_process_data\n",
    "\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "model_checkpoint = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModel.from_pretrained(model_checkpoint)\n",
    "data_path = \"../data/train/txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take average of all tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts, tokenizer = tokenizer, model= model):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def semantic_search_base(query_emb, doc_emb):\n",
    "    #Compute dot score between query and all document embeddings\n",
    "    scores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n",
    "\n",
    "    #Combine docs & scores\n",
    "    doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "    #Sort by decreasing score\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    #Output passages & scores\n",
    "    for doc, score in doc_score_pairs:\n",
    "        print(score, doc)\n",
    "        \n",
    "def forward(texts, tokenizer= tokenizer, model= model):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def forward_doc(texts, tokenizer= tokenizer, model= model, no_grad= False):\n",
    "    lines = texts.split(\"\\n\")\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    encoded_input_lines = tokenizer(lines, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Compute token embeddings\n",
    "    if no_grad:\n",
    "        with torch.no_grad():\n",
    "            model_output_lines = model(**encoded_input_lines, return_dict=True)\n",
    "    else :\n",
    "        model_output_lines = model(**encoded_input_lines, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings_lines = mean_pooling(model_output_lines, encoded_input_lines['attention_mask'])\n",
    "\n",
    "    # NOTE: This is an easy approach\n",
    "    # another mean pooling over the lines of the document\n",
    "    embeddings = torch.mean(embeddings_lines, 0).unsqueeze(0)\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Inference from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915637195110321 Around 9 Million people live in London\n",
      "0.49475765228271484 London is known for its financial district\n"
     ]
    }
   ],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "query = \"How many people live in London?\"\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "#Encode query and docs\n",
    "query_emb = encode(query)\n",
    "doc_emb = encode(docs)\n",
    "\n",
    "semantic_search_base(query_emb, doc_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.915637195110321 Around 9 Million people live in London\n",
    "\n",
    "\n",
    "0.49475765228271484 London is known for its financial district"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
    "model_output = model(**encoded_input, return_dict=True)\n",
    "# model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 384])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape : torch.Size([1, 384])\n",
      "a shape : torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "query = \"How many people live in London?\"\n",
    "answer = \"Around 9 Million people live in London\"\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "q = forward(query)\n",
    "print(\"q shape :\", q.shape)\n",
    "a = forward(answer)\n",
    "print(\"a shape :\", a.shape)\n",
    "\n",
    "loss = loss_fn(a,q)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"\"\n",
    "\n",
    "with open(\"../data/train/txt/018636330_DH.txt\") as f:\n",
    "    doc = f.read()\n",
    "    \n",
    "doc_emb = forward_doc(doc)\n",
    "doc_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26280614733695984 Around 9 Million people live in London\n"
     ]
    }
   ],
   "source": [
    "c_emb= encode(\"norvasc << test >>\")\n",
    "semantic_search_base(c_emb, doc_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embeddings():\n",
    "#     return np.random.rand(1,384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding documents: 100%|##########| 170/170 [05:42<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# what are the elements in the folder ../data/train/txt/\n",
    "all_docs = {}\n",
    "text_files = glob.glob(data_path + os.sep +  \"*.txt\")\n",
    "for file in tqdm(text_files, \"Encoding documents\", ascii=True):\n",
    "    with open(file) as f:\n",
    "        doc = f.read()\n",
    "    file_name = os.path.basename(file).split(\".\")[0]\n",
    "    all_docs[file_name] = forward_doc(doc, no_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + os.sep + \"all_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + os.sep + \"all_docs.pkl\", \"rb\") as f:\n",
    "#     D = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"allenai/scibert_scivocab_uncased\"\n",
    "# batch_size = 32\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "# data_loader = Get_and_process_data(tokenizer, train_split=0.95, add_unlabeled=True)\n",
    "# D = data_loader.get_dataset()\n",
    "# label_list = data_loader.get_label_list()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d650a3b94c458c273a536969f20832049d87778dafdd87df0ff4ffcada142abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
