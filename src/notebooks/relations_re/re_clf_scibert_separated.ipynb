{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mustapha-AJEGHRIR/medical_txt_parser/blob/main/src/notebooks/assertions_nli/ast_nli_scibert.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"4l_yTcIgxsd_"},"source":["# Relations classification\n","\n","Based of: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_xnli.py"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Jan 26 16:50:08 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   66C    P0    30W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n","| N/A   55C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":4620,"status":"ok","timestamp":1642252265905,"user":{"displayName":"Nouamane Tazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg753z6h9fmTPmGyKajJFbNQG48KIqPziiTsxl4Tw=s64","userId":"11345629174419407363"},"user_tz":-60},"id":"LwHxhQx0xseE"},"outputs":[],"source":["%%capture\n","!pip install seqeval transformers datasets spacy sentence_transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1908,"status":"ok","timestamp":1642251232672,"user":{"displayName":"Nouamane Tazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg753z6h9fmTPmGyKajJFbNQG48KIqPziiTsxl4Tw=s64","userId":"11345629174419407363"},"user_tz":-60},"id":"3rCJwcFyyU9J","outputId":"f75daf2c-9611-4b10-faf6-dd37b6e411f5"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_11423/1269076202.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/projects/medical_txt_parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/projects/medical_txt_parser"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1358,"status":"ok","timestamp":1642251234969,"user":{"displayName":"Nouamane Tazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg753z6h9fmTPmGyKajJFbNQG48KIqPziiTsxl4Tw=s64","userId":"11345629174419407363"},"user_tz":-60},"id":"FyDbgPv1xseF"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/jupyter/medical_txt_parser/src/notebooks\n","/home/jupyter/medical_txt_parser/src\n","/home/jupyter/medical_txt_parser\n"]}],"source":["%reload_ext autoreload\n","%autoreload 2\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","path = %pwd\n","while \"src\" in path:\n","    %cd ..\n","    path = %pwd\n","\n","import glob\n","import pandas as pd\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from pprint import pprint\n","import matplotlib.pyplot as plt\n","import random\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","import datasets\n","import numpy as np\n","from datasets import load_dataset, load_metric , Dataset\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","from transformers import pipeline\n","\n","require_version(\"datasets>=1.8.0\", \"To fix: pip install --upgrade datasets\")\n","\n","from src.utils.parse_data import parse_ast, parse_concept, parse_relation"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":219,"status":"ok","timestamp":1642251245139,"user":{"displayName":"Nouamane Tazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg753z6h9fmTPmGyKajJFbNQG48KIqPziiTsxl4Tw=s64","userId":"11345629174419407363"},"user_tz":-60},"id":"Sp1UC7psxseH"},"outputs":[],"source":["train_data_path = \"data/train\"\n","val_data_path = \"data/val\"\n","ast_folder_name = \"ast\"\n","concept_folder_name = \"concept\"\n","rel_folder_name = \"rel\"\n","txt_folder_name = \"txt\"\n","nli_data_path = \"data/nli\"\n","re_data_path = \"data/re\"\n","\n","# model args\n","model_name_or_path = \"allenai/scibert_scivocab_uncased\" # \"gsarti/scibert-nli\"  \"allenai/scibert_scivocab_uncased\"  \"models/scibert_scivocab_uncased-re-1\"\n","cache_dir = None\n","model_revision = None \n","tokenizer_name = model_name_or_path\n","do_lower_case = None\n","use_fast_tokenizer = True\n","fp16 = True\n","\n","# data args\n","pad_to_max_length = None\n","max_seq_length = None\n","\n","set_seed(42)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model Test - Problem"]},{"cell_type":"markdown","metadata":{"id":"zP8CizR4xseH"},"source":["### Import data"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["re_task = \"Te_P\""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>She had &lt;&lt; a workup &gt;&gt; by her neurologist and ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>She had &lt;&lt; a workup &gt;&gt; by her neurologist and ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>She had &lt;&lt; a workup &gt;&gt; by her neurologist and ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>She had a workup by her neurologist and &lt;&lt; an ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>She had a workup by her neurologist and &lt;&lt; an ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2145</th>\n","      <td>The patient had &lt;&lt; an echocardiogram &gt;&gt; on day...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2146</th>\n","      <td>The patient had &lt;&lt; an echocardiogram &gt;&gt; on day...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2147</th>\n","      <td>The patient had &lt;&lt; an echocardiogram &gt;&gt; on day...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2148</th>\n","      <td>The patient had &lt;&lt; an echocardiogram &gt;&gt; on day...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2149</th>\n","      <td>The patient had &lt;&lt; an echocardiogram &gt;&gt; on day...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2150 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   text  label\n","0     She had << a workup >> by her neurologist and ...      0\n","1     She had << a workup >> by her neurologist and ...      0\n","2     She had << a workup >> by her neurologist and ...      0\n","3     She had a workup by her neurologist and << an ...      1\n","4     She had a workup by her neurologist and << an ...      1\n","...                                                 ...    ...\n","2145  The patient had << an echocardiogram >> on day...      1\n","2146  The patient had << an echocardiogram >> on day...      1\n","2147  The patient had << an echocardiogram >> on day...      1\n","2148  The patient had << an echocardiogram >> on day...      1\n","2149  The patient had << an echocardiogram >> on day...      1\n","\n","[2150 rows x 2 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["relations_df = pd.read_csv(re_data_path + os.sep + f\"re_scibert_data_{re_task}.tsv\", sep=\"\\t\", header=None)\n","relations_df.columns = [\"text\", \"label\"]\n","label2id = {label: i for i, label in enumerate(relations_df[\"label\"].value_counts().index.tolist())}\n","id2label = {i: label for label, i in label2id.items()}\n","relations_df[\"label\"] = relations_df.label.map(label2id)\n","relations_df"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 2140\n"," }),\n"," Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 10\n"," }))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Build HuggingFace Dataset\n","\n","train_df, val_df = train_test_split(relations_df, train_size=None, shuffle=True, test_size=10, stratify=relations_df[\"label\"], random_state=42)\n","\n","features = datasets.Features({'text': datasets.Value(dtype='string'),\n"," 'label': datasets.ClassLabel(num_classes=len(id2label), names=list(id2label.values()))})\n","\n","train_dataset = Dataset.from_pandas(train_df, preserve_index=False, features=features)\n","eval_dataset = Dataset.from_pandas(val_df, preserve_index=False, features=features)\n","\n","label_list = train_dataset.features[\"label\"].names\n","num_labels = len(label_list)\n","\n","label_list = train_dataset.features[\"label\"].names\n","train_dataset, eval_dataset"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train_df labels: 1    988\n","0    987\n","2    165\n","Name: label, dtype: int64\n","val_df labels: 0    5\n","1    4\n","2    1\n","Name: label, dtype: int64\n"]}],"source":["# check labels balance\n","print(f\"train_df labels: {train_df['label'].value_counts()}\")\n","print(f\"val_df labels: {val_df['label'].value_counts()}\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load pretrained model and tokenizer\n","# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","# download model & vocab.\n","config = AutoConfig.from_pretrained(\n","    model_name_or_path,\n","    num_labels=num_labels,\n","    finetuning_task=\"re\",\n","    cache_dir=cache_dir,\n","    revision=model_revision,\n","    label2id=label2id,\n","    id2label=id2label\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\n","    tokenizer_name if tokenizer_name else model_name_or_path,\n","    # do_lower_case=do_lower_case,\n","    cache_dir=cache_dir,\n","    use_fast=use_fast_tokenizer,\n","    revision=model_revision,\n",")\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name_or_path,\n","    from_tf=bool(\".ckpt\" in model_name_or_path),\n","    config=config,\n","    cache_dir=cache_dir,\n","    revision=model_revision\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10ec5aaa13f04750bd6cdfe7fedfed7a","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"399b68198bf947e0a2d4ea40253bb139","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Sample 456 of the training set: {'text': 'Ext : warm , 2+ << DP >> b/l , [[ 2+ pitting edema to knees b/l ]]', 'label': 0, 'input_ids': [102, 1267, 862, 8591, 422, 170, 473, 962, 962, 6769, 1374, 1374, 132, 1352, 152, 422, 260, 260, 170, 473, 5304, 586, 12987, 147, 8710, 30113, 132, 1352, 152, 1901, 1901, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 102 of the training set: {'text': '<< An ultrasound of the right upper quadrant >> did not reveal any cholelithiasis or cholecystitis , however , an irregular hepatic contour was seen which is suggestive of [[ underlying chronic liver disease ]] .', 'label': 0, 'input_ids': [102, 962, 962, 130, 7801, 131, 111, 2083, 3105, 21337, 1374, 1374, 1544, 302, 2303, 843, 8104, 19478, 19851, 3353, 234, 25662, 3626, 422, 694, 422, 130, 11147, 7221, 10330, 241, 2187, 334, 165, 17292, 131, 260, 260, 3954, 3164, 2993, 1288, 1901, 1901, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 1126 of the training set: {'text': 'MOUTH NORMAL NECK NORMAL thyroid wnl BREASTS NORMAL no distinct masses NIPPLES NORMAL inverted [ b ] , evert w / stimulation CHEST NORMAL LCTA COR NORMAL RRR ABDOMEN NORMAL gravid EXTREM NORMAL SKIN NORMAL NODES NORMAL VULVA NORMAL no [[ lesions ]] , white d / c at introitus VAGINA NORMAL sml amt thin white d / c ph 4.5 , koh +amine , << NS >> +clue , neg trich CERVIX NORMAL 1/100/0 srom clear OS NORMAL closed ADNEXAE NORMAL no palp masses , NT UTERUS NORMAL gravid UTERINE SIZE IN WEEKS NORMAL term RECTUM NORMAL no ext lesions', 'label': 0, 'input_ids': [102, 12860, 1346, 7980, 1346, 8143, 15239, 30115, 3479, 30113, 1346, 425, 3646, 9686, 26119, 1024, 1346, 13455, 260, 132, 1901, 422, 1661, 30108, 124, 1352, 4156, 8693, 1346, 6087, 2219, 470, 1346, 5058, 30114, 17748, 1346, 10377, 173, 4847, 1346, 3843, 1346, 2207, 1346, 5992, 3833, 1346, 425, 260, 260, 4278, 1901, 1901, 422, 3606, 128, 1352, 115, 235, 1205, 8592, 7988, 1869, 1346, 659, 30115, 439, 30108, 5197, 3606, 128, 1352, 115, 375, 286, 205, 305, 422, 14158, 473, 18605, 422, 962, 962, 3281, 1374, 1374, 473, 326, 376, 422, 1415, 12489, 28765, 1346, 158, 1352, 1287, 1352, 244, 4193, 157, 1716, 3581, 1346, 4358, 290, 25498, 1611, 1346, 425, 21348, 9686, 422, 4405, 18718, 1346, 10377, 173, 14045, 1243, 121, 3272, 1346, 902, 25277, 1346, 425, 1267, 4278, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n"]}],"source":["# Preprocessing the datasets\n","# Padding strategy\n","if pad_to_max_length:\n","    padding = \"max_length\"\n","else:\n","    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","    padding = False\n","\n","\n","def preprocess_function(examples):\n","    # Tokenize the texts\n","    return tokenizer(\n","        examples[\"text\"],\n","        padding=padding,\n","        max_length=max_seq_length,\n","        truncation=True,\n","    )\n","\n","\n","train_dataset = train_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on train dataset\",\n",")\n","eval_dataset = eval_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on validation dataset\",\n",")\n","\n","# Log a few random samples from the training set:\n","for index in random.sample(range(len(train_dataset)), 3):\n","    print(f\"Sample {index} of the training set: {train_dataset[index]}.\\n\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Get the metric function\n","f1_metric = load_metric(\"f1\")\n","precision_metric = load_metric(\"precision\")\n","recall_metric = load_metric(\"recall\")\n","accuracy_metric = load_metric(\"accuracy\")\n","\n","# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","# predictions and label_ids field) and has to return a dictionary string to float.\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","\n","    metrics = {}\n","    metrics.update(f1_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(precision_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(recall_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(accuracy_metric.compute(predictions=preds, references=p.label_ids))\n","    return metrics"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","if pad_to_max_length:\n","    data_collator = default_data_collator\n","elif fp16:\n","    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","else:\n","    data_collator = None"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["'allenai/scibert_scivocab_uncased'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model_name_or_path"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# address class imbalance \n","import torch\n","from torch import nn\n","from transformers import Trainer\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","class_weights = compute_class_weight(class_weight='balanced', classes=list(id2label.keys()),y=train_df[\"label\"])\n","class_weights = torch.tensor(class_weights).log1p()\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        # forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float().to(logits.device))\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Initialize our Trainer\n","model_folder_name = f\"{model_name_or_path.split('/')[-1]}-re-{re_task}-1\"\n","args = TrainingArguments(\n","    f\"training_logs/{model_folder_name}\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"no\",\n","    learning_rate=3e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=10,\n","    weight_decay=0.05,\n","    logging_steps=1,\n","    warmup_ratio=0.1,\n",")\n","\n","trainer = CustomTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset ,\n","    eval_dataset=eval_dataset ,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running training *****\n","  Num examples = 2140\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 340\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [340/340 04:18, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.803600</td>\n","      <td>0.726446</td>\n","      <td>0.666667</td>\n","      <td>0.694444</td>\n","      <td>0.783333</td>\n","      <td>0.700000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.523400</td>\n","      <td>0.530641</td>\n","      <td>0.666667</td>\n","      <td>0.694444</td>\n","      <td>0.783333</td>\n","      <td>0.700000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.437100</td>\n","      <td>0.348330</td>\n","      <td>0.768519</td>\n","      <td>0.750000</td>\n","      <td>0.850000</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.496300</td>\n","      <td>0.290834</td>\n","      <td>0.850000</td>\n","      <td>0.850000</td>\n","      <td>0.850000</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.156000</td>\n","      <td>0.350409</td>\n","      <td>0.922078</td>\n","      <td>0.944444</td>\n","      <td>0.916667</td>\n","      <td>0.900000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.155800</td>\n","      <td>0.387758</td>\n","      <td>0.833333</td>\n","      <td>0.904762</td>\n","      <td>0.833333</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.027900</td>\n","      <td>0.351495</td>\n","      <td>0.922078</td>\n","      <td>0.944444</td>\n","      <td>0.916667</td>\n","      <td>0.900000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.144400</td>\n","      <td>0.387710</td>\n","      <td>0.922078</td>\n","      <td>0.944444</td>\n","      <td>0.916667</td>\n","      <td>0.900000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.032100</td>\n","      <td>0.369964</td>\n","      <td>0.922078</td>\n","      <td>0.944444</td>\n","      <td>0.916667</td>\n","      <td>0.900000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.008800</td>\n","      <td>0.406460</td>\n","      <td>0.922078</td>\n","      <td>0.944444</td>\n","      <td>0.916667</td>\n","      <td>0.900000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]}],"source":["train_result = trainer.train()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to models/scibert_scivocab_uncased-re-Te_P-1\n","Configuration saved in models/scibert_scivocab_uncased-re-Te_P-1/config.json\n","Model weights saved in models/scibert_scivocab_uncased-re-Te_P-1/pytorch_model.bin\n","tokenizer config file saved in models/scibert_scivocab_uncased-re-Te_P-1/tokenizer_config.json\n","Special tokens file saved in models/scibert_scivocab_uncased-re-Te_P-1/special_tokens_map.json\n"]},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =       10.0\n","  total_flos               =  1280704GF\n","  train_loss               =     0.3096\n","  train_runtime            = 0:04:23.37\n","  train_samples            =       2140\n","  train_samples_per_second =     81.253\n","  train_steps_per_second   =      1.291\n"]}],"source":["metrics = train_result.metrics\n","metrics[\"train_samples\"] = len(train_dataset)\n","\n","trainer.save_model(f\"models/{model_folder_name}\")  # Saves the tokenizer too for easy upload\n","\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["*** Evaluate ***\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       10.0\n","  eval_accuracy           =        0.9\n","  eval_f1                 =     0.9221\n","  eval_loss               =     0.4065\n","  eval_precision          =     0.9444\n","  eval_recall             =     0.9167\n","  eval_runtime            = 0:00:00.10\n","  eval_samples            =         10\n","  eval_samples_per_second =     97.977\n","  eval_steps_per_second   =      9.798\n"]}],"source":["print(\"*** Evaluate ***\") \n","metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","metrics[\"eval_samples\"] = len(eval_dataset)\n","\n","trainer.log_metrics(\"eval\", metrics)\n","trainer.save_metrics(\"eval\", metrics)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 10\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Other       0.83      1.00      0.91         5\n","        TeRP       1.00      0.75      0.86         4\n","        TeCP       1.00      1.00      1.00         1\n","\n","    accuracy                           0.90        10\n","   macro avg       0.94      0.92      0.92        10\n","weighted avg       0.92      0.90      0.90        10\n","\n"]}],"source":["predictions, labels, _ = trainer.predict(eval_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 1720\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Other       0.95      0.82      0.88       793\n","        TeRP       0.89      0.94      0.91       794\n","        TeCP       0.71      1.00      0.83       133\n","\n","    accuracy                           0.89      1720\n","   macro avg       0.85      0.92      0.87      1720\n","weighted avg       0.90      0.89      0.89      1720\n","\n"]}],"source":["predictions, labels, _ = trainer.predict(train_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Model Treatment - Problem"]},{"cell_type":"markdown","metadata":{"id":"zP8CizR4xseH"},"source":["### Import data"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["re_task = \"Tr_P\""]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["import torch\n","# empty cuda\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>She had a postoperative CT scan that revealed ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[[ Her pain ]] was under good control with &lt;&lt; ...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3. &lt;&lt; Percocet &gt;&gt; , 5/325 , 1-2 tabs PO q4-6h ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Take &lt;&lt; codeine &gt;&gt; prescribed by PCP with food...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Take &lt;&lt; codeine &gt;&gt; prescribed by PCP with food...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2904</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2905</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2906</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2907</th>\n","      <td>The patient was told he could return to work a...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2908</th>\n","      <td>The patient was told he could return to work a...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2909 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   text  label\n","0     She had a postoperative CT scan that revealed ...      0\n","1     [[ Her pain ]] was under good control with << ...      4\n","2     3. << Percocet >> , 5/325 , 1-2 tabs PO q4-6h ...      1\n","3     Take << codeine >> prescribed by PCP with food...      0\n","4     Take << codeine >> prescribed by PCP with food...      0\n","...                                                 ...    ...\n","2904  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","2905  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","2906  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","2907  The patient was told he could return to work a...      1\n","2908  The patient was told he could return to work a...      0\n","\n","[2909 rows x 2 columns]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["relations_df = pd.read_csv(re_data_path + os.sep + f\"re_scibert_data_{re_task}.tsv\", sep=\"\\t\", header=None)\n","relations_df.columns = [\"text\", \"label\"]\n","label2id = {label: i for i, label in enumerate(relations_df[\"label\"].value_counts().index.tolist())}\n","id2label = {i: label for label, i in label2id.items()}\n","relations_df[\"label\"] = relations_df.label.map(label2id)\n","relations_df"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 2899\n"," }),\n"," Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 10\n"," }))"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# Build HuggingFace Dataset\n","\n","train_df, val_df = train_test_split(relations_df, train_size=None, shuffle=True, test_size=10, stratify=relations_df[\"label\"], random_state=42)\n","\n","features = datasets.Features({'text': datasets.Value(dtype='string'),\n"," 'label': datasets.ClassLabel(num_classes=len(id2label), names=list(id2label.values()))})\n","\n","train_dataset = Dataset.from_pandas(train_df, preserve_index=False, features=features)\n","eval_dataset = Dataset.from_pandas(val_df, preserve_index=False, features=features)\n","\n","label_list = train_dataset.features[\"label\"].names\n","num_labels = len(label_list)\n","\n","label_list = train_dataset.features[\"label\"].names\n","train_dataset, eval_dataset"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train_df labels: 0    1698\n","1     881\n","2     183\n","3      62\n","4      51\n","5      24\n","Name: label, dtype: int64\n","val_df labels: 0    6\n","1    3\n","2    1\n","Name: label, dtype: int64\n"]}],"source":["# check labels balance\n","print(f\"train_df labels: {train_df['label'].value_counts()}\")\n","print(f\"val_df labels: {val_df['label'].value_counts()}\")"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"re\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Other\",\n","    \"1\": \"TrAP\",\n","    \"2\": \"TrCP\",\n","    \"3\": \"TrNAP\",\n","    \"4\": \"TrIP\",\n","    \"5\": \"TrWP\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Other\": 0,\n","    \"TrAP\": 1,\n","    \"TrCP\": 2,\n","    \"TrIP\": 4,\n","    \"TrNAP\": 3,\n","    \"TrWP\": 5\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/jupyter/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n","Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load pretrained model and tokenizer\n","# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","# download model & vocab.\n","config = AutoConfig.from_pretrained(\n","    model_name_or_path,\n","    num_labels=num_labels,\n","    finetuning_task=\"re\",\n","    cache_dir=cache_dir,\n","    revision=model_revision,\n","    label2id=label2id,\n","    id2label=id2label\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\n","    tokenizer_name if tokenizer_name else model_name_or_path,\n","    # do_lower_case=do_lower_case,\n","    cache_dir=cache_dir,\n","    use_fast=use_fast_tokenizer,\n","    revision=model_revision,\n",")\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name_or_path,\n","    from_tf=bool(\".ckpt\" in model_name_or_path),\n","    config=config,\n","    cache_dir=cache_dir,\n","    revision=model_revision,\n",")"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61d035268bbd461eb8431b06792e1f70","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"302dd73bfa994124a7f4b75cf34f9898","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Sample 2619 of the training set: {'text': '<< placement >> for [[ a bile leak from the duct of Luschka ]] .', 'label': 1, 'input_ids': [102, 962, 962, 8005, 1374, 1374, 168, 260, 260, 106, 11994, 7254, 263, 111, 12794, 131, 26672, 255, 3776, 1901, 1901, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 456 of the training set: {'text': '7) GI : Ms. Pimental is an unfortunate 95 year old woman , status post a recent fall necessitating right ORIF of the hip and [[ right wrist fracture ]] << ORIF >> , who was in the hospital for this event , when it was noted that she had abdominal pain , distention and coffee ground emesis with worsening mental status and renal failure .', 'label': 1, 'input_ids': [102, 450, 546, 4706, 862, 2400, 205, 24464, 1092, 165, 130, 7878, 28649, 5508, 996, 4289, 10221, 422, 2726, 1422, 106, 2151, 3913, 1956, 17642, 2083, 17368, 30122, 131, 111, 8142, 137, 260, 260, 2083, 19438, 7368, 1901, 1901, 962, 962, 17368, 30122, 1374, 1374, 422, 975, 241, 121, 111, 2278, 168, 238, 2607, 422, 603, 256, 241, 3742, 198, 2281, 883, 7777, 2675, 422, 553, 13021, 137, 17677, 3443, 562, 2921, 190, 22096, 4476, 2726, 137, 4283, 3018, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 102 of the training set: {'text': 'The stomach had evidence of << prior surgery >> and [[ old blood in the stomach ]] .', 'label': 0, 'input_ids': [102, 111, 13410, 883, 1775, 131, 962, 962, 1979, 2797, 1374, 1374, 137, 260, 260, 4289, 1702, 121, 111, 13410, 1901, 1901, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n"]}],"source":["# Preprocessing the datasets\n","# Padding strategy\n","if pad_to_max_length:\n","    padding = \"max_length\"\n","else:\n","    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","    padding = False\n","\n","\n","def preprocess_function(examples):\n","    # Tokenize the texts\n","    return tokenizer(\n","        examples[\"text\"],\n","        padding=padding,\n","        max_length=max_seq_length,\n","        truncation=True,\n","    )\n","\n","\n","train_dataset = train_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on train dataset\",\n",")\n","eval_dataset = eval_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on validation dataset\",\n",")\n","\n","# Log a few random samples from the training set:\n","for index in random.sample(range(len(train_dataset)), 3):\n","    print(f\"Sample {index} of the training set: {train_dataset[index]}.\\n\")\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# Get the metric function\n","f1_metric = load_metric(\"f1\")\n","precision_metric = load_metric(\"precision\")\n","recall_metric = load_metric(\"recall\")\n","accuracy_metric = load_metric(\"accuracy\")\n","\n","# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","# predictions and label_ids field) and has to return a dictionary string to float.\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","\n","    metrics = {}\n","    metrics.update(f1_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(precision_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(recall_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(accuracy_metric.compute(predictions=preds, references=p.label_ids))\n","    return metrics"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","if pad_to_max_length:\n","    data_collator = default_data_collator\n","elif fp16:\n","    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","else:\n","    data_collator = None"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/plain":["'allenai/scibert_scivocab_uncased'"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["model_name_or_path"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.2504, 0.4372, 1.2921, 2.1740, 2.3489, 3.0508])\n"]}],"source":["# address class imbalance \n","import torch\n","from torch import nn\n","from transformers import Trainer\n","\n","class_weights = [len(train_df)/ (len(train_df[train_df[\"label\"] == i])*len(id2label)) for i in id2label.keys()]\n","# apply log to weights\n","class_weights = torch.tensor(class_weights).log1p()\n","print(class_weights)\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        # forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float().to(logits.device))\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["# Initialize our Trainer\n","model_folder_name = f\"{model_name_or_path.split('/')[-1]}-re-{re_task}-1\"\n","args = TrainingArguments(\n","    f\"training_logs/{model_folder_name}\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"no\",\n","    learning_rate=3e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=10,\n","    weight_decay=0.05,\n","    logging_steps=1,\n","    warmup_ratio=0.1,\n",")\n","\n","trainer = CustomTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset ,\n","    eval_dataset=eval_dataset ,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running training *****\n","  Num examples = 2899\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 460\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [460/460 11:20, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.198800</td>\n","      <td>1.019472</td>\n","      <td>0.766234</td>\n","      <td>0.766667</td>\n","      <td>0.777778</td>\n","      <td>0.700000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.802800</td>\n","      <td>0.658054</td>\n","      <td>0.542857</td>\n","      <td>0.562500</td>\n","      <td>0.541667</td>\n","      <td>0.600000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.392800</td>\n","      <td>0.515419</td>\n","      <td>0.592857</td>\n","      <td>0.625000</td>\n","      <td>0.583333</td>\n","      <td>0.700000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.222400</td>\n","      <td>0.605726</td>\n","      <td>0.643939</td>\n","      <td>0.666667</td>\n","      <td>0.625000</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.386600</td>\n","      <td>0.413809</td>\n","      <td>0.592857</td>\n","      <td>0.625000</td>\n","      <td>0.583333</td>\n","      <td>0.700000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.285700</td>\n","      <td>0.642960</td>\n","      <td>0.541667</td>\n","      <td>0.600000</td>\n","      <td>0.541667</td>\n","      <td>0.600000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.189900</td>\n","      <td>0.489113</td>\n","      <td>0.643939</td>\n","      <td>0.666667</td>\n","      <td>0.625000</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.299300</td>\n","      <td>0.421152</td>\n","      <td>0.643939</td>\n","      <td>0.666667</td>\n","      <td>0.625000</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.059400</td>\n","      <td>0.448395</td>\n","      <td>0.643939</td>\n","      <td>0.666667</td>\n","      <td>0.625000</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.041200</td>\n","      <td>0.488282</td>\n","      <td>0.643939</td>\n","      <td>0.666667</td>\n","      <td>0.625000</td>\n","      <td>0.800000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]}],"source":["train_result = trainer.train()"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to models/scibert_scivocab_uncased-re-Tr_P-1\n","Configuration saved in models/scibert_scivocab_uncased-re-Tr_P-1/config.json\n","Model weights saved in models/scibert_scivocab_uncased-re-Tr_P-1/pytorch_model.bin\n","tokenizer config file saved in models/scibert_scivocab_uncased-re-Tr_P-1/tokenizer_config.json\n","Special tokens file saved in models/scibert_scivocab_uncased-re-Tr_P-1/special_tokens_map.json\n"]},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =       10.0\n","  total_flos               =  3631377GF\n","  train_loss               =     0.5157\n","  train_runtime            = 0:11:21.94\n","  train_samples            =       2899\n","  train_samples_per_second =     42.511\n","  train_steps_per_second   =      0.675\n"]}],"source":["metrics = train_result.metrics\n","metrics[\"train_samples\"] = len(train_dataset)\n","\n","trainer.save_model(f\"models/{model_folder_name}\")  # Saves the tokenizer too for easy upload\n","\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["*** Evaluate ***\n"]},{"ename":"ValueError","evalue":"You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_6147/3145802687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*** Evaluate ***\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_samples\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2164\u001b[0m             \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2165\u001b[0;31m             \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2166\u001b[0m         )\n\u001b[1;32m   2167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2320\u001b[0m         \u001b[0mobserved_num_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2321\u001b[0m         \u001b[0;31m# Main evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2323\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         )\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"label\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2734\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m             raise ValueError(\n\u001b[0;32m-> 2736\u001b[0;31m                 \u001b[0;34m\"You should supply an encoding or a list of encodings to this method \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2737\u001b[0m                 \u001b[0;34mf\"that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']"]}],"source":["print(\"*** Evaluate ***\") \n","metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","metrics[\"eval_samples\"] = len(eval_dataset)\n","\n","trainer.log_metrics(\"eval\", metrics)\n","trainer.save_metrics(\"eval\", metrics)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 10\n","  Batch size = 64\n"]},{"ename":"ValueError","evalue":"Number of classes, 4, does not match size of target_names, 6. Try specifying the labels parameter","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_6147/171177838.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"predict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1877\u001b[0m             )\n\u001b[1;32m   1878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Number of classes, 4, does not match size of target_names, 6. Try specifying the labels parameter"]}],"source":["predictions, labels, _ = trainer.predict(eval_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 2327\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Other       0.87      0.67      0.76      1363\n","        TrAP       0.72      0.57      0.64       707\n","        TrCP       0.33      0.84      0.47       147\n","       TrNAP       0.32      0.90      0.48        50\n","        TrIP       0.25      0.95      0.40        41\n","        TrWP       0.38      0.95      0.55        19\n","\n","    accuracy                           0.66      2327\n","   macro avg       0.48      0.81      0.55      2327\n","weighted avg       0.76      0.66      0.69      2327\n","\n"]}],"source":["predictions, labels, _ = trainer.predict(train_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["# empty cuda cache\n","import torch\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Wed Jan 26 16:28:37 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   67C    P0    40W /  70W |   3404MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n","| N/A   49C    P0    35W /  70W |   1592MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      6147      C   /opt/conda/bin/python            3401MiB |\n","|    1   N/A  N/A      6147      C   /opt/conda/bin/python            1589MiB |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["## Model Problem - Problem"]},{"cell_type":"markdown","metadata":{"id":"zP8CizR4xseH"},"source":["### Import data"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["re_task = \"P_P\""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;&lt; C5-6 disc herniation &gt;&gt; with [[ cord compre...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;&lt; C5-6 disc herniation &gt;&gt; with cord compressi...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[[ C5-6 disc herniation ]] with &lt;&lt; cord compre...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>C5-6 disc herniation with &lt;&lt; cord compression ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[[ C5-6 disc herniation ]] with cord compressi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>10279</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10280</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10281</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10282</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10283</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10284 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                    text  label\n","0      << C5-6 disc herniation >> with [[ cord compre...      1\n","1      << C5-6 disc herniation >> with cord compressi...      1\n","2      [[ C5-6 disc herniation ]] with << cord compre...      0\n","3      C5-6 disc herniation with << cord compression ...      0\n","4      [[ C5-6 disc herniation ]] with cord compressi...      0\n","...                                                  ...    ...\n","10279  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","10280  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","10281  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","10282  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","10283  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","\n","[10284 rows x 2 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["relations_df = pd.read_csv(re_data_path + os.sep + f\"re_scibert_data_{re_task}.tsv\", sep=\"\\t\", header=None)\n","relations_df.columns = [\"text\", \"label\"]\n","label2id = {label: i for i, label in enumerate(relations_df[\"label\"].value_counts().index.tolist())}\n","id2label = {i: label for label, i in label2id.items()}\n","relations_df[\"label\"] = relations_df.label.map(label2id)\n","relations_df"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 10274\n"," }),\n"," Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 10\n"," }))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Build HuggingFace Dataset\n","\n","train_df, val_df = train_test_split(relations_df, train_size=None, shuffle=True, test_size=10, stratify=relations_df[\"label\"], random_state=42)\n","\n","features = datasets.Features({'text': datasets.Value(dtype='string'),\n"," 'label': datasets.ClassLabel(num_classes=len(id2label), names=list(id2label.values()))})\n","\n","train_dataset = Dataset.from_pandas(train_df, preserve_index=False, features=features)\n","eval_dataset = Dataset.from_pandas(val_df, preserve_index=False, features=features)\n","\n","label_list = train_dataset.features[\"label\"].names\n","num_labels = len(label_list)\n","\n","label_list = train_dataset.features[\"label\"].names\n","train_dataset, eval_dataset"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train_df labels: 0    9520\n","1     754\n","Name: label, dtype: int64\n","val_df labels: 0    9\n","1    1\n","Name: label, dtype: int64\n"]}],"source":["# check labels balance\n","print(f\"train_df labels: {train_df['label'].value_counts()}\")\n","print(f\"val_df labels: {val_df['label'].value_counts()}\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load pretrained model and tokenizer\n","# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","# download model & vocab.\n","config = AutoConfig.from_pretrained(\n","    model_name_or_path,\n","    num_labels=num_labels,\n","    finetuning_task=\"re\",\n","    cache_dir=cache_dir,\n","    revision=model_revision,\n","    label2id=label2id,\n","    id2label=id2label\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\n","    tokenizer_name if tokenizer_name else model_name_or_path,\n","    # do_lower_case=do_lower_case,\n","    cache_dir=cache_dir,\n","    use_fast=use_fast_tokenizer,\n","    revision=model_revision,\n",")\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name_or_path,\n","    from_tf=bool(\".ckpt\" in model_name_or_path),\n","    config=config,\n","    cache_dir=cache_dir,\n","    revision=model_revision,\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2efea7df0d1f42fdb8b581362101cf96","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/11 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c74f48f740c64449ac976358bfa3b374","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Sample 1824 of the training set: {'text': 'Gen : in << NAD >> , but winces in [[ pain ]] with movements of his back .', 'label': 0, 'input_ids': [102, 341, 862, 121, 962, 962, 8101, 1374, 1374, 422, 563, 8168, 565, 121, 260, 260, 2675, 1901, 1901, 190, 6873, 131, 1972, 1542, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 409 of the training set: {'text': 'Return to the ED for worsening chest pain , shortness of breath , nausea / vomiting , [[ fever ]] / chills , << weakness >> / dizziness/numbness , or any other problems .', 'label': 0, 'input_ids': [102, 3988, 147, 111, 777, 168, 22096, 8693, 2675, 422, 2001, 1076, 131, 10062, 422, 18816, 1352, 18644, 422, 260, 260, 10551, 1901, 1901, 1352, 7418, 4078, 30113, 422, 962, 962, 11688, 1374, 1374, 1352, 346, 10207, 2902, 1352, 541, 6605, 250, 422, 234, 843, 494, 2010, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 4506 of the training set: {'text': 'History of gastrointestinal intolerance to Tetracycline , Erythromycin , declomycin associated with edema and rash , Talwin associated with disorientation and gastrointestinal symptoms , Dilantin has caused [[ rash ]] and edema , morphine associated with << itching >> and gastrointestinal symptoms , Demerol unclear .', 'label': 0, 'input_ids': [102, 2999, 131, 5987, 9413, 27875, 147, 25365, 422, 8154, 16372, 422, 4168, 8563, 1111, 190, 12987, 137, 22679, 422, 6031, 9320, 1111, 190, 271, 24140, 137, 5987, 9413, 3049, 422, 3851, 1528, 30111, 434, 3111, 260, 260, 22679, 1901, 1901, 137, 12987, 422, 16589, 1111, 190, 962, 962, 256, 2215, 1374, 1374, 137, 5987, 9413, 3049, 422, 857, 2063, 30115, 7646, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n"]}],"source":["# Preprocessing the datasets\n","# Padding strategy\n","if pad_to_max_length:\n","    padding = \"max_length\"\n","else:\n","    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","    padding = False\n","\n","\n","def preprocess_function(examples):\n","    # Tokenize the texts\n","    return tokenizer(\n","        examples[\"text\"],\n","        padding=padding,\n","        max_length=max_seq_length,\n","        truncation=True,\n","    )\n","\n","\n","train_dataset = train_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on train dataset\",\n",")\n","eval_dataset = eval_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on validation dataset\",\n",")\n","\n","# Log a few random samples from the training set:\n","for index in random.sample(range(len(train_dataset)), 3):\n","    print(f\"Sample {index} of the training set: {train_dataset[index]}.\\n\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Get the metric function\n","f1_metric = load_metric(\"f1\")\n","precision_metric = load_metric(\"precision\")\n","recall_metric = load_metric(\"recall\")\n","accuracy_metric = load_metric(\"accuracy\")\n","\n","# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","# predictions and label_ids field) and has to return a dictionary string to float.\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","\n","    metrics = {}\n","    metrics.update(f1_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(precision_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(recall_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(accuracy_metric.compute(predictions=preds, references=p.label_ids))\n","    return metrics"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","if pad_to_max_length:\n","    data_collator = default_data_collator\n","elif fp16:\n","    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","else:\n","    data_collator = None"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["'allenai/scibert_scivocab_uncased'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model_name_or_path"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.4315, 2.0558])\n"]}],"source":["# address class imbalance \n","import torch\n","from torch import nn\n","from transformers import Trainer\n","\n","class_weights = [len(train_df)/ (len(train_df[train_df[\"label\"] == i])*len(id2label)) for i in id2label.keys()]\n","class_weights = torch.tensor(class_weights).log1p()\n","\n","print(class_weights)\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        # forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float().to(logits.device))\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Initialize our Trainer\n","model_folder_name = f\"{model_name_or_path.split('/')[-1]}-re-{re_task}-1\"\n","args = TrainingArguments(\n","    f\"training_logs/{model_folder_name}\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"no\",\n","    learning_rate=3e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=4,\n","    weight_decay=0.05,\n","    logging_steps=1,\n","    warmup_ratio=0.1,\n",")\n","\n","trainer = CustomTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset ,\n","    eval_dataset=eval_dataset ,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running training *****\n","  Num examples = 10274\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 644\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='644' max='644' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [644/644 09:37, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.241400</td>\n","      <td>0.375891</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.286300</td>\n","      <td>0.301649</td>\n","      <td>0.803922</td>\n","      <td>0.750000</td>\n","      <td>0.944444</td>\n","      <td>0.900000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.042900</td>\n","      <td>0.326441</td>\n","      <td>0.473684</td>\n","      <td>0.450000</td>\n","      <td>0.500000</td>\n","      <td>0.900000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.067200</td>\n","      <td>0.423108</td>\n","      <td>0.473684</td>\n","      <td>0.450000</td>\n","      <td>0.500000</td>\n","      <td>0.900000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]}],"source":["train_result = trainer.train()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to models/scibert_scivocab_uncased-re-P_P-1\n","Configuration saved in models/scibert_scivocab_uncased-re-P_P-1/config.json\n","Model weights saved in models/scibert_scivocab_uncased-re-P_P-1/pytorch_model.bin\n","tokenizer config file saved in models/scibert_scivocab_uncased-re-P_P-1/tokenizer_config.json\n","Special tokens file saved in models/scibert_scivocab_uncased-re-P_P-1/special_tokens_map.json\n"]},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =        4.0\n","  total_flos               =  2926007GF\n","  train_loss               =     0.2803\n","  train_runtime            = 0:09:41.74\n","  train_samples            =      10274\n","  train_samples_per_second =     70.642\n","  train_steps_per_second   =      1.107\n"]}],"source":["metrics = train_result.metrics\n","metrics[\"train_samples\"] = len(train_dataset)\n","\n","trainer.save_model(f\"models/{model_folder_name}\")  # Saves the tokenizer too for easy upload\n","\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 2057\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["*** Evaluate ***\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='195' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [33/33 01:10]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =        5.0\n","  eval_accuracy           =     0.8843\n","  eval_f1                 =     0.7109\n","  eval_loss               =     0.4348\n","  eval_precision          =     0.6696\n","  eval_recall             =     0.8217\n","  eval_runtime            = 0:00:11.14\n","  eval_samples            =       2057\n","  eval_samples_per_second =    184.569\n","  eval_steps_per_second   =      2.961\n"]}],"source":["print(\"*** Evaluate ***\") \n","metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","metrics[\"eval_samples\"] = len(eval_dataset)\n","\n","trainer.log_metrics(\"eval\", metrics)\n","trainer.save_metrics(\"eval\", metrics)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 2057\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Other       0.98      0.90      0.93      1906\n","         PIP       0.36      0.75      0.49       151\n","\n","    accuracy                           0.88      2057\n","   macro avg       0.67      0.82      0.71      2057\n","weighted avg       0.93      0.88      0.90      2057\n","\n"]}],"source":["predictions, labels, _ = trainer.predict(eval_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 8227\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Other       1.00      0.91      0.95      7623\n","         PIP       0.46      0.96      0.62       604\n","\n","    accuracy                           0.92      8227\n","   macro avg       0.73      0.93      0.79      8227\n","weighted avg       0.96      0.92      0.93      8227\n","\n"]}],"source":["predictions, labels, _ = trainer.predict(train_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"markdown","metadata":{},"source":["## Final Predictions"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["val_data_path = \"data/test\""]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 128/128 [00:00<00:00, 1059.87it/s]\n","100%|██████████| 128/128 [00:01<00:00, 86.04it/s]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>start_line</th>\n","      <th>concept_text_x</th>\n","      <th>concept_text_y</th>\n","      <th>concept_type_x</th>\n","      <th>concept_type_y</th>\n","      <th>start_word_number_x</th>\n","      <th>end_word_number_x</th>\n","      <th>start_word_number_y</th>\n","      <th>end_word_number_y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0001</td>\n","      <td>18</td>\n","      <td>Mesenteric angiograpm</td>\n","      <td>bleeding vessel</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0001</td>\n","      <td>18</td>\n","      <td>coil embolization</td>\n","      <td>bleeding vessel</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0001</td>\n","      <td>22</td>\n","      <td>cabg</td>\n","      <td>dm2</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>9</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001</td>\n","      <td>22</td>\n","      <td>cabg</td>\n","      <td>cad</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001</td>\n","      <td>22</td>\n","      <td>cabg</td>\n","      <td>DVT</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>15</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>18127</th>\n","      <td>0477</td>\n","      <td>109</td>\n","      <td>Saline wet to dry dressing</td>\n","      <td>penis and pelvis decubiti</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>18128</th>\n","      <td>0477</td>\n","      <td>109</td>\n","      <td>40% humidified oxygen</td>\n","      <td>penis and pelvis decubiti</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>15</td>\n","      <td>17</td>\n","      <td>10</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>18129</th>\n","      <td>0477</td>\n","      <td>109</td>\n","      <td>Fluconazole</td>\n","      <td>penis and pelvis decubiti</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>19</td>\n","      <td>19</td>\n","      <td>10</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>18130</th>\n","      <td>0477</td>\n","      <td>132</td>\n","      <td>a surgical procedure</td>\n","      <td>his constrictive pericarditis</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>18</td>\n","      <td>20</td>\n","      <td>13</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>18131</th>\n","      <td>0477</td>\n","      <td>133</td>\n","      <td>Followup cardiac studies</td>\n","      <td>his constrictive pericarditis</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>9</td>\n","      <td>11</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>18132 rows × 10 columns</p>\n","</div>"],"text/plain":["      filename  start_line              concept_text_x  \\\n","0         0001          18       Mesenteric angiograpm   \n","1         0001          18           coil embolization   \n","2         0001          22                        cabg   \n","3         0001          22                        cabg   \n","4         0001          22                        cabg   \n","...        ...         ...                         ...   \n","18127     0477         109  Saline wet to dry dressing   \n","18128     0477         109       40% humidified oxygen   \n","18129     0477         109                 Fluconazole   \n","18130     0477         132        a surgical procedure   \n","18131     0477         133    Followup cardiac studies   \n","\n","                      concept_text_y concept_type_x concept_type_y  \\\n","0                    bleeding vessel      treatment        problem   \n","1                    bleeding vessel      treatment        problem   \n","2                                dm2      treatment        problem   \n","3                                cad      treatment        problem   \n","4                                DVT      treatment        problem   \n","...                              ...            ...            ...   \n","18127      penis and pelvis decubiti      treatment        problem   \n","18128      penis and pelvis decubiti      treatment        problem   \n","18129      penis and pelvis decubiti      treatment        problem   \n","18130  his constrictive pericarditis      treatment        problem   \n","18131  his constrictive pericarditis           test        problem   \n","\n","       start_word_number_x  end_word_number_x  start_word_number_y  \\\n","0                        0                  1                    6   \n","1                        3                  4                    6   \n","2                       13                 13                    9   \n","3                       13                 13                   11   \n","4                       13                 13                   15   \n","...                    ...                ...                  ...   \n","18127                    0                  4                   10   \n","18128                   15                 17                   10   \n","18129                   19                 19                   10   \n","18130                   18                 20                   13   \n","18131                    0                  2                    9   \n","\n","       end_word_number_y  \n","0                      7  \n","1                      7  \n","2                      9  \n","3                     11  \n","4                     15  \n","...                  ...  \n","18127                 13  \n","18128                 13  \n","18129                 13  \n","18130                 15  \n","18131                 11  \n","\n","[18132 rows x 10 columns]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["text_files = glob.glob(val_data_path + os.sep + txt_folder_name + os.sep +  \"*.txt\")\n","filename = \"\"\n","df = pd.DataFrame()\n","for file in tqdm(text_files):\n","    with open(file, 'r') as f:\n","        text = f.read()\n","        # split lines\n","        lines = text.split('\\n')\n","        filename =[ file.split(\"/\")[-1].split(\".\")[0]] * len(lines)\n","        df = df.append(pd.DataFrame({\"text\": lines, \"filename\": filename, \"line_number\": range(len(lines))}), ignore_index=True)\n","\n","df = df.sort_values(by=[\"filename\", \"line_number\"])\n","# remove empty text lines\n","# df = df[df.text != \"\"]\n","# df = df.reset_index(drop=True)\n","\n","# add concepts\n","rel_df = pd.DataFrame()\n","for fname in tqdm(df[\"filename\"].unique()):\n","    concept_dict = parse_concept(val_data_path + os.sep + concept_folder_name + os.sep + fname + \".con\")\n","    \n","    concept_df = pd.DataFrame(concept_dict).drop(columns=[\"end_line\"])\n","    test_concept_df = concept_df[concept_df[\"concept_type\"] == \"test\"]\n","    problem_concept_df = concept_df[concept_df[\"concept_type\"] == \"problem\"]\n","    treatment_concept_df = concept_df[concept_df[\"concept_type\"] == \"treatment\"]\n","\n","    # class test --> problem\n","    test_problem_df = pd.merge(test_concept_df, problem_concept_df, how=\"inner\", on=\"start_line\")\n","\n","    # class treatment --> problem\n","    treatment_problem_df = pd.merge(treatment_concept_df, problem_concept_df, how=\"inner\", on=\"start_line\")\n","\n","    # class problem --> problem\n","    problem_problem_df = pd.merge(problem_concept_df, problem_concept_df, how=\"inner\", on=\"start_line\")\n","    problem_problem_df = problem_problem_df[problem_problem_df[\"concept_text_x\"] != problem_problem_df[\"concept_text_y\"]] # TODO: remove duplicates ?\n","\n","    tmp = pd.concat([test_problem_df, treatment_problem_df, problem_problem_df], axis=0)\n","    tmp[\"filename\"] = fname\n","    rel_df = rel_df.append(tmp, ignore_index=True)\n","            \n","rel_df = rel_df.sort_values(by=[\"filename\", \"start_line\"])\n","rel_df = rel_df.reset_index(drop=True)\n","\n","rel_df = rel_df[[ \"filename\", \"start_line\", \"concept_text_x\", \"concept_text_y\", \"concept_type_x\", \"concept_type_y\", \"start_word_number_x\", \"end_word_number_x\", \"start_word_number_y\", \"end_word_number_y\"]]\n","rel_df"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>start_line</th>\n","      <th>concept_text_x</th>\n","      <th>concept_text_y</th>\n","      <th>concept_type_x</th>\n","      <th>concept_type_y</th>\n","      <th>start_word_number_x</th>\n","      <th>end_word_number_x</th>\n","      <th>start_word_number_y</th>\n","      <th>end_word_number_y</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0001</td>\n","      <td>18</td>\n","      <td>Mesenteric angiograpm</td>\n","      <td>bleeding vessel</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>&lt;&lt; Mesenteric angiograpm &gt;&gt; w/ coil embolizati...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0001</td>\n","      <td>18</td>\n","      <td>coil embolization</td>\n","      <td>bleeding vessel</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>Mesenteric angiograpm w/ &lt;&lt; coil embolization ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0001</td>\n","      <td>22</td>\n","      <td>cabg</td>\n","      <td>dm2</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>HPI: Pt is a 71 y/o male with h/o [[ dm2 ]] , ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001</td>\n","      <td>22</td>\n","      <td>cabg</td>\n","      <td>cad</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>HPI: Pt is a 71 y/o male with h/o dm2 , [[ cad...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001</td>\n","      <td>22</td>\n","      <td>cabg</td>\n","      <td>DVT</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>HPI: Pt is a 71 y/o male with h/o dm2 , cad s/...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>18127</th>\n","      <td>0477</td>\n","      <td>109</td>\n","      <td>Saline wet to dry dressing</td>\n","      <td>penis and pelvis decubiti</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>13</td>\n","      <td>&lt;&lt; Saline wet to dry dressing &gt;&gt; changes three...</td>\n","    </tr>\n","    <tr>\n","      <th>18128</th>\n","      <td>0477</td>\n","      <td>109</td>\n","      <td>40% humidified oxygen</td>\n","      <td>penis and pelvis decubiti</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>15</td>\n","      <td>17</td>\n","      <td>10</td>\n","      <td>13</td>\n","      <td>Saline wet to dry dressing changes three times...</td>\n","    </tr>\n","    <tr>\n","      <th>18129</th>\n","      <td>0477</td>\n","      <td>109</td>\n","      <td>Fluconazole</td>\n","      <td>penis and pelvis decubiti</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>19</td>\n","      <td>19</td>\n","      <td>10</td>\n","      <td>13</td>\n","      <td>Saline wet to dry dressing changes three times...</td>\n","    </tr>\n","    <tr>\n","      <th>18130</th>\n","      <td>0477</td>\n","      <td>132</td>\n","      <td>a surgical procedure</td>\n","      <td>his constrictive pericarditis</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>18</td>\n","      <td>20</td>\n","      <td>13</td>\n","      <td>15</td>\n","      <td>The patient was seen at Ph University Of Medic...</td>\n","    </tr>\n","    <tr>\n","      <th>18131</th>\n","      <td>0477</td>\n","      <td>133</td>\n","      <td>Followup cardiac studies</td>\n","      <td>his constrictive pericarditis</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>9</td>\n","      <td>11</td>\n","      <td>&lt;&lt; Followup cardiac studies &gt;&gt; showed there wa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>18132 rows × 11 columns</p>\n","</div>"],"text/plain":["      filename  start_line              concept_text_x  \\\n","0         0001          18       Mesenteric angiograpm   \n","1         0001          18           coil embolization   \n","2         0001          22                        cabg   \n","3         0001          22                        cabg   \n","4         0001          22                        cabg   \n","...        ...         ...                         ...   \n","18127     0477         109  Saline wet to dry dressing   \n","18128     0477         109       40% humidified oxygen   \n","18129     0477         109                 Fluconazole   \n","18130     0477         132        a surgical procedure   \n","18131     0477         133    Followup cardiac studies   \n","\n","                      concept_text_y concept_type_x concept_type_y  \\\n","0                    bleeding vessel      treatment        problem   \n","1                    bleeding vessel      treatment        problem   \n","2                                dm2      treatment        problem   \n","3                                cad      treatment        problem   \n","4                                DVT      treatment        problem   \n","...                              ...            ...            ...   \n","18127      penis and pelvis decubiti      treatment        problem   \n","18128      penis and pelvis decubiti      treatment        problem   \n","18129      penis and pelvis decubiti      treatment        problem   \n","18130  his constrictive pericarditis      treatment        problem   \n","18131  his constrictive pericarditis           test        problem   \n","\n","       start_word_number_x  end_word_number_x  start_word_number_y  \\\n","0                        0                  1                    6   \n","1                        3                  4                    6   \n","2                       13                 13                    9   \n","3                       13                 13                   11   \n","4                       13                 13                   15   \n","...                    ...                ...                  ...   \n","18127                    0                  4                   10   \n","18128                   15                 17                   10   \n","18129                   19                 19                   10   \n","18130                   18                 20                   13   \n","18131                    0                  2                    9   \n","\n","       end_word_number_y                                               text  \n","0                      7  << Mesenteric angiograpm >> w/ coil embolizati...  \n","1                      7  Mesenteric angiograpm w/ << coil embolization ...  \n","2                      9  HPI: Pt is a 71 y/o male with h/o [[ dm2 ]] , ...  \n","3                     11  HPI: Pt is a 71 y/o male with h/o dm2 , [[ cad...  \n","4                     15  HPI: Pt is a 71 y/o male with h/o dm2 , cad s/...  \n","...                  ...                                                ...  \n","18127                 13  << Saline wet to dry dressing >> changes three...  \n","18128                 13  Saline wet to dry dressing changes three times...  \n","18129                 13  Saline wet to dry dressing changes three times...  \n","18130                 15  The patient was seen at Ph University Of Medic...  \n","18131                 11  << Followup cardiac studies >> showed there wa...  \n","\n","[18132 rows x 11 columns]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# make predict dataset\n","def preprocess_text(row):\n","    # find line\n","    line =  df[(df[\"filename\"] == row[\"filename\"]) & (df[\"line_number\"] == row[\"start_line\"]-1)][\"text\"].values[0]\n","    # line = line.lower()\n","    line = \" \".join(line.split()) # remove multiple spaces\n","\n","    concept_text_x = \"<< \"+ \" \".join(line.split()[row[\"start_word_number_x\"]:row[\"end_word_number_x\"]+1]) + \" >>\"\n","    concept_text_y = \"[[ \" + \" \".join(line.split()[row[\"start_word_number_y\"]:row[\"end_word_number_y\"]+1]) + \" ]]\"\n","    start_word_number_x = row[\"start_word_number_x\"]\n","    end_word_number_x = row[\"end_word_number_x\"]\n","    start_word_number_y = row[\"start_word_number_y\"]\n","    end_word_number_y = row[\"end_word_number_y\"]\n","\n","    if row[\"start_word_number_x\"] > row[\"start_word_number_y\"]:\n","        concept_text_x, concept_text_y = concept_text_y, concept_text_x\n","        start_word_number_x, start_word_number_y = start_word_number_y, start_word_number_x\n","        end_word_number_x, end_word_number_y = end_word_number_y, end_word_number_x\n","    text = \" \".join(line.split()[: start_word_number_x] + [concept_text_x] + line.split()[end_word_number_x+1: start_word_number_y] + [concept_text_y] + line.split()[end_word_number_y+1:])\n","\n","    row[\"text\"] = text\n","    return row\n","\n","predict_df = rel_df.apply(preprocess_text, axis=1)\n","predict_df"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["orig_predict_df = predict_df.copy()"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file models/scibert_scivocab_uncased-re-Tr_P-1/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"models/scibert_scivocab_uncased-re-Tr_P-1\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"re\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Other\",\n","    \"1\": \"TrAP\",\n","    \"2\": \"TrCP\",\n","    \"3\": \"TrNAP\",\n","    \"4\": \"TrIP\",\n","    \"5\": \"TrWP\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Other\": 0,\n","    \"TrAP\": 1,\n","    \"TrCP\": 2,\n","    \"TrIP\": 4,\n","    \"TrNAP\": 3,\n","    \"TrWP\": 5\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading weights file models/scibert_scivocab_uncased-re-Tr_P-1/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at models/scibert_scivocab_uncased-re-Tr_P-1.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","Didn't find file models/scibert_scivocab_uncased-re-Tr_P-1/added_tokens.json. We won't load it.\n","loading file models/scibert_scivocab_uncased-re-Tr_P-1/vocab.txt\n","loading file models/scibert_scivocab_uncased-re-Tr_P-1/tokenizer.json\n","loading file None\n","loading file models/scibert_scivocab_uncased-re-Tr_P-1/special_tokens_map.json\n","loading file models/scibert_scivocab_uncased-re-Tr_P-1/tokenizer_config.json\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["re_task = \"Tr_P\"\n","\n","if re_task == \"P_P\":\n","    # problem --> problem\n","    predict_df = orig_predict_df[(orig_predict_df[\"concept_type_x\"] == \"problem\") & (orig_predict_df[\"concept_type_y\"] == \"problem\")]\n","    label2id = {'Other': 0, 'PIP': 1}\n","elif re_task == \"Tr_P\":\n","    # treatment --> problem\n","    predict_df = orig_predict_df[(orig_predict_df[\"concept_type_x\"] == \"treatment\") & (orig_predict_df[\"concept_type_y\"] == \"problem\")]\n","    label2id = {'Other': 0, 'TrAP': 1, 'TrCP': 2, 'TrNAP': 3, 'TrIP': 4, 'TrWP': 5}\n","elif re_task == \"Te_P\":\n","    # test --> problem\n","    predict_df = orig_predict_df[(orig_predict_df[\"concept_type_x\"] == \"test\") & (orig_predict_df[\"concept_type_y\"] == \"problem\")]\n","    label2id = {'Other': 0, 'TeRP': 1, 'TeCP': 2}\n","id2label = {v: k for k, v in label2id.items()}\n","\n","\n","model_folder_name = f\"{model_name_or_path.split('/')[-1]}-re-{re_task}-1\"\n","model = AutoModelForSequenceClassification.from_pretrained(f\"models/{model_folder_name}\", label2id=label2id, id2label=id2label)\n","tokenizer = AutoTokenizer.from_pretrained(f\"models/{model_folder_name}\")\n","\n","# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","if pad_to_max_length:\n","    data_collator = default_data_collator\n","elif fp16:\n","    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","else:\n","    data_collator = None\n","    \n","# Initialize our Trainer\n","args = TrainingArguments(\n","    f\"training_logs/{model_folder_name}\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"no\",\n","    learning_rate=1e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=5,\n","    weight_decay=0.05,\n","    logging_steps=1,\n","    warmup_ratio=0.1,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    # args=args,\n","    # train_dataset=train_dataset ,\n","    # eval_dataset=eval_dataset ,\n","    # compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bc3e857a2a64e94ad1bbd3f6a2dfd12","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on prediction dataset:   0%|          | 0/4 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['filename', 'start_line', 'concept_text_x', 'concept_text_y', 'concept_type_x', 'concept_type_y', 'start_word_number_x', 'end_word_number_x', 'start_word_number_y', 'end_word_number_y', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 3521\n","})"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["# Preprocessing the datasets\n","# Padding strategy\n","if pad_to_max_length:\n","    padding = \"max_length\"\n","else:\n","    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","    padding = False\n","\n","\n","def preprocess_function(examples):\n","    # Tokenize the texts\n","    return tokenizer(\n","        examples[\"text\"],\n","        padding=padding,\n","        max_length=max_seq_length,\n","        truncation=True,\n","    )\n","\n","predict_dataset = Dataset.from_pandas(predict_df, preserve_index=False)\n","# predict_dataset = predict_dataset.select(range(10))\n","predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","predict_dataset"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: start_word_number_y, start_word_number_x, concept_text_y, start_line, text, end_word_number_y, end_word_number_x, concept_type_x, concept_text_x, concept_type_y, filename.\n","***** Running Prediction *****\n","  Num examples = 3521\n","  Batch size = 16\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='221' max='221' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [221/221 00:22]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["3521"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","len(predictions)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>start_line</th>\n","      <th>concept_text_x</th>\n","      <th>concept_text_y</th>\n","      <th>concept_type_x</th>\n","      <th>concept_type_y</th>\n","      <th>start_word_number_x</th>\n","      <th>end_word_number_x</th>\n","      <th>start_word_number_y</th>\n","      <th>end_word_number_y</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0001</td>\n","      <td>18</td>\n","      <td>Mesenteric angiograpm</td>\n","      <td>bleeding vessel</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>TrAP</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0001</td>\n","      <td>18</td>\n","      <td>coil embolization</td>\n","      <td>bleeding vessel</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>TrAP</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0001</td>\n","      <td>22</td>\n","      <td>cabg</td>\n","      <td>dm2</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001</td>\n","      <td>22</td>\n","      <td>cabg</td>\n","      <td>cad</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001</td>\n","      <td>22</td>\n","      <td>cabg</td>\n","      <td>DVT</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>18127</th>\n","      <td>0477</td>\n","      <td>109</td>\n","      <td>Saline wet to dry dressing</td>\n","      <td>penis and pelvis decubiti</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>13</td>\n","      <td>TrAP</td>\n","    </tr>\n","    <tr>\n","      <th>18128</th>\n","      <td>0477</td>\n","      <td>109</td>\n","      <td>40% humidified oxygen</td>\n","      <td>penis and pelvis decubiti</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>15</td>\n","      <td>17</td>\n","      <td>10</td>\n","      <td>13</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>18129</th>\n","      <td>0477</td>\n","      <td>109</td>\n","      <td>Fluconazole</td>\n","      <td>penis and pelvis decubiti</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>19</td>\n","      <td>19</td>\n","      <td>10</td>\n","      <td>13</td>\n","      <td>TrAP</td>\n","    </tr>\n","    <tr>\n","      <th>18130</th>\n","      <td>0477</td>\n","      <td>132</td>\n","      <td>a surgical procedure</td>\n","      <td>his constrictive pericarditis</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>18</td>\n","      <td>20</td>\n","      <td>13</td>\n","      <td>15</td>\n","      <td>TrAP</td>\n","    </tr>\n","    <tr>\n","      <th>18131</th>\n","      <td>0477</td>\n","      <td>133</td>\n","      <td>Followup cardiac studies</td>\n","      <td>his constrictive pericarditis</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>9</td>\n","      <td>11</td>\n","      <td>TeRP</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>18132 rows × 11 columns</p>\n","</div>"],"text/plain":["      filename  start_line              concept_text_x  \\\n","0         0001          18       Mesenteric angiograpm   \n","1         0001          18           coil embolization   \n","2         0001          22                        cabg   \n","3         0001          22                        cabg   \n","4         0001          22                        cabg   \n","...        ...         ...                         ...   \n","18127     0477         109  Saline wet to dry dressing   \n","18128     0477         109       40% humidified oxygen   \n","18129     0477         109                 Fluconazole   \n","18130     0477         132        a surgical procedure   \n","18131     0477         133    Followup cardiac studies   \n","\n","                      concept_text_y concept_type_x concept_type_y  \\\n","0                    bleeding vessel      treatment        problem   \n","1                    bleeding vessel      treatment        problem   \n","2                                dm2      treatment        problem   \n","3                                cad      treatment        problem   \n","4                                DVT      treatment        problem   \n","...                              ...            ...            ...   \n","18127      penis and pelvis decubiti      treatment        problem   \n","18128      penis and pelvis decubiti      treatment        problem   \n","18129      penis and pelvis decubiti      treatment        problem   \n","18130  his constrictive pericarditis      treatment        problem   \n","18131  his constrictive pericarditis           test        problem   \n","\n","       start_word_number_x  end_word_number_x  start_word_number_y  \\\n","0                        0                  1                    6   \n","1                        3                  4                    6   \n","2                       13                 13                    9   \n","3                       13                 13                   11   \n","4                       13                 13                   15   \n","...                    ...                ...                  ...   \n","18127                    0                  4                   10   \n","18128                   15                 17                   10   \n","18129                   19                 19                   10   \n","18130                   18                 20                   13   \n","18131                    0                  2                    9   \n","\n","       end_word_number_y prediction  \n","0                      7       TrAP  \n","1                      7       TrAP  \n","2                      9      Other  \n","3                     11      Other  \n","4                     15      Other  \n","...                  ...        ...  \n","18127                 13       TrAP  \n","18128                 13      Other  \n","18129                 13       TrAP  \n","18130                 15       TrAP  \n","18131                 11       TeRP  \n","\n","[18132 rows x 11 columns]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["predict_df[\"prediction\"] = [id2label[label] for label in predictions]\n","rel_df.loc[predict_df.index, \"prediction\"] = predict_df[\"prediction\"]\n","rel_df"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/plain":["Other    14163\n","TrAP      1207\n","PIP       1161\n","TeRP      1140\n","TrCP       161\n","TeCP       150\n","TrIP        82\n","TrNAP       54\n","TrWP        14\n","Name: prediction, dtype: int64"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["rel_df[\"prediction\"].value_counts()\n"]},{"cell_type":"markdown","metadata":{},"source":["you can now set another re_task"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["18132it [00:01, 14043.15it/s]\n"]}],"source":["# for each file create <filename>.con\n","os.makedirs(val_data_path + os.sep + rel_folder_name, exist_ok=True)\n","# empty folder if exists\n","files = glob.glob(val_data_path + os.sep + rel_folder_name + os.sep + \"*.rel\")\n","for file in files:\n","    os.remove(file)\n","\n","for i, row in tqdm(rel_df.iterrows()):\n","    filename = row[\"filename\"]\n","    concept_text_x = row[\"concept_text_x\"].lower()\n","    concept_text_y = row[\"concept_text_y\"].lower()\n","    concept_type_x = row[\"concept_type_x\"]\n","    concept_type_y = row[\"concept_type_y\"]\n","    start_word_number_x = row[\"start_word_number_x\"]\n","    end_word_number_x = row[\"end_word_number_x\"]\n","    start_word_number_y = row[\"start_word_number_y\"]\n","    end_word_number_y = row[\"end_word_number_y\"]\n","    line_number = row[\"start_line\"]\n","    prediction = row[\"prediction\"]\n","    if prediction != \"Other\":\n","        with open(val_data_path + os.sep + rel_folder_name + os.sep + filename + \".rel\", \"a\") as f:\n","            # fill like this c=\"pefusion imaging\" 19:6 19:7||r=\"TeRP\"||c=\"perfusion defects\" 19:12 19:13\n","            f.write(\n","                f\"c=\\\"{concept_text_x}\\\" {line_number}:{start_word_number_x} {line_number}:{end_word_number_x}||r=\\\"{prediction}\\\"||c=\\\"{concept_text_y}\\\" {line_number}:{start_word_number_y} {line_number}:{end_word_number_y}\\n\"\n","            )\n","    \n"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":["{'0046', '0154', '0182', '0226', '0265', '0274', '0326', '0413'}"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["rel_files = glob.glob(val_data_path + os.sep + rel_folder_name + os.sep + \"*.rel\")\n","rel_files = [f.split(os.sep)[-1][:-4] for f in rel_files]\n","txt_files = [f.split(os.sep)[-1][:-4] for f in text_files]\n","# find missing files\n","missing_files = set(txt_files) - set(rel_files)\n","missing_files\n"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["# create empty files for missing files\n","for f in missing_files:\n","    with open(val_data_path + os.sep + rel_folder_name + os.sep + f + \".rel\", \"w\") as f:\n","        f.write(\"\")"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/plain":["set()"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["rel_files = glob.glob(val_data_path + os.sep + rel_folder_name + os.sep + \"*.rel\")\n","rel_files = [f.split(os.sep)[-1][:-4] for f in rel_files]\n","txt_files = [f.split(os.sep)[-1][:-4] for f in text_files]\n","# find missing files\n","missing_files = set(txt_files) - set(rel_files)\n","missing_files"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","c=\"mesenteric angiograpm\" 18:0 18:1||r=\"TrAP\"||c=\"bleeding vessel\" 18:6 18:7\n","c=\"coil embolization\" 18:3 18:4||r=\"TrAP\"||c=\"bleeding vessel\" 18:6 18:7\n","c=\"long term anti-coagulation\" 22:17 22:19||r=\"TrAP\"||c=\"dvt\" 22:15 22:15\n","c=\"long term anti-coagulation\" 22:17 22:19||r=\"TrAP\"||c=\"pe\" 22:15 22:15\n","c=\"ciprofloxacin\" 24:5 24:5||r=\"TrAP\"||c=\"a uti\" 24:7 24:8\n","c=\"an initial dre\" 27:1 27:3||r=\"TeRP\"||c=\"clot\" 27:6 27:6\n","c=\"3v-cabg\" 34:3 34:3||r=\"TrAP\"||c=\"cad\" 34:1 34:1\n","c=\"ef\" 35:3 35:3||r=\"TeRP\"||c=\"chf\" 35:1 35:1\n","c=\"1996 cardiac\" 36:6 36:7||r=\"TrCP\"||c=\"right parietal intracranial bleeding\" 36:1 36:4\n","c=\"s/p\" 38:4 38:4||r=\"TrAP\"||c=\"sinus node dysfunction\" 38:1 38:3\n","c=\"lifetime\" 39:13 39:13||r=\"TrAP\"||c=\"subsequent pe\" 39:8 39:9\n","c=\"drainage\" 44:4 44:4||r=\"TrAP\"||c=\"r intracerebral hemorrhage\" 44:1 44:3\n","c=\"study\" 85:2 85:2||r=\"TeRP\"||c=\"gi bleeding\" 85:0 85:1\n","c=\"increased tracer activity\" 87:2 87:4||r=\"PIP\"||c=\"active\" 87:22 87:22\n","c=\"increased tracer activity\" 87:2 87:4||r=\"PIP\"||c=\"bleeding\" 87:23 87:23\n","c=\"active\" 87:22 87:22||r=\"PIP\"||c=\"bleeding\" 87:23 87:23\n","c=\"left adrenal fat-containing lesion\" 92:4 92:7||r=\"PIP\"||c=\"a myelolipoma\" 92:10 92:11\n","c=\"a myelolipoma\" 92:10 92:11||r=\"PIP\"||c=\"left adrenal fat-containing lesion\" 92:4 92:7\n","c=\"hypodense lesion within the caudate lobe\" 93:4 93:9||r=\"PIP\"||c=\"liver\" 93:12 93:12\n","c=\"liver\" 93:12 93:12||r=\"PIP\"||c=\"hypodense lesion within the caudate lobe\" 93:4 93:9\n","c=\"followup ct of the chest\" 96:13 96:17||r=\"TeCP\"||c=\"a primary malignancy\" 96:9 96:11\n","c=\"followup ct of the chest\" 96:13 96:17||r=\"TeCP\"||c=\"this finding\" 96:28 96:29\n","c=\"this finding\" 96:28 96:29||r=\"PIP\"||c=\"a primary malignancy\" 96:9 96:11\n","c=\"brbpr\" 100:1 100:1||r=\"PIP\"||c=\"his lower gi bleed\" 100:12 100:15\n","c=\"an angiography\" 101:0 101:1||r=\"TeRP\"||c=\"bleeding in two vessels\" 101:3 101:6\n","c=\"his coumadin\" 103:0 103:1||r=\"TrAP\"||c=\"his acute bleed\" 103:8 103:10\n","c=\"a flex sigmoidoscopy\" 104:4 104:6||r=\"TeRP\"||c=\"old blood in the rectal vault\" 104:14 104:19\n","c=\"a flex sigmoidoscopy\" 104:4 104:6||r=\"TeRP\"||c=\"bleeding\" 104:25 104:25\n","c=\"a colonoscopy\" 105:10 105:11||r=\"TeCP\"||c=\"further bleeding\" 105:15 105:16\n","c=\"a repeat colonoscopy\" 108:4 108:6||r=\"TeRP\"||c=\"moderate ulcerative colitis\" 108:15 108:17\n","c=\"a repeat colonoscopy\" 108:4 108:6||r=\"TeRP\"||c=\"polyps\" 108:20 108:20\n","c=\"a repeat colonoscopy\" 108:4 108:6||r=\"TeRP\"||c=\"8 mm ulcer\" 108:23 108:25\n","c=\"the patient 's asacol\" 110:4 110:7||r=\"TrAP\"||c=\"ulcerative colitis\" 110:1 110:2\n","c=\"his home regimen\" 111:8 111:10||r=\"TrAP\"||c=\"cad/chf\" 111:1 111:1\n","c=\"his home regimen\" 111:8 111:10||r=\"TrAP\"||c=\"his acute gi bleed\" 111:20 111:23\n","c=\"his home insulin regimen\" 114:8 114:11||r=\"TrAP\"||c=\"dm-2\" 114:1 114:1\n","c=\"long-term anticoagulation\" 118:29 118:30||r=\"TrAP\"||c=\"recurrent dvts\" 118:15 118:16\n","c=\"long-term anticoagulation\" 118:29 118:30||r=\"TrAP\"||c=\"a pe\" 118:19 118:20\n","c=\"v. winchester filter\" 119:14 119:16||r=\"TrAP\"||c=\"bleeding\" 119:8 119:8\n","c=\"pioglitazone\" 149:1 149:1||r=\"TrAP\"||c=\"dm\" 149:20 149:20\n","c=\"mesalamine\" 150:1 150:1||r=\"TrAP\"||c=\"ulcerative colitis\" 150:35 150:36\n","c=\"mesalamine\" 150:1 150:1||r=\"TrAP\"||c=\"severe flares\" 150:39 150:40\n","c=\"rectal bleeding\" 160:0 160:1||r=\"PIP\"||c=\"mesenteric\" 160:4 160:4\n"]}],"source":["!cat data/test/rel/0001.rel"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","  adding: data/test/rel/ (stored 0%)\n","  adding: data/test/rel/0233.rel (deflated 64%)\n","  adding: data/test/rel/0463.rel (deflated 65%)\n","  adding: data/test/rel/0427.rel (deflated 73%)\n","  adding: data/test/rel/0058.rel (deflated 72%)\n","  adding: data/test/rel/0460.rel (deflated 74%)\n","  adding: data/test/rel/0265.rel (stored 0%)\n","  adding: data/test/rel/0214.rel (deflated 66%)\n","  adding: data/test/rel/0021.rel (deflated 68%)\n","  adding: data/test/rel/0415.rel (deflated 51%)\n","  adding: data/test/rel/0005.rel (deflated 67%)\n","  adding: data/test/rel/0281.rel (deflated 69%)\n","  adding: data/test/rel/0037.rel (deflated 70%)\n","  adding: data/test/rel/0129.rel (deflated 64%)\n","  adding: data/test/rel/0462.rel (deflated 60%)\n","  adding: data/test/rel/0326.rel (stored 0%)\n","  adding: data/test/rel/0130.rel (deflated 71%)\n","  adding: data/test/rel/0416.rel (deflated 59%)\n","  adding: data/test/rel/0270.rel (deflated 79%)\n","  adding: data/test/rel/0110.rel (deflated 60%)\n","  adding: data/test/rel/0451.rel (deflated 49%)\n","  adding: data/test/rel/0293.rel (deflated 56%)\n","  adding: data/test/rel/0062.rel (deflated 71%)\n","  adding: data/test/rel/0042.rel (deflated 71%)\n","  adding: data/test/rel/0466.rel (deflated 58%)\n","  adding: data/test/rel/0089.rel (deflated 64%)\n","  adding: data/test/rel/0477.rel (deflated 63%)\n","  adding: data/test/rel/0226.rel (stored 0%)\n","  adding: data/test/rel/0338.rel (deflated 70%)\n","  adding: data/test/rel/0313.rel (deflated 72%)\n","  adding: data/test/rel/0086.rel (deflated 64%)\n","  adding: data/test/rel/0454.rel (deflated 71%)\n","  adding: data/test/rel/0358.rel (deflated 64%)\n","  adding: data/test/rel/0009.rel (deflated 69%)\n","  adding: data/test/rel/0181.rel (deflated 67%)\n","  adding: data/test/rel/0261.rel (deflated 66%)\n","  adding: data/test/rel/0374.rel (deflated 52%)\n","  adding: data/test/rel/0413.rel (stored 0%)\n","  adding: data/test/rel/0393.rel (deflated 75%)\n","  adding: data/test/rel/0081.rel (deflated 76%)\n","  adding: data/test/rel/0436.rel (deflated 71%)\n","  adding: data/test/rel/0106.rel (deflated 71%)\n","  adding: data/test/rel/0470.rel (deflated 52%)\n","  adding: data/test/rel/0002.rel (deflated 66%)\n","  adding: data/test/rel/0257.rel (deflated 72%)\n","  adding: data/test/rel/0254.rel (deflated 64%)\n","  adding: data/test/rel/0250.rel (deflated 66%)\n","  adding: data/test/rel/0274.rel (stored 0%)\n","  adding: data/test/rel/0378.rel (deflated 38%)\n","  adding: data/test/rel/0437.rel (deflated 74%)\n","  adding: data/test/rel/0346.rel (deflated 69%)\n","  adding: data/test/rel/0448.rel (deflated 50%)\n","  adding: data/test/rel/0118.rel (deflated 41%)\n","  adding: data/test/rel/0097.rel (deflated 60%)\n","  adding: data/test/rel/0109.rel (deflated 55%)\n","  adding: data/test/rel/0253.rel (deflated 67%)\n","  adding: data/test/rel/0443.rel (deflated 10%)\n","  adding: data/test/rel/0321.rel (deflated 72%)\n","  adding: data/test/rel/0298.rel (deflated 69%)\n","  adding: data/test/rel/0290.rel (deflated 73%)\n","  adding: data/test/rel/0209.rel (deflated 70%)\n","  adding: data/test/rel/0010.rel (deflated 60%)\n","  adding: data/test/rel/0078.rel (deflated 58%)\n","  adding: data/test/rel/0286.rel (deflated 66%)\n","  adding: data/test/rel/0221.rel (deflated 71%)\n","  adding: data/test/rel/0014.rel (deflated 68%)\n","  adding: data/test/rel/0277.rel (deflated 68%)\n","  adding: data/test/rel/0046.rel (stored 0%)\n","  adding: data/test/rel/0310.rel (deflated 72%)\n","  adding: data/test/rel/0154.rel (stored 0%)\n","  adding: data/test/rel/0442.rel (deflated 67%)\n","  adding: data/test/rel/0001.rel (deflated 67%)\n","  adding: data/test/rel/0125.rel (deflated 71%)\n","  adding: data/test/rel/0406.rel (deflated 71%)\n","  adding: data/test/rel/0431.rel (deflated 60%)\n","  adding: data/test/rel/0476.rel (deflated 74%)\n","  adding: data/test/rel/0114.rel (deflated 12%)\n","  adding: data/test/rel/0461.rel (deflated 70%)\n","  adding: data/test/rel/0177.rel (deflated 69%)\n","  adding: data/test/rel/0190.rel (deflated 63%)\n","  adding: data/test/rel/0285.rel (deflated 71%)\n","  adding: data/test/rel/0178.rel (deflated 71%)\n","  adding: data/test/rel/0401.rel (deflated 67%)\n","  adding: data/test/rel/0473.rel (deflated 62%)\n","  adding: data/test/rel/0205.rel (deflated 68%)\n","  adding: data/test/rel/0419.rel (deflated 72%)\n","  adding: data/test/rel/0090.rel (deflated 72%)\n","  adding: data/test/rel/0026.rel (deflated 62%)\n","  adding: data/test/rel/0306.rel (deflated 69%)\n","  adding: data/test/rel/0057.rel (deflated 76%)\n","  adding: data/test/rel/0241.rel (deflated 59%)\n","  adding: data/test/rel/0418.rel (deflated 55%)\n","  adding: data/test/rel/0238.rel (deflated 71%)\n","  adding: data/test/rel/0182.rel (stored 0%)\n","  adding: data/test/rel/0297.rel (deflated 67%)\n","  adding: data/test/rel/0094.rel (deflated 65%)\n","  adding: data/test/rel/0440.rel (deflated 18%)\n","  adding: data/test/rel/0113.rel (deflated 74%)\n","  adding: data/test/rel/0301.rel (deflated 71%)\n","  adding: data/test/rel/0439.rel (deflated 68%)\n","  adding: data/test/rel/0074.rel (deflated 64%)\n","  adding: data/test/rel/0370.rel (deflated 69%)\n","  adding: data/test/rel/0245.rel (deflated 70%)\n","  adding: data/test/rel/0029.rel (deflated 58%)\n","  adding: data/test/rel/0434.rel (deflated 70%)\n","  adding: data/test/rel/0050.rel (deflated 72%)\n","  adding: data/test/rel/0102.rel (deflated 63%)\n","  adding: data/test/rel/0428.rel (deflated 62%)\n","  adding: data/test/rel/0353.rel (deflated 70%)\n","  adding: data/test/rel/0054.rel (deflated 66%)\n","  adding: data/test/rel/0098.rel (deflated 70%)\n","  adding: data/test/rel/0452.rel (deflated 53%)\n","  adding: data/test/rel/0073.rel (deflated 62%)\n","  adding: data/test/rel/0389.rel (deflated 60%)\n","  adding: data/test/rel/0365.rel (deflated 58%)\n","  adding: data/test/rel/0165.rel (deflated 71%)\n","  adding: data/test/rel/0149.rel (deflated 76%)\n","  adding: data/test/rel/0318.rel (deflated 70%)\n","  adding: data/test/rel/0322.rel (deflated 67%)\n","  adding: data/test/rel/0262.rel (deflated 69%)\n","  adding: data/test/rel/0458.rel (deflated 69%)\n","  adding: data/test/rel/0142.rel (deflated 73%)\n","  adding: data/test/rel/0425.rel (deflated 55%)\n","  adding: data/test/rel/0349.rel (deflated 55%)\n","  adding: data/test/rel/0465.rel (deflated 55%)\n","  adding: data/test/rel/0412.rel (deflated 67%)\n","  adding: data/test/rel/0201.rel (deflated 72%)\n","  adding: data/test/rel/0449.rel (deflated 39%)\n","  adding: data/test/rel/0145.rel (deflated 70%)\n"]}],"source":["!zip -r scibert-test-rel-2-sep.zip data/test/rel/"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"concepts_ner.ipynb","provenance":[]},"interpreter":{"hash":"e2a6cb9a7be66dff740cce83a34a934469b6132ecbf60107b2f98f6b92406a0e"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"0031c8a4147f4c5c85c425ca129a5c51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"01907e7b03da4dfdac85aa0324325c8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0618d990083e4e3eb2a7d74908128b31":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07bbc987a9b74bc584d6a57a65893a5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"125ddf0cef75420db6739a63d2f4d680":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e4d0aca5e5a465191369dc629dbe373","IPY_MODEL_aa416acfbe544e449ea909815de4b49f","IPY_MODEL_fdb76d6039dc4c69a86e966d2a058ab1"],"layout":"IPY_MODEL_e6510e86dfaa42f6886ba1400ca7ca1a"}},"12691f6dfd2a4ce8aeabe5cb9d93c8df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8cdfdd2de87468da1d381c6912e2b82","IPY_MODEL_a1efb9f259a34ec9be759135ef31fea2","IPY_MODEL_17898f9c65b54d3b9e2d7ab9210530f4"],"layout":"IPY_MODEL_f24e05a562014456b2e67f00fa6568bd"}},"17898f9c65b54d3b9e2d7ab9210530f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a33891d35724498a89d810da84749cff","placeholder":"​","style":"IPY_MODEL_ff6a44e5da994f1b9145fc69564f233f","value":" 5970/5970 [00:04&lt;00:00, 1493.50ex/s]"}},"18b75a441c5b4eb7911e4584101117a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74c397ef7ba44c0e8890373029d08bd5","placeholder":"​","style":"IPY_MODEL_ce790bd0d45d456f8c6fcd934985fea0","value":" 1/1 [00:00&lt;00:00, 25.61it/s]"}},"1da3cf5bd32e4d29a2b9f99dfe28216a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0618d990083e4e3eb2a7d74908128b31","placeholder":"​","style":"IPY_MODEL_761b526d690346f1bca69b8cfece1206","value":"100%"}},"1f93a0c9add84c7ebba08fae104b24b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"25dab51ddfd1471199bd6b32fbb6fa76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cac560d62e243dcb5e6df1498a2ef77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e4d0aca5e5a465191369dc629dbe373":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b03358ce39745fe9cae3cc2541f9296","placeholder":"​","style":"IPY_MODEL_c53842a95eab47d495f124932b11e45d","value":"Downloading: "}},"2f0879ec781e4c5bb3bcd9c542d759c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f60f22da9424936914b93bafaf1cb55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5de35062707d4d7eb09bf1884fa6c4d8","max":227845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1f93a0c9add84c7ebba08fae104b24b6","value":227845}},"312d061ff23b43f082436fe0fa977b37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"334126ec80dc48acaca342f4f5ad93cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"355f639079654aaa828992c022263d39":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36369643c8c149c7b91acceebfbc6be8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3906f4df4bf443cc8c3f3d40cb80be83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e364ad3e8146466ab156d33b3926f00c","max":1990,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1879aae6e0645b59a054afd3ce72184","value":1990}},"3ab2cf683dc349f7b56f9622c3ddd798":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3efe3b45a309437880e0a5ad9c58be68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_663f407f9f704aa1ae77f28664627b33","placeholder":"​","style":"IPY_MODEL_6f4ca626282a4fb99e36f0f891b6f179","value":" 1/1 [00:00&lt;00:00, 31.55it/s]"}},"4b03358ce39745fe9cae3cc2541f9296":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c3c8c3323ef4120b682052ab9befb10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d54f94b8d9443fd91d10bd0711b3cc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8a72fb4797c464a8a35d03f22710bb8","placeholder":"​","style":"IPY_MODEL_693fef2351804fe3891ea0b40c12c9d0","value":" 385/385 [00:00&lt;00:00, 11.5kB/s]"}},"50e5a452eb4b457f9002660afb3b34c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51908f695fc54c4287b99ad0255605ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51e017bb913047eca450f712bd3cfe66":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"544c7e05491a41bbaa8624b6fd75a415":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54af4e9de2944e038c72803865280256":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_961292b1eff14bf380638899def43491","placeholder":"​","style":"IPY_MODEL_8fe76e81d1b0457b882eb6faacce232b","value":" 223k/223k [00:00&lt;00:00, 5.57MB/s]"}},"55333aab86cb4d97989af438acc53a67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01907e7b03da4dfdac85aa0324325c8d","placeholder":"​","style":"IPY_MODEL_c14429bc51fb446bbfe06f13645dd9e6","value":"Downloading: 100%"}},"55b06b56923c403eb2e9253bd884ac73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ccdd3984ca54798807d10a6c0a66006":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5de35062707d4d7eb09bf1884fa6c4d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eb7f47673534d2a9e3fc2182e8af844":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51e017bb913047eca450f712bd3cfe66","placeholder":"​","style":"IPY_MODEL_a4d87ccb9ade497999496e6f2a7935f0","value":"Downloading: 100%"}},"663f407f9f704aa1ae77f28664627b33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"666a926bc63a425494364ae71be37139":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"693681e1cca447699d0b173687067ecf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"693fef2351804fe3891ea0b40c12c9d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ae8d63f39b147fe920f1a1c144f494b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d45f6829591c400883aae0b615846105","IPY_MODEL_d8c48cc0805c4238b4b203292230c47b","IPY_MODEL_3efe3b45a309437880e0a5ad9c58be68"],"layout":"IPY_MODEL_dfc62081f19842319c6e76d620968c58"}},"6b64b3abcc2f4310a1f5ee8e0e77ec2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a566ebf1274541659cb3d19c1b3c0a37","IPY_MODEL_2f60f22da9424936914b93bafaf1cb55","IPY_MODEL_54af4e9de2944e038c72803865280256"],"layout":"IPY_MODEL_55b06b56923c403eb2e9253bd884ac73"}},"6c43c98fef8144a8af856cc97e3645f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6dbe2689dbda41de9b0080d14d90e513":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88fb311edb584296a6a21b860ac82128","placeholder":"​","style":"IPY_MODEL_bf58e38cd92a41fcb75f6029d3968e8d","value":" 422M/422M [00:13&lt;00:00, 21.4MB/s]"}},"6f0591484ddb4454af6c7f42fa0a0279":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bec30dba6dac4986a00e4e6f7c91b394","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_334126ec80dc48acaca342f4f5ad93cc","value":1}},"6f0d92e849bc4afb8e64f84be533d343":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5eb7f47673534d2a9e3fc2182e8af844","IPY_MODEL_b3e3f68fd94c4721a7928a224e7ac33f","IPY_MODEL_6dbe2689dbda41de9b0080d14d90e513"],"layout":"IPY_MODEL_aa4694bc45484e3c9a88f1f5b9a56c5b"}},"6f4ca626282a4fb99e36f0f891b6f179":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f69233013fc49dba3750855c3f4d800":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"719a00e525dc49eead3e49b31880d692":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74c397ef7ba44c0e8890373029d08bd5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75cd97ebbba949108cd363baec7afcdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8b620c510c7483388b46339e894bc7a","IPY_MODEL_a3816c2408f0448890eece697fdbd24f","IPY_MODEL_b09ca72d7cc34d558c5f8855b0816f8f"],"layout":"IPY_MODEL_e9401f754dcd4e9fa7065397c0233f29"}},"761b526d690346f1bca69b8cfece1206":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fce5b3c31df493381489879e1b262e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85f75296cfd74088959934adf6fd4639":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88fb311edb584296a6a21b860ac82128":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e23a217b590464ea448c604c5451fc0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8fe76e81d1b0457b882eb6faacce232b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"929de88e159b43dda7c1f68fa4fd0a4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1da3cf5bd32e4d29a2b9f99dfe28216a","IPY_MODEL_6f0591484ddb4454af6c7f42fa0a0279","IPY_MODEL_18b75a441c5b4eb7911e4584101117a9"],"layout":"IPY_MODEL_85f75296cfd74088959934adf6fd4639"}},"961292b1eff14bf380638899def43491":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1efb9f259a34ec9be759135ef31fea2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_693681e1cca447699d0b173687067ecf","max":5970,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fbe3059b937544b793dde2008418cce6","value":5970}},"a33891d35724498a89d810da84749cff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3816c2408f0448890eece697fdbd24f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_355f639079654aaa828992c022263d39","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50e5a452eb4b457f9002660afb3b34c1","value":1}},"a4d87ccb9ade497999496e6f2a7935f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a566ebf1274541659cb3d19c1b3c0a37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51908f695fc54c4287b99ad0255605ce","placeholder":"​","style":"IPY_MODEL_e568daf438fe4acfaed352d9c88448b2","value":"Downloading: 100%"}},"a8a72fb4797c464a8a35d03f22710bb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8cdfdd2de87468da1d381c6912e2b82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_719a00e525dc49eead3e49b31880d692","placeholder":"​","style":"IPY_MODEL_c268420d42214c3c80309f13cf42c1cb","value":"100%"}},"aa416acfbe544e449ea909815de4b49f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d91d65fe4bcb4d12952dd11e0ff0347b","max":2482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0031c8a4147f4c5c85c425ca129a5c51","value":2482}},"aa4694bc45484e3c9a88f1f5b9a56c5b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aee2836685554158b61b6167dbff7a1f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f76c37ef6ba149cba11176bb460b6a5e","placeholder":"​","style":"IPY_MODEL_8e23a217b590464ea448c604c5451fc0","value":"100%"}},"b09ca72d7cc34d558c5f8855b0816f8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c3c8c3323ef4120b682052ab9befb10","placeholder":"​","style":"IPY_MODEL_666a926bc63a425494364ae71be37139","value":" 1/1 [00:00&lt;00:00, 14.00it/s]"}},"b3e3f68fd94c4721a7928a224e7ac33f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fce5b3c31df493381489879e1b262e9","max":442221694,"min":0,"orientation":"horizontal","style":"IPY_MODEL_312d061ff23b43f082436fe0fa977b37","value":442221694}},"b8b620c510c7483388b46339e894bc7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_544c7e05491a41bbaa8624b6fd75a415","placeholder":"​","style":"IPY_MODEL_2f0879ec781e4c5bb3bcd9c542d759c6","value":"100%"}},"bec30dba6dac4986a00e4e6f7c91b394":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf58e38cd92a41fcb75f6029d3968e8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c14429bc51fb446bbfe06f13645dd9e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1879aae6e0645b59a054afd3ce72184":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c268420d42214c3c80309f13cf42c1cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c53842a95eab47d495f124932b11e45d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca682a887b394e4293ccbf8654e12eda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aee2836685554158b61b6167dbff7a1f","IPY_MODEL_3906f4df4bf443cc8c3f3d40cb80be83","IPY_MODEL_dbda26dcf5b24e2eb3699ca0d6a86d2e"],"layout":"IPY_MODEL_2cac560d62e243dcb5e6df1498a2ef77"}},"ce790bd0d45d456f8c6fcd934985fea0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d45f6829591c400883aae0b615846105":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f08d3be1fa7b41ecb1f0fdd7ca4c3080","placeholder":"​","style":"IPY_MODEL_36369643c8c149c7b91acceebfbc6be8","value":"100%"}},"d89f9874a50a41db87a1a77a89f7d41d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8c48cc0805c4238b4b203292230c47b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d89f9874a50a41db87a1a77a89f7d41d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c43c98fef8144a8af856cc97e3645f2","value":1}},"d91d65fe4bcb4d12952dd11e0ff0347b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbda26dcf5b24e2eb3699ca0d6a86d2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07bbc987a9b74bc584d6a57a65893a5a","placeholder":"​","style":"IPY_MODEL_5ccdd3984ca54798807d10a6c0a66006","value":" 1990/1990 [00:01&lt;00:00, 1500.29ex/s]"}},"dfc62081f19842319c6e76d620968c58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e364ad3e8146466ab156d33b3926f00c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e568daf438fe4acfaed352d9c88448b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6510e86dfaa42f6886ba1400ca7ca1a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e74e3069dad646898887ab6ef29a153d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7d69a97aa9e46998bbb521d77d62f22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55333aab86cb4d97989af438acc53a67","IPY_MODEL_f2cb3d3a4b30461bb3b99bc1372a45bb","IPY_MODEL_4d54f94b8d9443fd91d10bd0711b3cc3"],"layout":"IPY_MODEL_e74e3069dad646898887ab6ef29a153d"}},"e9401f754dcd4e9fa7065397c0233f29":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea845d9491834a3a9b1ab4d0b4030fb1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f08d3be1fa7b41ecb1f0fdd7ca4c3080":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f24e05a562014456b2e67f00fa6568bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2cb3d3a4b30461bb3b99bc1372a45bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea845d9491834a3a9b1ab4d0b4030fb1","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f69233013fc49dba3750855c3f4d800","value":385}},"f76c37ef6ba149cba11176bb460b6a5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbe3059b937544b793dde2008418cce6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fdb76d6039dc4c69a86e966d2a058ab1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ab2cf683dc349f7b56f9622c3ddd798","placeholder":"​","style":"IPY_MODEL_25dab51ddfd1471199bd6b32fbb6fa76","value":" 6.34k/? [00:00&lt;00:00, 157kB/s]"}},"ff6a44e5da994f1b9145fc69564f233f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
