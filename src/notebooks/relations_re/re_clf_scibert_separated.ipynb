{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mustapha-AJEGHRIR/medical_txt_parser/blob/main/src/notebooks/assertions_nli/ast_nli_scibert.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"4l_yTcIgxsd_"},"source":["# Relations classification\n","\n","Based of: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_xnli.py"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Jan 25 13:27:33 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   69C    P0    32W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n","| N/A   68C    P0    30W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":4620,"status":"ok","timestamp":1642252265905,"user":{"displayName":"Nouamane Tazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg753z6h9fmTPmGyKajJFbNQG48KIqPziiTsxl4Tw=s64","userId":"11345629174419407363"},"user_tz":-60},"id":"LwHxhQx0xseE"},"outputs":[],"source":["%%capture\n","!pip install seqeval transformers datasets spacy sentence_transformers"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1908,"status":"ok","timestamp":1642251232672,"user":{"displayName":"Nouamane Tazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg753z6h9fmTPmGyKajJFbNQG48KIqPziiTsxl4Tw=s64","userId":"11345629174419407363"},"user_tz":-60},"id":"3rCJwcFyyU9J","outputId":"f75daf2c-9611-4b10-faf6-dd37b6e411f5"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7e273c3077db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/projects/medical_txt_parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/projects/medical_txt_parser"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1358,"status":"ok","timestamp":1642251234969,"user":{"displayName":"Nouamane Tazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg753z6h9fmTPmGyKajJFbNQG48KIqPziiTsxl4Tw=s64","userId":"11345629174419407363"},"user_tz":-60},"id":"FyDbgPv1xseF"},"outputs":[],"source":["%reload_ext autoreload\n","%autoreload 2\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","path = %pwd\n","while \"src\" in path:\n","    %cd ..\n","    path = %pwd\n","\n","import glob\n","import pandas as pd\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from pprint import pprint\n","import matplotlib.pyplot as plt\n","import random\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","import datasets\n","import numpy as np\n","from datasets import load_dataset, load_metric , Dataset\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","from transformers import pipeline\n","\n","require_version(\"datasets>=1.8.0\", \"To fix: pip install --upgrade datasets\")\n","\n","from src.utils.parse_data import parse_ast, parse_concept, parse_relation"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":219,"status":"ok","timestamp":1642251245139,"user":{"displayName":"Nouamane Tazi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg753z6h9fmTPmGyKajJFbNQG48KIqPziiTsxl4Tw=s64","userId":"11345629174419407363"},"user_tz":-60},"id":"Sp1UC7psxseH"},"outputs":[],"source":["train_data_path = \"data/train\"\n","val_data_path = \"data/val\"\n","ast_folder_name = \"ast\"\n","concept_folder_name = \"concept\"\n","rel_folder_name = \"rel\"\n","txt_folder_name = \"txt\"\n","nli_data_path = \"data/nli\"\n","re_data_path = \"data/re\"\n","\n","# model args\n","model_name_or_path = \"allenai/scibert_scivocab_uncased\" # \"gsarti/scibert-nli\"  \"allenai/scibert_scivocab_uncased\"  \"models/scibert_scivocab_uncased-re-1\"\n","cache_dir = None\n","model_revision = None \n","tokenizer_name = model_name_or_path\n","do_lower_case = None\n","use_fast_tokenizer = True\n","fp16 = True\n","\n","# data args\n","pad_to_max_length = None\n","max_seq_length = None\n","\n","set_seed(42)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model Test - Problem"]},{"cell_type":"markdown","metadata":{"id":"zP8CizR4xseH"},"source":["### Import data"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["re_task = \"Te_P\""]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>She had &lt;&lt; a workup &gt;&gt; by her neurologist and ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>She had &lt;&lt; a workup &gt;&gt; by her neurologist and ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>She had &lt;&lt; a workup &gt;&gt; by her neurologist and ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>She had a workup by her neurologist and &lt;&lt; an ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>She had a workup by her neurologist and &lt;&lt; an ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2145</th>\n","      <td>The patient had &lt;&lt; an echocardiogram &gt;&gt; on day...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2146</th>\n","      <td>The patient had &lt;&lt; an echocardiogram &gt;&gt; on day...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2147</th>\n","      <td>The patient had &lt;&lt; an echocardiogram &gt;&gt; on day...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2148</th>\n","      <td>The patient had &lt;&lt; an echocardiogram &gt;&gt; on day...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2149</th>\n","      <td>The patient had &lt;&lt; an echocardiogram &gt;&gt; on day...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2150 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   text  label\n","0     She had << a workup >> by her neurologist and ...      0\n","1     She had << a workup >> by her neurologist and ...      0\n","2     She had << a workup >> by her neurologist and ...      0\n","3     She had a workup by her neurologist and << an ...      1\n","4     She had a workup by her neurologist and << an ...      1\n","...                                                 ...    ...\n","2145  The patient had << an echocardiogram >> on day...      1\n","2146  The patient had << an echocardiogram >> on day...      1\n","2147  The patient had << an echocardiogram >> on day...      1\n","2148  The patient had << an echocardiogram >> on day...      1\n","2149  The patient had << an echocardiogram >> on day...      1\n","\n","[2150 rows x 2 columns]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["relations_df = pd.read_csv(re_data_path + os.sep + f\"re_scibert_data_{re_task}.tsv\", sep=\"\\t\", header=None)\n","relations_df.columns = [\"text\", \"label\"]\n","label2id = {label: i for i, label in enumerate(relations_df[\"label\"].value_counts().index.tolist())}\n","id2label = {i: label for label, i in label2id.items()}\n","relations_df[\"label\"] = relations_df.label.map(label2id)\n","relations_df"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 1720\n"," }),\n"," Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 430\n"," }))"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Build HuggingFace Dataset\n","\n","train_df, val_df = train_test_split(relations_df, train_size=None, shuffle=True, test_size=0.2, stratify=relations_df[\"label\"], random_state=42)\n","\n","features = datasets.Features({'text': datasets.Value(dtype='string'),\n"," 'label': datasets.ClassLabel(num_classes=len(id2label), names=list(id2label.values()))})\n","\n","train_dataset = Dataset.from_pandas(train_df, preserve_index=False, features=features)\n","eval_dataset = Dataset.from_pandas(val_df, preserve_index=False, features=features)\n","\n","label_list = train_dataset.features[\"label\"].names\n","num_labels = len(label_list)\n","\n","label_list = train_dataset.features[\"label\"].names\n","train_dataset, eval_dataset"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train_df labels: 1    794\n","0    793\n","2    133\n","Name: label, dtype: int64\n","val_df labels: 0    199\n","1    198\n","2     33\n","Name: label, dtype: int64\n"]}],"source":["# check labels balance\n","print(f\"train_df labels: {train_df['label'].value_counts()}\")\n","print(f\"val_df labels: {val_df['label'].value_counts()}\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load pretrained model and tokenizer\n","# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","# download model & vocab.\n","config = AutoConfig.from_pretrained(\n","    model_name_or_path,\n","    num_labels=num_labels,\n","    finetuning_task=\"re\",\n","    cache_dir=cache_dir,\n","    revision=model_revision,\n","    label2id=label2id,\n","    id2label=id2label\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\n","    tokenizer_name if tokenizer_name else model_name_or_path,\n","    # do_lower_case=do_lower_case,\n","    cache_dir=cache_dir,\n","    use_fast=use_fast_tokenizer,\n","    revision=model_revision,\n",")\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name_or_path,\n","    from_tf=bool(\".ckpt\" in model_name_or_path),\n","    config=config,\n","    cache_dir=cache_dir,\n","    revision=model_revision\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97c3f795b1cb4c2eb1c3ba03bb2b958e","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4a36cfd650e4986b4a3089f63e59310","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Sample 1309 of the training set: {'text': '<< The liver enzimes >> come down , no signs of [[ biliary stasis ]] .', 'label': 0, 'input_ids': [102, 962, 962, 111, 2993, 279, 5889, 8608, 1374, 1374, 6096, 1922, 422, 425, 6482, 131, 260, 260, 17496, 15934, 4554, 1901, 1901, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 228 of the training set: {'text': '<< Chest x-ray >> shows no [[ infiltrate ]] or pneumothorax .', 'label': 1, 'input_ids': [102, 962, 962, 8693, 412, 579, 7930, 1374, 1374, 1402, 425, 260, 260, 28051, 1901, 1901, 234, 6335, 27948, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 51 of the training set: {'text': 'On << physical examination >> , she was a very pleasant , moderately obese female in no [[ apparent distress ]] .', 'label': 0, 'input_ids': [102, 191, 962, 962, 2121, 4373, 1374, 1374, 422, 2281, 241, 106, 1248, 28510, 422, 13686, 10171, 3672, 121, 425, 260, 260, 4408, 10581, 1901, 1901, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n"]}],"source":["# Preprocessing the datasets\n","# Padding strategy\n","if pad_to_max_length:\n","    padding = \"max_length\"\n","else:\n","    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","    padding = False\n","\n","\n","def preprocess_function(examples):\n","    # Tokenize the texts\n","    return tokenizer(\n","        examples[\"text\"],\n","        padding=padding,\n","        max_length=max_seq_length,\n","        truncation=True,\n","    )\n","\n","\n","train_dataset = train_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on train dataset\",\n",")\n","eval_dataset = eval_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on validation dataset\",\n",")\n","\n","# Log a few random samples from the training set:\n","for index in random.sample(range(len(train_dataset)), 3):\n","    print(f\"Sample {index} of the training set: {train_dataset[index]}.\\n\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Get the metric function\n","f1_metric = load_metric(\"f1\")\n","precision_metric = load_metric(\"precision\")\n","recall_metric = load_metric(\"recall\")\n","accuracy_metric = load_metric(\"accuracy\")\n","\n","# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","# predictions and label_ids field) and has to return a dictionary string to float.\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","\n","    metrics = {}\n","    metrics.update(f1_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(precision_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(recall_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(accuracy_metric.compute(predictions=preds, references=p.label_ids))\n","    return metrics"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","if pad_to_max_length:\n","    data_collator = default_data_collator\n","elif fp16:\n","    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","else:\n","    data_collator = None"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["'allenai/scibert_scivocab_uncased'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model_name_or_path"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# address class imbalance \n","import torch\n","from torch import nn\n","from transformers import Trainer\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","class_weights = compute_class_weight(class_weight='balanced', classes=list(id2label.keys()),y=train_df[\"label\"])\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        # forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float().to(logits.device))\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["# Initialize our Trainer\n","model_folder_name = f\"{model_name_or_path.split('/')[-1]}-re-{re_task}\"\n","args = TrainingArguments(\n","    f\"training_logs/{model_folder_name}\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"no\",\n","    learning_rate=1e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=10,\n","    weight_decay=0.05,\n","    logging_steps=1,\n","    warmup_ratio=0.1,\n",")\n","\n","trainer = CustomTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset ,\n","    eval_dataset=eval_dataset ,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running training *****\n","  Num examples = 1720\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 270\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [270/270 04:00, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.914700</td>\n","      <td>0.862243</td>\n","      <td>0.591877</td>\n","      <td>0.579334</td>\n","      <td>0.655635</td>\n","      <td>0.639535</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.782500</td>\n","      <td>0.740154</td>\n","      <td>0.612529</td>\n","      <td>0.602095</td>\n","      <td>0.716114</td>\n","      <td>0.653488</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.615400</td>\n","      <td>0.697649</td>\n","      <td>0.646892</td>\n","      <td>0.642525</td>\n","      <td>0.748295</td>\n","      <td>0.686047</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.482900</td>\n","      <td>0.641081</td>\n","      <td>0.669257</td>\n","      <td>0.646956</td>\n","      <td>0.727916</td>\n","      <td>0.704651</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.464300</td>\n","      <td>0.641414</td>\n","      <td>0.678677</td>\n","      <td>0.661465</td>\n","      <td>0.748194</td>\n","      <td>0.709302</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.214800</td>\n","      <td>0.612059</td>\n","      <td>0.712023</td>\n","      <td>0.692206</td>\n","      <td>0.776763</td>\n","      <td>0.737209</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.223100</td>\n","      <td>0.626099</td>\n","      <td>0.717725</td>\n","      <td>0.699795</td>\n","      <td>0.756476</td>\n","      <td>0.744186</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.264300</td>\n","      <td>0.593740</td>\n","      <td>0.732274</td>\n","      <td>0.706698</td>\n","      <td>0.779977</td>\n","      <td>0.753488</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.374500</td>\n","      <td>0.625240</td>\n","      <td>0.732229</td>\n","      <td>0.716297</td>\n","      <td>0.759767</td>\n","      <td>0.760465</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.167900</td>\n","      <td>0.628189</td>\n","      <td>0.732404</td>\n","      <td>0.717083</td>\n","      <td>0.766552</td>\n","      <td>0.758140</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 430\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 430\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 430\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 430\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 430\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 430\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 430\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 430\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 430\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 430\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]}],"source":["train_result = trainer.train()"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to models/scibert_scivocab_uncased-re-Te_P\n","Configuration saved in models/scibert_scivocab_uncased-re-Te_P/config.json\n","Model weights saved in models/scibert_scivocab_uncased-re-Te_P/pytorch_model.bin\n","tokenizer config file saved in models/scibert_scivocab_uncased-re-Te_P/tokenizer_config.json\n","Special tokens file saved in models/scibert_scivocab_uncased-re-Te_P/special_tokens_map.json\n"]},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =       10.0\n","  total_flos               =  1050899GF\n","  train_loss               =      0.494\n","  train_runtime            = 0:04:01.01\n","  train_samples            =       1720\n","  train_samples_per_second =     71.364\n","  train_steps_per_second   =       1.12\n"]}],"source":["metrics = train_result.metrics\n","metrics[\"train_samples\"] = len(train_dataset)\n","\n","trainer.save_model(f\"models/{model_folder_name}\")  # Saves the tokenizer too for easy upload\n","\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 430\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["*** Evaluate ***\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='41' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7/7 00:12]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       10.0\n","  eval_accuracy           =     0.7581\n","  eval_f1                 =     0.7324\n","  eval_loss               =     0.6282\n","  eval_precision          =     0.7171\n","  eval_recall             =     0.7666\n","  eval_runtime            = 0:00:02.10\n","  eval_samples            =        430\n","  eval_samples_per_second =     204.41\n","  eval_steps_per_second   =      3.328\n"]}],"source":["print(\"*** Evaluate ***\") \n","metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","metrics[\"eval_samples\"] = len(eval_dataset)\n","\n","trainer.log_metrics(\"eval\", metrics)\n","trainer.save_metrics(\"eval\", metrics)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 430\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Other       0.83      0.67      0.74       199\n","        TeRP       0.74      0.84      0.79       198\n","        TeCP       0.58      0.79      0.67        33\n","\n","    accuracy                           0.76       430\n","   macro avg       0.72      0.77      0.73       430\n","weighted avg       0.77      0.76      0.76       430\n","\n"]}],"source":["predictions, labels, _ = trainer.predict(eval_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 1720\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Other       0.95      0.82      0.88       793\n","        TeRP       0.89      0.94      0.91       794\n","        TeCP       0.71      1.00      0.83       133\n","\n","    accuracy                           0.89      1720\n","   macro avg       0.85      0.92      0.87      1720\n","weighted avg       0.90      0.89      0.89      1720\n","\n"]}],"source":["predictions, labels, _ = trainer.predict(train_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Model Treatment - Problem"]},{"cell_type":"markdown","metadata":{"id":"zP8CizR4xseH"},"source":["### Import data"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["re_task = \"Tr_P\""]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>She had a postoperative CT scan that revealed ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[[ Her pain ]] was under good control with &lt;&lt; ...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3. &lt;&lt; Percocet &gt;&gt; , 5/325 , 1-2 tabs PO q4-6h ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Take &lt;&lt; codeine &gt;&gt; prescribed by PCP with food...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Take &lt;&lt; codeine &gt;&gt; prescribed by PCP with food...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2904</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2905</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2906</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2907</th>\n","      <td>The patient was told he could return to work a...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2908</th>\n","      <td>The patient was told he could return to work a...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2909 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   text  label\n","0     She had a postoperative CT scan that revealed ...      0\n","1     [[ Her pain ]] was under good control with << ...      4\n","2     3. << Percocet >> , 5/325 , 1-2 tabs PO q4-6h ...      1\n","3     Take << codeine >> prescribed by PCP with food...      0\n","4     Take << codeine >> prescribed by PCP with food...      0\n","...                                                 ...    ...\n","2904  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","2905  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","2906  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","2907  The patient was told he could return to work a...      1\n","2908  The patient was told he could return to work a...      0\n","\n","[2909 rows x 2 columns]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["relations_df = pd.read_csv(re_data_path + os.sep + f\"re_scibert_data_{re_task}.tsv\", sep=\"\\t\", header=None)\n","relations_df.columns = [\"text\", \"label\"]\n","label2id = {label: i for i, label in enumerate(relations_df[\"label\"].value_counts().index.tolist())}\n","id2label = {i: label for label, i in label2id.items()}\n","relations_df[\"label\"] = relations_df.label.map(label2id)\n","relations_df"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 2327\n"," }),\n"," Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 582\n"," }))"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Build HuggingFace Dataset\n","\n","train_df, val_df = train_test_split(relations_df, train_size=None, shuffle=True, test_size=0.2, stratify=relations_df[\"label\"], random_state=42)\n","\n","features = datasets.Features({'text': datasets.Value(dtype='string'),\n"," 'label': datasets.ClassLabel(num_classes=len(id2label), names=list(id2label.values()))})\n","\n","train_dataset = Dataset.from_pandas(train_df, preserve_index=False, features=features)\n","eval_dataset = Dataset.from_pandas(val_df, preserve_index=False, features=features)\n","\n","label_list = train_dataset.features[\"label\"].names\n","num_labels = len(label_list)\n","\n","label_list = train_dataset.features[\"label\"].names\n","train_dataset, eval_dataset"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train_df labels: 0    1363\n","1     707\n","2     147\n","3      50\n","4      41\n","5      19\n","Name: label, dtype: int64\n","val_df labels: 0    341\n","1    177\n","2     37\n","3     12\n","4     10\n","5      5\n","Name: label, dtype: int64\n"]}],"source":["# check labels balance\n","print(f\"train_df labels: {train_df['label'].value_counts()}\")\n","print(f\"val_df labels: {val_df['label'].value_counts()}\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load pretrained model and tokenizer\n","# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","# download model & vocab.\n","config = AutoConfig.from_pretrained(\n","    model_name_or_path,\n","    num_labels=num_labels,\n","    finetuning_task=\"re\",\n","    cache_dir=cache_dir,\n","    revision=model_revision,\n","    label2id=label2id,\n","    id2label=id2label\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\n","    tokenizer_name if tokenizer_name else model_name_or_path,\n","    # do_lower_case=do_lower_case,\n","    cache_dir=cache_dir,\n","    use_fast=use_fast_tokenizer,\n","    revision=model_revision,\n",")\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name_or_path,\n","    from_tf=bool(\".ckpt\" in model_name_or_path),\n","    config=config,\n","    cache_dir=cache_dir,\n","    revision=model_revision,\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a66fb83247e4f59aa7551bf2600ddff","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f93c944d32314f1c8e05e094e83b8834","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Sample 456 of the training set: {'text': 'There was still concern for her effusion contributing to her SOB at the OSH so she was transferred for << drainage >> and treatment of [[ CHF ]] .', 'label': 0, 'input_ids': [102, 461, 241, 2077, 6366, 168, 1750, 24522, 10027, 147, 1750, 25859, 235, 111, 3581, 30117, 564, 2281, 241, 6388, 168, 962, 962, 12808, 1374, 1374, 137, 922, 131, 260, 260, 25794, 1901, 1901, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 102 of the training set: {'text': \"Mr. Pohl is a 53 - year-old male with a history of ETOH , hypertension who presented int he emergency room with increased agitation , like secondary to ETOH intoxication and developed [[ subsequent hypotension ]] , systolic blood pressures in the 80 's, status post << Ativan >> and Haldol , but was responsive\", 'label': 2, 'input_ids': [102, 2122, 205, 1494, 4697, 165, 106, 5393, 579, 996, 579, 4289, 3398, 190, 106, 2999, 131, 27817, 422, 5801, 975, 1632, 391, 299, 7410, 4095, 190, 1175, 25572, 422, 1967, 3287, 147, 27817, 25116, 1442, 137, 1815, 260, 260, 3766, 21983, 1901, 1901, 422, 11152, 1702, 10670, 121, 111, 2833, 2505, 112, 422, 2726, 1422, 962, 962, 24673, 7580, 1374, 1374, 137, 2547, 2904, 30115, 422, 563, 241, 14346, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 1126 of the training set: {'text': '<< Lopressor >> 100 mg p.o. b.i.d. ; Nifedipine 10 mg p.o. t.i.d. ; Timoptic solution 0.5% OS b.i.d. ; Coumadin 1 mg alternating with 1.5 mg q.o.d. at q.h.s. and Haldol 2 mg p.o. q.h.s. p.r.n. in the evening , only for agitation associated with [[ confusion ]] .', 'label': 0, 'input_ids': [102, 962, 962, 881, 29575, 1374, 1374, 1287, 1529, 118, 205, 116, 205, 132, 205, 259, 205, 128, 205, 1814, 25401, 2233, 14333, 566, 1529, 118, 205, 116, 205, 105, 205, 259, 205, 128, 205, 1814, 445, 19185, 1368, 244, 205, 305, 1863, 3581, 132, 205, 259, 205, 128, 205, 1814, 2808, 19780, 107, 158, 1529, 15529, 190, 158, 205, 305, 1529, 735, 205, 116, 205, 128, 205, 235, 735, 205, 151, 205, 112, 205, 137, 2547, 2904, 30115, 170, 1529, 118, 205, 116, 205, 735, 205, 151, 205, 112, 205, 118, 205, 182, 205, 146, 205, 121, 111, 23459, 422, 617, 168, 25572, 1111, 190, 260, 260, 14153, 1901, 1901, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n"]}],"source":["# Preprocessing the datasets\n","# Padding strategy\n","if pad_to_max_length:\n","    padding = \"max_length\"\n","else:\n","    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","    padding = False\n","\n","\n","def preprocess_function(examples):\n","    # Tokenize the texts\n","    return tokenizer(\n","        examples[\"text\"],\n","        padding=padding,\n","        max_length=max_seq_length,\n","        truncation=True,\n","    )\n","\n","\n","train_dataset = train_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on train dataset\",\n",")\n","eval_dataset = eval_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on validation dataset\",\n",")\n","\n","# Log a few random samples from the training set:\n","for index in random.sample(range(len(train_dataset)), 3):\n","    print(f\"Sample {index} of the training set: {train_dataset[index]}.\\n\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Get the metric function\n","f1_metric = load_metric(\"f1\")\n","precision_metric = load_metric(\"precision\")\n","recall_metric = load_metric(\"recall\")\n","accuracy_metric = load_metric(\"accuracy\")\n","\n","# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","# predictions and label_ids field) and has to return a dictionary string to float.\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","\n","    metrics = {}\n","    metrics.update(f1_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(precision_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(recall_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(accuracy_metric.compute(predictions=preds, references=p.label_ids))\n","    return metrics"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","if pad_to_max_length:\n","    data_collator = default_data_collator\n","elif fp16:\n","    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","else:\n","    data_collator = None"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["'allenai/scibert_scivocab_uncased'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["model_name_or_path"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.28454389826363413, 0.5485619990570486, 2.6383219954648527, 7.756666666666667, 9.459349593495935, 20.412280701754387]\n"]}],"source":["# address class imbalance \n","import torch\n","from torch import nn\n","from transformers import Trainer\n","\n","class_weights = [len(train_df)/ (len(train_df[train_df[\"label\"] == i])*len(id2label)) for i in id2label.keys()]\n","print(class_weights)\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        # forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float().to(logits.device))\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Initialize our Trainer\n","model_folder_name = f\"{model_name_or_path.split('/')[-1]}-re-{re_task}\"\n","args = TrainingArguments(\n","    f\"training_logs/{model_folder_name}\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"no\",\n","    learning_rate=1e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=5,\n","    weight_decay=0.05,\n","    logging_steps=1,\n","    warmup_ratio=0.1,\n",")\n","\n","trainer = CustomTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset ,\n","    eval_dataset=eval_dataset ,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running training *****\n","  Num examples = 2327\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 185\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='185' max='185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [185/185 05:04, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.751000</td>\n","      <td>1.623897</td>\n","      <td>0.266744</td>\n","      <td>0.276491</td>\n","      <td>0.374617</td>\n","      <td>0.494845</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.534900</td>\n","      <td>1.461777</td>\n","      <td>0.360805</td>\n","      <td>0.415869</td>\n","      <td>0.440072</td>\n","      <td>0.548110</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.394300</td>\n","      <td>1.322689</td>\n","      <td>0.409176</td>\n","      <td>0.400643</td>\n","      <td>0.554285</td>\n","      <td>0.587629</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.859200</td>\n","      <td>1.242026</td>\n","      <td>0.459844</td>\n","      <td>0.426182</td>\n","      <td>0.618987</td>\n","      <td>0.625430</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.814500</td>\n","      <td>1.209963</td>\n","      <td>0.466858</td>\n","      <td>0.432025</td>\n","      <td>0.640575</td>\n","      <td>0.627148</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 582\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 582\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 582\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 582\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 582\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]}],"source":["train_result = trainer.train()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to models/scibert_scivocab_uncased-re-Tr_P\n","Configuration saved in models/scibert_scivocab_uncased-re-Tr_P/config.json\n","Model weights saved in models/scibert_scivocab_uncased-re-Tr_P/pytorch_model.bin\n","tokenizer config file saved in models/scibert_scivocab_uncased-re-Tr_P/tokenizer_config.json\n","Special tokens file saved in models/scibert_scivocab_uncased-re-Tr_P/special_tokens_map.json\n"]},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =        5.0\n","  total_flos               =  1448866GF\n","  train_loss               =      1.313\n","  train_runtime            = 0:05:10.23\n","  train_samples            =       2327\n","  train_samples_per_second =     37.504\n","  train_steps_per_second   =      0.596\n"]}],"source":["metrics = train_result.metrics\n","metrics[\"train_samples\"] = len(train_dataset)\n","\n","trainer.save_model(f\"models/{model_folder_name}\")  # Saves the tokenizer too for easy upload\n","\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 582\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["*** Evaluate ***\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='57' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [10/10 00:32]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =        5.0\n","  eval_accuracy           =     0.6271\n","  eval_f1                 =     0.4669\n","  eval_loss               =       1.21\n","  eval_precision          =      0.432\n","  eval_recall             =     0.6406\n","  eval_runtime            = 0:00:05.38\n","  eval_samples            =        582\n","  eval_samples_per_second =      108.0\n","  eval_steps_per_second   =      1.856\n"]}],"source":["print(\"*** Evaluate ***\") \n","metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","metrics[\"eval_samples\"] = len(eval_dataset)\n","\n","trainer.log_metrics(\"eval\", metrics)\n","trainer.save_metrics(\"eval\", metrics)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 582\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Other       0.84      0.65      0.73       341\n","        TrAP       0.73      0.56      0.63       177\n","        TrCP       0.27      0.70      0.39        37\n","       TrNAP       0.48      0.83      0.61        12\n","        TrIP       0.14      0.70      0.23        10\n","        TrWP       0.14      0.40      0.21         5\n","\n","    accuracy                           0.63       582\n","   macro avg       0.43      0.64      0.47       582\n","weighted avg       0.74      0.63      0.66       582\n","\n"]}],"source":["predictions, labels, _ = trainer.predict(eval_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 2327\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Other       0.87      0.67      0.76      1363\n","        TrAP       0.72      0.57      0.64       707\n","        TrCP       0.33      0.84      0.47       147\n","       TrNAP       0.32      0.90      0.48        50\n","        TrIP       0.25      0.95      0.40        41\n","        TrWP       0.38      0.95      0.55        19\n","\n","    accuracy                           0.66      2327\n","   macro avg       0.48      0.81      0.55      2327\n","weighted avg       0.76      0.66      0.69      2327\n","\n"]}],"source":["predictions, labels, _ = trainer.predict(train_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# empty cuda cache\n","import torch\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Tue Jan 25 17:24:06 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   76C    P0    45W /  70W |   3452MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n","| N/A   76C    P0    43W /  70W |   1592MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A     20404      C   /opt/conda/bin/python            3449MiB |\n","|    1   N/A  N/A     20404      C   /opt/conda/bin/python            1589MiB |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["## Model Problem - Problem"]},{"cell_type":"markdown","metadata":{"id":"zP8CizR4xseH"},"source":["### Import data"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["re_task = \"P_P\""]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;&lt; C5-6 disc herniation &gt;&gt; with [[ cord compre...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;&lt; C5-6 disc herniation &gt;&gt; with cord compressi...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[[ C5-6 disc herniation ]] with &lt;&lt; cord compre...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>C5-6 disc herniation with &lt;&lt; cord compression ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[[ C5-6 disc herniation ]] with cord compressi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>10279</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10280</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10281</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10282</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10283</th>\n","      <td>Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10284 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                    text  label\n","0      << C5-6 disc herniation >> with [[ cord compre...      1\n","1      << C5-6 disc herniation >> with cord compressi...      1\n","2      [[ C5-6 disc herniation ]] with << cord compre...      0\n","3      C5-6 disc herniation with << cord compression ...      0\n","4      [[ C5-6 disc herniation ]] with cord compressi...      0\n","...                                                  ...    ...\n","10279  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","10280  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","10281  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","10282  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","10283  Aspirin 325 mg q.d. , Plavix 75 mg q.d. , Lipi...      0\n","\n","[10284 rows x 2 columns]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["relations_df = pd.read_csv(re_data_path + os.sep + f\"re_scibert_data_{re_task}.tsv\", sep=\"\\t\", header=None)\n","relations_df.columns = [\"text\", \"label\"]\n","label2id = {label: i for i, label in enumerate(relations_df[\"label\"].value_counts().index.tolist())}\n","id2label = {i: label for label, i in label2id.items()}\n","relations_df[\"label\"] = relations_df.label.map(label2id)\n","relations_df"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 8227\n"," }),\n"," Dataset({\n","     features: ['text', 'label'],\n","     num_rows: 2057\n"," }))"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# Build HuggingFace Dataset\n","\n","train_df, val_df = train_test_split(relations_df, train_size=None, shuffle=True, test_size=0.2, stratify=relations_df[\"label\"], random_state=42)\n","\n","features = datasets.Features({'text': datasets.Value(dtype='string'),\n"," 'label': datasets.ClassLabel(num_classes=len(id2label), names=list(id2label.values()))})\n","\n","train_dataset = Dataset.from_pandas(train_df, preserve_index=False, features=features)\n","eval_dataset = Dataset.from_pandas(val_df, preserve_index=False, features=features)\n","\n","label_list = train_dataset.features[\"label\"].names\n","num_labels = len(label_list)\n","\n","label_list = train_dataset.features[\"label\"].names\n","train_dataset, eval_dataset"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train_df labels: 0    7623\n","1     604\n","Name: label, dtype: int64\n","val_df labels: 0    1906\n","1     151\n","Name: label, dtype: int64\n"]}],"source":["# check labels balance\n","print(f\"train_df labels: {train_df['label'].value_counts()}\")\n","print(f\"val_df labels: {val_df['label'].value_counts()}\")"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"re\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Other\",\n","    \"1\": \"PIP\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Other\": 0,\n","    \"PIP\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/jupyter/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n","Model config BertConfig {\n","  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n","Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load pretrained model and tokenizer\n","# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","# download model & vocab.\n","config = AutoConfig.from_pretrained(\n","    model_name_or_path,\n","    num_labels=num_labels,\n","    finetuning_task=\"re\",\n","    cache_dir=cache_dir,\n","    revision=model_revision,\n","    label2id=label2id,\n","    id2label=id2label\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\n","    tokenizer_name if tokenizer_name else model_name_or_path,\n","    # do_lower_case=do_lower_case,\n","    cache_dir=cache_dir,\n","    use_fast=use_fast_tokenizer,\n","    revision=model_revision,\n",")\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name_or_path,\n","    from_tf=bool(\".ckpt\" in model_name_or_path),\n","    config=config,\n","    cache_dir=cache_dir,\n","    revision=model_revision,\n",")"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df43d6f74829421f9fe7285643670b2f","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/9 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c49378b11b34bd4b1ca020294cb9b39","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Sample 1824 of the training set: {'text': 'MOUTH NORMAL NECK NORMAL thyroid wnl BREASTS NORMAL no distinct masses NIPPLES NORMAL << inverted >> [ b ] , evert w / stimulation CHEST NORMAL LCTA COR NORMAL RRR ABDOMEN NORMAL gravid EXTREM NORMAL SKIN NORMAL NODES NORMAL VULVA NORMAL no lesions , [[ white d / c at introitus ]] VAGINA NORMAL sml amt thin white d / c ph 4.5 , koh +amine , NS +clue , neg trich CERVIX NORMAL 1/100/0 srom clear OS NORMAL closed ADNEXAE NORMAL no palp masses , NT UTERUS NORMAL gravid UTERINE SIZE IN WEEKS NORMAL term RECTUM NORMAL no ext lesions', 'label': 0, 'input_ids': [102, 12860, 1346, 7980, 1346, 8143, 15239, 30115, 3479, 30113, 1346, 425, 3646, 9686, 26119, 1024, 1346, 962, 962, 13455, 1374, 1374, 260, 132, 1901, 422, 1661, 30108, 124, 1352, 4156, 8693, 1346, 6087, 2219, 470, 1346, 5058, 30114, 17748, 1346, 10377, 173, 4847, 1346, 3843, 1346, 2207, 1346, 5992, 3833, 1346, 425, 4278, 422, 260, 260, 3606, 128, 1352, 115, 235, 1205, 8592, 1901, 1901, 7988, 1869, 1346, 659, 30115, 439, 30108, 5197, 3606, 128, 1352, 115, 375, 286, 205, 305, 422, 14158, 473, 18605, 422, 3281, 473, 326, 376, 422, 1415, 12489, 28765, 1346, 158, 1352, 1287, 1352, 244, 4193, 157, 1716, 3581, 1346, 4358, 290, 25498, 1611, 1346, 425, 21348, 9686, 422, 4405, 18718, 1346, 10377, 173, 14045, 1243, 121, 3272, 1346, 902, 25277, 1346, 425, 1267, 4278, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 409 of the training set: {'text': 'He denies any recent chest pain , palpitations , [[ headache ]] , confusion , weakness , << numbness >> , abd pain , or hematuria .', 'label': 0, 'input_ids': [102, 299, 1157, 301, 843, 2151, 8693, 2675, 422, 21348, 29504, 422, 260, 260, 15281, 1901, 1901, 422, 14153, 422, 11688, 422, 962, 962, 541, 6605, 250, 1374, 1374, 422, 12378, 2675, 422, 234, 6187, 11029, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n","Sample 4506 of the training set: {'text': 'If you have [[ chest pain ]] , << shortness of breath >> , diahrrea or fevers please contact your PCP .', 'label': 0, 'input_ids': [102, 543, 3034, 360, 260, 260, 8693, 2675, 1901, 1901, 422, 962, 962, 2001, 1076, 131, 10062, 1374, 1374, 422, 346, 9979, 17747, 234, 10551, 30113, 8611, 3585, 5296, 23253, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","\n"]}],"source":["# Preprocessing the datasets\n","# Padding strategy\n","if pad_to_max_length:\n","    padding = \"max_length\"\n","else:\n","    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","    padding = False\n","\n","\n","def preprocess_function(examples):\n","    # Tokenize the texts\n","    return tokenizer(\n","        examples[\"text\"],\n","        padding=padding,\n","        max_length=max_seq_length,\n","        truncation=True,\n","    )\n","\n","\n","train_dataset = train_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on train dataset\",\n",")\n","eval_dataset = eval_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on validation dataset\",\n",")\n","\n","# Log a few random samples from the training set:\n","for index in random.sample(range(len(train_dataset)), 3):\n","    print(f\"Sample {index} of the training set: {train_dataset[index]}.\\n\")\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# Get the metric function\n","f1_metric = load_metric(\"f1\")\n","precision_metric = load_metric(\"precision\")\n","recall_metric = load_metric(\"recall\")\n","accuracy_metric = load_metric(\"accuracy\")\n","\n","# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","# predictions and label_ids field) and has to return a dictionary string to float.\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","\n","    metrics = {}\n","    metrics.update(f1_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(precision_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(recall_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\"))\n","    metrics.update(accuracy_metric.compute(predictions=preds, references=p.label_ids))\n","    return metrics"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","if pad_to_max_length:\n","    data_collator = default_data_collator\n","elif fp16:\n","    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","else:\n","    data_collator = None"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/plain":["'allenai/scibert_scivocab_uncased'"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["model_name_or_path"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.5396169487078578, 6.810430463576159]\n"]}],"source":["# address class imbalance \n","import torch\n","from torch import nn\n","from transformers import Trainer\n","\n","class_weights = [len(train_df)/ (len(train_df[train_df[\"label\"] == i])*len(id2label)) for i in id2label.keys()]\n","print(class_weights)\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        # forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float().to(logits.device))\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["# Initialize our Trainer\n","model_folder_name = f\"{model_name_or_path.split('/')[-1]}-re-{re_task}\"\n","args = TrainingArguments(\n","    f\"training_logs/{model_folder_name}\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"no\",\n","    learning_rate=1e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=5,\n","    weight_decay=0.05,\n","    logging_steps=1,\n","    warmup_ratio=0.1,\n",")\n","\n","trainer = CustomTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset ,\n","    eval_dataset=eval_dataset ,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running training *****\n","  Num examples = 8227\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 645\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='645' max='645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [645/645 10:57, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.372400</td>\n","      <td>0.432803</td>\n","      <td>0.611964</td>\n","      <td>0.607165</td>\n","      <td>0.817301</td>\n","      <td>0.774429</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.323800</td>\n","      <td>0.437401</td>\n","      <td>0.707597</td>\n","      <td>0.672974</td>\n","      <td>0.776170</td>\n","      <td>0.895965</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.237000</td>\n","      <td>0.389542</td>\n","      <td>0.701080</td>\n","      <td>0.660426</td>\n","      <td>0.841277</td>\n","      <td>0.869713</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.327800</td>\n","      <td>0.403065</td>\n","      <td>0.697631</td>\n","      <td>0.657812</td>\n","      <td>0.837179</td>\n","      <td>0.867769</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.175800</td>\n","      <td>0.434782</td>\n","      <td>0.710932</td>\n","      <td>0.669617</td>\n","      <td>0.821706</td>\n","      <td>0.884298</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 2057\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 2057\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 2057\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 2057\n","  Batch size = 64\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 2057\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]}],"source":["train_result = trainer.train()"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to models/scibert_scivocab_uncased-re-P_P\n","Configuration saved in models/scibert_scivocab_uncased-re-P_P/config.json\n","Model weights saved in models/scibert_scivocab_uncased-re-P_P/pytorch_model.bin\n","tokenizer config file saved in models/scibert_scivocab_uncased-re-P_P/tokenizer_config.json\n","Special tokens file saved in models/scibert_scivocab_uncased-re-P_P/special_tokens_map.json\n"]},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =        5.0\n","  total_flos               =  2953218GF\n","  train_loss               =     0.3607\n","  train_runtime            = 0:10:58.19\n","  train_samples            =       8227\n","  train_samples_per_second =     62.497\n","  train_steps_per_second   =       0.98\n"]}],"source":["metrics = train_result.metrics\n","metrics[\"train_samples\"] = len(train_dataset)\n","\n","trainer.save_model(f\"models/{model_folder_name}\")  # Saves the tokenizer too for easy upload\n","\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Evaluation *****\n","  Num examples = 2057\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["*** Evaluate ***\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='195' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [33/33 01:10]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =        5.0\n","  eval_accuracy           =     0.8843\n","  eval_f1                 =     0.7109\n","  eval_loss               =     0.4348\n","  eval_precision          =     0.6696\n","  eval_recall             =     0.8217\n","  eval_runtime            = 0:00:11.14\n","  eval_samples            =       2057\n","  eval_samples_per_second =    184.569\n","  eval_steps_per_second   =      2.961\n"]}],"source":["print(\"*** Evaluate ***\") \n","metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","metrics[\"eval_samples\"] = len(eval_dataset)\n","\n","trainer.log_metrics(\"eval\", metrics)\n","trainer.save_metrics(\"eval\", metrics)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 2057\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Other       0.98      0.90      0.93      1906\n","         PIP       0.36      0.75      0.49       151\n","\n","    accuracy                           0.88      2057\n","   macro avg       0.67      0.82      0.71      2057\n","weighted avg       0.93      0.88      0.90      2057\n","\n"]}],"source":["predictions, labels, _ = trainer.predict(eval_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n","***** Running Prediction *****\n","  Num examples = 8227\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Other       1.00      0.91      0.95      7623\n","         PIP       0.46      0.96      0.62       604\n","\n","    accuracy                           0.92      8227\n","   macro avg       0.73      0.93      0.79      8227\n","weighted avg       0.96      0.92      0.93      8227\n","\n"]}],"source":["predictions, labels, _ = trainer.predict(train_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","print(classification_report(labels, predictions,target_names=label_list))"]},{"cell_type":"markdown","metadata":{},"source":["## Final Predictions"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 128/128 [00:00<00:00, 1032.03it/s]\n","100%|██████████| 128/128 [00:01<00:00, 82.97it/s]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>start_line</th>\n","      <th>concept_text_x</th>\n","      <th>concept_text_y</th>\n","      <th>concept_type_x</th>\n","      <th>concept_type_y</th>\n","      <th>start_word_number_x</th>\n","      <th>end_word_number_x</th>\n","      <th>start_word_number_y</th>\n","      <th>end_word_number_y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>cardiac cath</td>\n","      <td>vt</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>stent</td>\n","      <td>vt</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>amp</td>\n","      <td>vt</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>amio loading</td>\n","      <td>vt</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>9</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0006</td>\n","      <td>87</td>\n","      <td>house</td>\n","      <td>low chol</td>\n","      <td>problem</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14330</th>\n","      <td>0475</td>\n","      <td>109</td>\n","      <td>the serum albumin gradient</td>\n","      <td>portal hypertension</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>8</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>14331</th>\n","      <td>0475</td>\n","      <td>112</td>\n","      <td>her platelet count</td>\n","      <td>low</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>14332</th>\n","      <td>0475</td>\n","      <td>112</td>\n","      <td>cimetidine</td>\n","      <td>low</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>5</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>14333</th>\n","      <td>0475</td>\n","      <td>120</td>\n","      <td>beta blocker</td>\n","      <td>a persistent wheeze</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>9</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>14334</th>\n","      <td>0475</td>\n","      <td>120</td>\n","      <td>the beta blocker</td>\n","      <td>a persistent wheeze</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>20</td>\n","      <td>22</td>\n","      <td>5</td>\n","      <td>7</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14335 rows × 10 columns</p>\n","</div>"],"text/plain":["      filename  start_line              concept_text_x       concept_text_y  \\\n","0         0006           5                cardiac cath                   vt   \n","1         0006           5                       stent                   vt   \n","2         0006           5                         amp                   vt   \n","3         0006           5                amio loading                   vt   \n","4         0006          87                       house             low chol   \n","...        ...         ...                         ...                  ...   \n","14330     0475         109  the serum albumin gradient  portal hypertension   \n","14331     0475         112          her platelet count                  low   \n","14332     0475         112                  cimetidine                  low   \n","14333     0475         120                beta blocker  a persistent wheeze   \n","14334     0475         120            the beta blocker  a persistent wheeze   \n","\n","      concept_type_x concept_type_y  start_word_number_x  end_word_number_x  \\\n","0               test        problem                    2                  3   \n","1          treatment        problem                    5                  5   \n","2          treatment        problem                    7                  7   \n","3          treatment        problem                    9                 10   \n","4            problem        problem                    0                  0   \n","...              ...            ...                  ...                ...   \n","14330           test        problem                    0                  3   \n","14331           test        problem                    0                  2   \n","14332      treatment        problem                   15                 15   \n","14333      treatment        problem                    9                 10   \n","14334      treatment        problem                   20                 22   \n","\n","       start_word_number_y  end_word_number_y  \n","0                        0                  0  \n","1                        0                  0  \n","2                        0                  0  \n","3                        0                  0  \n","4                        2                  3  \n","...                    ...                ...  \n","14330                    8                  9  \n","14331                    5                  5  \n","14332                    5                  5  \n","14333                    5                  7  \n","14334                    5                  7  \n","\n","[14335 rows x 10 columns]"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["text_files = glob.glob(val_data_path + os.sep + txt_folder_name + os.sep +  \"*.txt\")\n","filename = \"\"\n","df = pd.DataFrame()\n","for file in tqdm(text_files):\n","    with open(file, 'r') as f:\n","        text = f.read()\n","        # split lines\n","        lines = text.split('\\n')\n","        filename =[ file.split(\"/\")[-1].split(\".\")[0]] * len(lines)\n","        df = df.append(pd.DataFrame({\"text\": lines, \"filename\": filename, \"line_number\": range(len(lines))}), ignore_index=True)\n","\n","df = df.sort_values(by=[\"filename\", \"line_number\"])\n","# remove empty text lines\n","# df = df[df.text != \"\"]\n","# df = df.reset_index(drop=True)\n","\n","# add concepts\n","rel_df = pd.DataFrame()\n","for fname in tqdm(df[\"filename\"].unique()):\n","    concept_dict = parse_concept(val_data_path + os.sep + concept_folder_name + os.sep + fname + \".con\")\n","    \n","    concept_df = pd.DataFrame(concept_dict).drop(columns=[\"end_line\"])\n","    test_concept_df = concept_df[concept_df[\"concept_type\"] == \"test\"]\n","    problem_concept_df = concept_df[concept_df[\"concept_type\"] == \"problem\"]\n","    treatment_concept_df = concept_df[concept_df[\"concept_type\"] == \"treatment\"]\n","\n","    # class test --> problem\n","    test_problem_df = pd.merge(test_concept_df, problem_concept_df, how=\"inner\", on=\"start_line\")\n","\n","    # class treatment --> problem\n","    treatment_problem_df = pd.merge(treatment_concept_df, problem_concept_df, how=\"inner\", on=\"start_line\")\n","\n","    # class problem --> problem\n","    problem_problem_df = pd.merge(problem_concept_df, problem_concept_df, how=\"inner\", on=\"start_line\")\n","    problem_problem_df = problem_problem_df[problem_problem_df[\"concept_text_x\"] != problem_problem_df[\"concept_text_y\"]] # TODO: remove duplicates ?\n","\n","    tmp = pd.concat([test_problem_df, treatment_problem_df, problem_problem_df], axis=0)\n","    tmp[\"filename\"] = fname\n","    rel_df = rel_df.append(tmp, ignore_index=True)\n","            \n","rel_df = rel_df.sort_values(by=[\"filename\", \"start_line\"])\n","rel_df = rel_df.reset_index(drop=True)\n","\n","rel_df = rel_df[[ \"filename\", \"start_line\", \"concept_text_x\", \"concept_text_y\", \"concept_type_x\", \"concept_type_y\", \"start_word_number_x\", \"end_word_number_x\", \"start_word_number_y\", \"end_word_number_y\"]]\n","rel_df"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>start_line</th>\n","      <th>concept_text_x</th>\n","      <th>concept_text_y</th>\n","      <th>concept_type_x</th>\n","      <th>concept_type_y</th>\n","      <th>start_word_number_x</th>\n","      <th>end_word_number_x</th>\n","      <th>start_word_number_y</th>\n","      <th>end_word_number_y</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>cardiac cath</td>\n","      <td>vt</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[[ VT ]] s/p &lt;&lt; cardiac cath &gt;&gt; , stent and am...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>stent</td>\n","      <td>vt</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[[ VT ]] s/p cardiac cath , &lt;&lt; stent &gt;&gt; and am...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>amp</td>\n","      <td>vt</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[[ VT ]] s/p cardiac cath , stent and &lt;&lt; amp &gt;...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>amio loading</td>\n","      <td>vt</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>9</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[[ VT ]] s/p cardiac cath , stent and amp ; &lt;&lt;...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0006</td>\n","      <td>87</td>\n","      <td>house</td>\n","      <td>low chol</td>\n","      <td>problem</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>&lt;&lt; House &gt;&gt; / [[ Low chol ]] / low sat. fat</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14330</th>\n","      <td>0475</td>\n","      <td>109</td>\n","      <td>the serum albumin gradient</td>\n","      <td>portal hypertension</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>8</td>\n","      <td>9</td>\n","      <td>&lt;&lt; The serum albumin gradient &gt;&gt; was 1.8 consi...</td>\n","    </tr>\n","    <tr>\n","      <th>14331</th>\n","      <td>0475</td>\n","      <td>112</td>\n","      <td>her platelet count</td>\n","      <td>low</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>&lt;&lt; Her platelet count &gt;&gt; stayed persistently [...</td>\n","    </tr>\n","    <tr>\n","      <th>14332</th>\n","      <td>0475</td>\n","      <td>112</td>\n","      <td>cimetidine</td>\n","      <td>low</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>Her platelet count stayed persistently [[ low ...</td>\n","    </tr>\n","    <tr>\n","      <th>14333</th>\n","      <td>0475</td>\n","      <td>120</td>\n","      <td>beta blocker</td>\n","      <td>a persistent wheeze</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>9</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>The patient however complained of [[ a persist...</td>\n","    </tr>\n","    <tr>\n","      <th>14334</th>\n","      <td>0475</td>\n","      <td>120</td>\n","      <td>the beta blocker</td>\n","      <td>a persistent wheeze</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>20</td>\n","      <td>22</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>The patient however complained of [[ a persist...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14335 rows × 11 columns</p>\n","</div>"],"text/plain":["      filename  start_line              concept_text_x       concept_text_y  \\\n","0         0006           5                cardiac cath                   vt   \n","1         0006           5                       stent                   vt   \n","2         0006           5                         amp                   vt   \n","3         0006           5                amio loading                   vt   \n","4         0006          87                       house             low chol   \n","...        ...         ...                         ...                  ...   \n","14330     0475         109  the serum albumin gradient  portal hypertension   \n","14331     0475         112          her platelet count                  low   \n","14332     0475         112                  cimetidine                  low   \n","14333     0475         120                beta blocker  a persistent wheeze   \n","14334     0475         120            the beta blocker  a persistent wheeze   \n","\n","      concept_type_x concept_type_y  start_word_number_x  end_word_number_x  \\\n","0               test        problem                    2                  3   \n","1          treatment        problem                    5                  5   \n","2          treatment        problem                    7                  7   \n","3          treatment        problem                    9                 10   \n","4            problem        problem                    0                  0   \n","...              ...            ...                  ...                ...   \n","14330           test        problem                    0                  3   \n","14331           test        problem                    0                  2   \n","14332      treatment        problem                   15                 15   \n","14333      treatment        problem                    9                 10   \n","14334      treatment        problem                   20                 22   \n","\n","       start_word_number_y  end_word_number_y  \\\n","0                        0                  0   \n","1                        0                  0   \n","2                        0                  0   \n","3                        0                  0   \n","4                        2                  3   \n","...                    ...                ...   \n","14330                    8                  9   \n","14331                    5                  5   \n","14332                    5                  5   \n","14333                    5                  7   \n","14334                    5                  7   \n","\n","                                                    text  \n","0      [[ VT ]] s/p << cardiac cath >> , stent and am...  \n","1      [[ VT ]] s/p cardiac cath , << stent >> and am...  \n","2      [[ VT ]] s/p cardiac cath , stent and << amp >...  \n","3      [[ VT ]] s/p cardiac cath , stent and amp ; <<...  \n","4            << House >> / [[ Low chol ]] / low sat. fat  \n","...                                                  ...  \n","14330  << The serum albumin gradient >> was 1.8 consi...  \n","14331  << Her platelet count >> stayed persistently [...  \n","14332  Her platelet count stayed persistently [[ low ...  \n","14333  The patient however complained of [[ a persist...  \n","14334  The patient however complained of [[ a persist...  \n","\n","[14335 rows x 11 columns]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["# make predict dataset\n","def preprocess_text(row):\n","    # find line\n","    line =  df[(df[\"filename\"] == row[\"filename\"]) & (df[\"line_number\"] == row[\"start_line\"]-1)][\"text\"].values[0]\n","    # line = line.lower()\n","    line = \" \".join(line.split()) # remove multiple spaces\n","\n","    concept_text_x = \"<< \"+ \" \".join(line.split()[row[\"start_word_number_x\"]:row[\"end_word_number_x\"]+1]) + \" >>\"\n","    concept_text_y = \"[[ \" + \" \".join(line.split()[row[\"start_word_number_y\"]:row[\"end_word_number_y\"]+1]) + \" ]]\"\n","    start_word_number_x = row[\"start_word_number_x\"]\n","    end_word_number_x = row[\"end_word_number_x\"]\n","    start_word_number_y = row[\"start_word_number_y\"]\n","    end_word_number_y = row[\"end_word_number_y\"]\n","\n","    if row[\"start_word_number_x\"] > row[\"start_word_number_y\"]:\n","        concept_text_x, concept_text_y = concept_text_y, concept_text_x\n","        start_word_number_x, start_word_number_y = start_word_number_y, start_word_number_x\n","        end_word_number_x, end_word_number_y = end_word_number_y, end_word_number_x\n","    text = \" \".join(line.split()[: start_word_number_x] + [concept_text_x] + line.split()[end_word_number_x+1: start_word_number_y] + [concept_text_y] + line.split()[end_word_number_y+1:])\n","\n","    row[\"text\"] = text\n","    return row\n","\n","predict_df = rel_df.apply(preprocess_text, axis=1)\n","predict_df"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["orig_predict_df = predict_df.copy()"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file models/scibert_scivocab_uncased-re-Te_P/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"models/scibert_scivocab_uncased-re-Te_P\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"re\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Other\",\n","    \"1\": \"TeRP\",\n","    \"2\": \"TeCP\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Other\": 0,\n","    \"TeCP\": 2,\n","    \"TeRP\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 31090\n","}\n","\n","loading weights file models/scibert_scivocab_uncased-re-Te_P/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at models/scibert_scivocab_uncased-re-Te_P.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","Didn't find file models/scibert_scivocab_uncased-re-Te_P/added_tokens.json. We won't load it.\n","loading file models/scibert_scivocab_uncased-re-Te_P/vocab.txt\n","loading file models/scibert_scivocab_uncased-re-Te_P/tokenizer.json\n","loading file None\n","loading file models/scibert_scivocab_uncased-re-Te_P/special_tokens_map.json\n","loading file models/scibert_scivocab_uncased-re-Te_P/tokenizer_config.json\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["re_task = \"Te_P\"\n","\n","if re_task == \"P_P\":\n","    # problem --> problem\n","    predict_df = orig_predict_df[(orig_predict_df[\"concept_type_x\"] == \"problem\") & (orig_predict_df[\"concept_type_y\"] == \"problem\")]\n","    label2id = {'Other': 0, 'PIP': 1}\n","elif re_task == \"Tr_P\":\n","    # treatment --> problem\n","    predict_df = orig_predict_df[(orig_predict_df[\"concept_type_x\"] == \"treatment\") & (orig_predict_df[\"concept_type_y\"] == \"problem\")]\n","    label2id = {'Other': 0, 'TrAP': 1, 'TrCP': 2, 'TrNAP': 3, 'TrIP': 4, 'TrWP': 5}\n","elif re_task == \"Te_P\":\n","    # test --> problem\n","    predict_df = orig_predict_df[(orig_predict_df[\"concept_type_x\"] == \"test\") & (orig_predict_df[\"concept_type_y\"] == \"problem\")]\n","    label2id = {'Other': 0, 'TeRP': 1, 'TeCP': 2}\n","id2label = {v: k for k, v in label2id.items()}\n","\n","\n","model_folder_name = f\"{model_name_or_path.split('/')[-1]}-re-{re_task}\"\n","model = AutoModelForSequenceClassification.from_pretrained(f\"models/{model_folder_name}\", label2id=label2id, id2label=id2label)\n","tokenizer = AutoTokenizer.from_pretrained(f\"models/{model_folder_name}\")\n","\n","# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","if pad_to_max_length:\n","    data_collator = default_data_collator\n","elif fp16:\n","    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","else:\n","    data_collator = None\n","    \n","# Initialize our Trainer\n","args = TrainingArguments(\n","    f\"training_logs/{model_folder_name}\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"no\",\n","    learning_rate=1e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=5,\n","    weight_decay=0.05,\n","    logging_steps=1,\n","    warmup_ratio=0.1,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    # args=args,\n","    # train_dataset=train_dataset ,\n","    # eval_dataset=eval_dataset ,\n","    # compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46fd481d2aae4416995e059d2c0aa38a","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on prediction dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['filename', 'start_line', 'concept_text_x', 'concept_text_y', 'concept_type_x', 'concept_type_y', 'start_word_number_x', 'end_word_number_x', 'start_word_number_y', 'end_word_number_y', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 2261\n","})"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["# Preprocessing the datasets\n","# Padding strategy\n","if pad_to_max_length:\n","    padding = \"max_length\"\n","else:\n","    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","    padding = False\n","\n","\n","def preprocess_function(examples):\n","    # Tokenize the texts\n","    return tokenizer(\n","        examples[\"text\"],\n","        padding=padding,\n","        max_length=max_seq_length,\n","        truncation=True,\n","    )\n","\n","predict_dataset = Dataset.from_pandas(predict_df, preserve_index=False)\n","# predict_dataset = predict_dataset.select(range(10))\n","predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","predict_dataset"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: filename, start_line, concept_text_y, start_word_number_x, end_word_number_x, concept_type_x, concept_type_y, end_word_number_y, start_word_number_y, concept_text_x, text.\n","***** Running Prediction *****\n","  Num examples = 2261\n","  Batch size = 16\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [142/142 00:11]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["2261"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","predictions = np.argmax(predictions, axis=1)\n","len(predictions)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>start_line</th>\n","      <th>concept_text_x</th>\n","      <th>concept_text_y</th>\n","      <th>concept_type_x</th>\n","      <th>concept_type_y</th>\n","      <th>start_word_number_x</th>\n","      <th>end_word_number_x</th>\n","      <th>start_word_number_y</th>\n","      <th>end_word_number_y</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>cardiac cath</td>\n","      <td>vt</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>stent</td>\n","      <td>vt</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>amp</td>\n","      <td>vt</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0006</td>\n","      <td>5</td>\n","      <td>amio loading</td>\n","      <td>vt</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>9</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0006</td>\n","      <td>87</td>\n","      <td>house</td>\n","      <td>low chol</td>\n","      <td>problem</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14330</th>\n","      <td>0475</td>\n","      <td>109</td>\n","      <td>the serum albumin gradient</td>\n","      <td>portal hypertension</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>8</td>\n","      <td>9</td>\n","      <td>TeRP</td>\n","    </tr>\n","    <tr>\n","      <th>14331</th>\n","      <td>0475</td>\n","      <td>112</td>\n","      <td>her platelet count</td>\n","      <td>low</td>\n","      <td>test</td>\n","      <td>problem</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>TeRP</td>\n","    </tr>\n","    <tr>\n","      <th>14332</th>\n","      <td>0475</td>\n","      <td>112</td>\n","      <td>cimetidine</td>\n","      <td>low</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>TrCP</td>\n","    </tr>\n","    <tr>\n","      <th>14333</th>\n","      <td>0475</td>\n","      <td>120</td>\n","      <td>beta blocker</td>\n","      <td>a persistent wheeze</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>9</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>TrNAP</td>\n","    </tr>\n","    <tr>\n","      <th>14334</th>\n","      <td>0475</td>\n","      <td>120</td>\n","      <td>the beta blocker</td>\n","      <td>a persistent wheeze</td>\n","      <td>treatment</td>\n","      <td>problem</td>\n","      <td>20</td>\n","      <td>22</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>TrNAP</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14335 rows × 11 columns</p>\n","</div>"],"text/plain":["      filename  start_line              concept_text_x       concept_text_y  \\\n","0         0006           5                cardiac cath                   vt   \n","1         0006           5                       stent                   vt   \n","2         0006           5                         amp                   vt   \n","3         0006           5                amio loading                   vt   \n","4         0006          87                       house             low chol   \n","...        ...         ...                         ...                  ...   \n","14330     0475         109  the serum albumin gradient  portal hypertension   \n","14331     0475         112          her platelet count                  low   \n","14332     0475         112                  cimetidine                  low   \n","14333     0475         120                beta blocker  a persistent wheeze   \n","14334     0475         120            the beta blocker  a persistent wheeze   \n","\n","      concept_type_x concept_type_y  start_word_number_x  end_word_number_x  \\\n","0               test        problem                    2                  3   \n","1          treatment        problem                    5                  5   \n","2          treatment        problem                    7                  7   \n","3          treatment        problem                    9                 10   \n","4            problem        problem                    0                  0   \n","...              ...            ...                  ...                ...   \n","14330           test        problem                    0                  3   \n","14331           test        problem                    0                  2   \n","14332      treatment        problem                   15                 15   \n","14333      treatment        problem                    9                 10   \n","14334      treatment        problem                   20                 22   \n","\n","       start_word_number_y  end_word_number_y prediction  \n","0                        0                  0      Other  \n","1                        0                  0      Other  \n","2                        0                  0      Other  \n","3                        0                  0      Other  \n","4                        2                  3      Other  \n","...                    ...                ...        ...  \n","14330                    8                  9       TeRP  \n","14331                    5                  5       TeRP  \n","14332                    5                  5       TrCP  \n","14333                    5                  7      TrNAP  \n","14334                    5                  7      TrNAP  \n","\n","[14335 rows x 11 columns]"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["predict_df[\"prediction\"] = [id2label[label] for label in predictions]\n","rel_df.loc[predict_df.index, \"prediction\"] = predict_df[\"prediction\"]\n","rel_df"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/plain":["Other    9776\n","PIP      1564\n","TeRP     1066\n","TrAP      648\n","TrCP      435\n","TrIP      403\n","TeCP      272\n","TrNAP     160\n","TrWP       11\n","Name: prediction, dtype: int64"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["rel_df[\"prediction\"].value_counts()\n"]},{"cell_type":"markdown","metadata":{},"source":["you can now set another re_task"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["14335it [00:01, 13333.71it/s]\n"]}],"source":["# for each file create <filename>.con\n","os.makedirs(val_data_path + os.sep + rel_folder_name, exist_ok=True)\n","# empty folder if exists\n","files = glob.glob(val_data_path + os.sep + rel_folder_name + os.sep + \"*.rel\")\n","for file in files:\n","    os.remove(file)\n","\n","for i, row in tqdm(rel_df.iterrows()):\n","    filename = row[\"filename\"]\n","    concept_text_x = row[\"concept_text_x\"]\n","    concept_text_y = row[\"concept_text_y\"]\n","    concept_type_x = row[\"concept_type_x\"]\n","    concept_type_y = row[\"concept_type_y\"]\n","    start_word_number_x = row[\"start_word_number_x\"]\n","    end_word_number_x = row[\"end_word_number_x\"]\n","    start_word_number_y = row[\"start_word_number_y\"]\n","    end_word_number_y = row[\"end_word_number_y\"]\n","    line_number = row[\"start_line\"]\n","    prediction = row[\"prediction\"]\n","    if prediction != \"Other\":\n","        with open(val_data_path + os.sep + rel_folder_name + os.sep + filename + \".rel\", \"a\") as f:\n","            # fill like this c=\"pefusion imaging\" 19:6 19:7||r=\"TeRP\"||c=\"perfusion defects\" 19:12 19:13\n","            f.write(\n","                f\"c=\\\"{concept_text_x}\\\" {line_number}:{start_word_number_x} {line_number}:{end_word_number_x}||r=\\\"{prediction}\\\"||c=\\\"{concept_text_y}\\\" {line_number}:{start_word_number_y} {line_number}:{end_word_number_y}\\n\"\n","            )\n","    \n"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"data":{"text/plain":["{'0049', '0066', '0146', '0202', '0230', '0305', '0366', '0398'}"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["rel_files = glob.glob(val_data_path + os.sep + rel_folder_name + os.sep + \"*.rel\")\n","rel_files = [f.split(os.sep)[-1][:-4] for f in rel_files]\n","txt_files = [f.split(os.sep)[-1][:-4] for f in text_files]\n","# find missing files\n","missing_files = set(txt_files) - set(rel_files)\n","missing_files\n"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["# create empty files for missing files\n","for f in missing_files:\n","    with open(val_data_path + os.sep + rel_folder_name + os.sep + f + \".rel\", \"w\") as f:\n","        f.write(\"\")"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"data":{"text/plain":["set()"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["rel_files = glob.glob(val_data_path + os.sep + rel_folder_name + os.sep + \"*.rel\")\n","rel_files = [f.split(os.sep)[-1][:-4] for f in rel_files]\n","txt_files = [f.split(os.sep)[-1][:-4] for f in text_files]\n","# find missing files\n","missing_files = set(txt_files) - set(rel_files)\n","missing_files"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","c=\"house\" 87:0 87:0||r=\"PIP\"||c=\"low sat\" 87:5 87:6\n","c=\"fat\" 87:7 87:7||r=\"PIP\"||c=\"house\" 87:0 87:0\n","c=\"fat\" 87:7 87:7||r=\"PIP\"||c=\"low chol\" 87:2 87:3\n","c=\"monitor\" 104:10 104:10||r=\"TeRP\"||c=\"vt\" 104:8 104:8\n","c=\"further icd shocks\" 104:4 104:6||r=\"TrCP\"||c=\"vt\" 104:8 104:8\n","c=\"exam\" 106:0 106:0||r=\"TeRP\"||c=\"b / l carotid bruits\" 106:3 106:7\n","c=\"exam\" 106:0 106:0||r=\"TeRP\"||c=\"soft murmur at l sternal border\" 106:9 106:14\n","c=\"exam\" 106:0 106:0||r=\"TeRP\"||c=\"1 + b / l edema\" 106:19 106:23\n","c=\"ecg\" 117:0 117:0||r=\"TeRP\"||c=\"prior\" 117:5 117:5\n","c=\"ecg\" 117:0 117:0||r=\"TeRP\"||c=\"shower r\" 117:6 117:7\n","c=\"ecg\" 117:0 117:0||r=\"TeRP\"||c=\"vp\" 117:10 117:10\n","c=\"ecg\" 117:0 117:0||r=\"TeRP\"||c=\"monomorphic vt\" 117:12 117:13\n","c=\"av paced\" 117:2 117:3||r=\"TrAP\"||c=\"prior\" 117:5 117:5\n","c=\"av paced\" 117:2 117:3||r=\"TrAP\"||c=\"shower r\" 117:6 117:7\n","c=\"av paced\" 117:2 117:3||r=\"TrAP\"||c=\"vp\" 117:10 117:10\n","c=\"av paced\" 117:2 117:3||r=\"TrAP\"||c=\"monomorphic vt\" 117:12 117:13\n","c=\"t\" 117:9 117:9||r=\"TrAP\"||c=\"shower r\" 117:6 117:7\n","c=\"t\" 117:9 117:9||r=\"TrAP\"||c=\"vp\" 117:10 117:10\n","c=\"t\" 117:9 117:9||r=\"TrAP\"||c=\"monomorphic vt\" 117:12 117:13\n","c=\"##b\" 117:10 117:10||r=\"TrAP\"||c=\"prior\" 117:5 117:5\n","c=\"##b\" 117:10 117:10||r=\"TrAP\"||c=\"shower r\" 117:6 117:7\n","c=\"##b\" 117:10 117:10||r=\"TrAP\"||c=\"vp\" 117:10 117:10\n","c=\"##b\" 117:10 117:10||r=\"TrAP\"||c=\"monomorphic vt\" 117:12 117:13\n","c=\"overdrive paced\" 117:15 117:16||r=\"TrAP\"||c=\"prior\" 117:5 117:5\n","c=\"overdrive paced\" 117:15 117:16||r=\"TrAP\"||c=\"shower r\" 117:6 117:7\n","c=\"overdrive paced\" 117:15 117:16||r=\"TrAP\"||c=\"vp\" 117:10 117:10\n","c=\"overdrive paced\" 117:15 117:16||r=\"TrAP\"||c=\"monomorphic vt\" 117:12 117:13\n","c=\"prior\" 117:5 117:5||r=\"PIP\"||c=\"shower r\" 117:6 117:7\n","c=\"prior\" 117:5 117:5||r=\"PIP\"||c=\"vp\" 117:10 117:10\n","c=\"prior\" 117:5 117:5||r=\"PIP\"||c=\"monomorphic vt\" 117:12 117:13\n","c=\"shower r\" 117:6 117:7||r=\"PIP\"||c=\"prior\" 117:5 117:5\n","c=\"shower r\" 117:6 117:7||r=\"PIP\"||c=\"vp\" 117:10 117:10\n","c=\"shower r\" 117:6 117:7||r=\"PIP\"||c=\"monomorphic vt\" 117:12 117:13\n","c=\"lispro\" 121:12 121:12||r=\"TrNAP\"||c=\"elevated glu\" 121:0 121:1\n","c=\"baseline\" 125:0 125:0||r=\"TeRP\"||c=\"elevated\" 125:12 125:12\n","c=\"lfts\" 125:1 125:1||r=\"TeRP\"||c=\"elevated\" 125:12 125:12\n","c=\"glu\" 125:10 125:10||r=\"TeRP\"||c=\"elevated\" 125:12 125:12\n","c=\"cardiac / ada diet\" 125:5 125:8||r=\"TrAP\"||c=\"elevated\" 125:12 125:12\n","c=\"adenosine stress mibi\" 132:0 132:2||r=\"TeRP\"||c=\"a small reversible defect in the pda territory\" 132:4 132:11\n","c=\"adenosine stress mibi\" 132:0 132:2||r=\"TeRP\"||c=\"a 70 % lesion in the svg to pda\" 132:22 132:29\n","c=\"coronary catheterisation\" 132:16 132:17||r=\"TrCP\"||c=\"a small reversible defect in the pda territory\" 132:4 132:11\n","c=\"coronary catheterisation\" 132:16 132:17||r=\"TrIP\"||c=\"a 70 % lesion in the svg to pda\" 132:22 132:29\n","c=\"cypher\" 132:31 132:31||r=\"TrCP\"||c=\"a small reversible defect in the pda territory\" 132:4 132:11\n","c=\"cypher\" 132:31 132:31||r=\"TrCP\"||c=\"a 70 % lesion in the svg to pda\" 132:22 132:29\n","c=\"occasional ventricular extopy\" 133:2 133:4||r=\"TeRP\"||c=\"ventricular tachycardia\" 133:11 133:12\n","c=\"telemetry\" 133:5 133:5||r=\"TeRP\"||c=\"ventricular tachycardia\" 133:11 133:12\n","c=\"his am fasting glucose levels\" 134:4 134:8||r=\"TeRP\"||c=\"intermittently elevated\" 134:10 134:11\n","c=\"dietary control\" 135:30 135:31||r=\"TrNAP\"||c=\"some glucose intolerance\" 135:10 135:12\n"]}],"source":["!cat data/val/rel/0006.rel"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","  adding: data/val/rel/ (stored 0%)\n","  adding: data/val/rel/0309.rel (deflated 77%)\n","  adding: data/val/rel/0085.rel (deflated 56%)\n","  adding: data/val/rel/0101.rel (deflated 76%)\n","  adding: data/val/rel/0382.rel (deflated 65%)\n","  adding: data/val/rel/0282.rel (deflated 56%)\n","  adding: data/val/rel/0194.rel (deflated 80%)\n","  adding: data/val/rel/0161.rel (deflated 78%)\n","  adding: data/val/rel/0169.rel (deflated 70%)\n","  adding: data/val/rel/0049.rel (stored 0%)\n","  adding: data/val/rel/0317.rel (deflated 65%)\n","  adding: data/val/rel/0294.rel (deflated 64%)\n","  adding: data/val/rel/0273.rel (deflated 70%)\n","  adding: data/val/rel/0369.rel (deflated 78%)\n","  adding: data/val/rel/0464.rel (deflated 74%)\n","  adding: data/val/rel/0329.rel (deflated 75%)\n","  adding: data/val/rel/0093.rel (deflated 71%)\n","  adding: data/val/rel/0218.rel (deflated 67%)\n","  adding: data/val/rel/0381.rel (deflated 65%)\n","  adding: data/val/rel/0333.rel (deflated 71%)\n","  adding: data/val/rel/0134.rel (deflated 56%)\n","  adding: data/val/rel/0033.rel (deflated 67%)\n","  adding: data/val/rel/0246.rel (deflated 71%)\n","  adding: data/val/rel/0202.rel (stored 0%)\n","  adding: data/val/rel/0121.rel (deflated 71%)\n","  adding: data/val/rel/0117.rel (deflated 63%)\n","  adding: data/val/rel/0446.rel (deflated 77%)\n","  adding: data/val/rel/0061.rel (deflated 78%)\n","  adding: data/val/rel/0266.rel (deflated 75%)\n","  adding: data/val/rel/0314.rel (deflated 84%)\n","  adding: data/val/rel/0330.rel (deflated 73%)\n","  adding: data/val/rel/0166.rel (deflated 63%)\n","  adding: data/val/rel/0402.rel (deflated 74%)\n","  adding: data/val/rel/0133.rel (deflated 58%)\n","  adding: data/val/rel/0225.rel (deflated 66%)\n","  adding: data/val/rel/0229.rel (deflated 75%)\n","  adding: data/val/rel/0070.rel (deflated 69%)\n","  adding: data/val/rel/0362.rel (deflated 72%)\n","  adding: data/val/rel/0468.rel (deflated 61%)\n","  adding: data/val/rel/0345.rel (deflated 77%)\n","  adding: data/val/rel/0237.rel (deflated 77%)\n","  adding: data/val/rel/0186.rel (deflated 63%)\n","  adding: data/val/rel/0141.rel (deflated 72%)\n","  adding: data/val/rel/0289.rel (deflated 73%)\n","  adding: data/val/rel/0189.rel (deflated 66%)\n","  adding: data/val/rel/0258.rel (deflated 45%)\n","  adding: data/val/rel/0278.rel (deflated 68%)\n","  adding: data/val/rel/0471.rel (deflated 70%)\n","  adding: data/val/rel/0038.rel (deflated 73%)\n","  adding: data/val/rel/.ipynb_checkpoints/ (stored 0%)\n","  adding: data/val/rel/.ipynb_checkpoints/0006-checkpoint.rel (deflated 87%)\n","  adding: data/val/rel/.ipynb_checkpoints/0061-checkpoint.rel (deflated 84%)\n","  adding: data/val/rel/0409.rel (deflated 75%)\n","  adding: data/val/rel/0193.rel (deflated 72%)\n","  adding: data/val/rel/0342.rel (deflated 72%)\n","  adding: data/val/rel/0398.rel (stored 0%)\n","  adding: data/val/rel/0069.rel (deflated 65%)\n","  adding: data/val/rel/0350.rel (deflated 71%)\n","  adding: data/val/rel/0198.rel (deflated 75%)\n","  adding: data/val/rel/0445.rel (deflated 77%)\n","  adding: data/val/rel/0206.rel (deflated 57%)\n","  adding: data/val/rel/0430.rel (deflated 74%)\n","  adding: data/val/rel/0122.rel (deflated 70%)\n","  adding: data/val/rel/0472.rel (deflated 70%)\n","  adding: data/val/rel/0025.rel (deflated 64%)\n","  adding: data/val/rel/0422.rel (deflated 73%)\n","  adding: data/val/rel/0386.rel (deflated 63%)\n","  adding: data/val/rel/0150.rel (deflated 63%)\n","  adding: data/val/rel/0217.rel (deflated 69%)\n","  adding: data/val/rel/0105.rel (deflated 76%)\n","  adding: data/val/rel/0337.rel (deflated 76%)\n","  adding: data/val/rel/0397.rel (deflated 65%)\n","  adding: data/val/rel/0146.rel (stored 0%)\n","  adding: data/val/rel/0341.rel (deflated 70%)\n","  adding: data/val/rel/0469.rel (deflated 74%)\n","  adding: data/val/rel/0377.rel (deflated 70%)\n","  adding: data/val/rel/0433.rel (deflated 73%)\n","  adding: data/val/rel/0366.rel (stored 0%)\n","  adding: data/val/rel/0467.rel (deflated 59%)\n","  adding: data/val/rel/0041.rel (deflated 72%)\n","  adding: data/val/rel/0197.rel (deflated 11%)\n","  adding: data/val/rel/0077.rel (deflated 77%)\n","  adding: data/val/rel/0213.rel (deflated 65%)\n","  adding: data/val/rel/0034.rel (deflated 61%)\n","  adding: data/val/rel/0424.rel (deflated 72%)\n","  adding: data/val/rel/0249.rel (deflated 53%)\n","  adding: data/val/rel/0455.rel (deflated 70%)\n","  adding: data/val/rel/0153.rel (deflated 72%)\n","  adding: data/val/rel/0170.rel (deflated 9%)\n","  adding: data/val/rel/0357.rel (deflated 55%)\n","  adding: data/val/rel/0066.rel (stored 0%)\n","  adding: data/val/rel/0013.rel (deflated 72%)\n","  adding: data/val/rel/0421.rel (deflated 73%)\n","  adding: data/val/rel/0334.rel (deflated 73%)\n","  adding: data/val/rel/0269.rel (deflated 54%)\n","  adding: data/val/rel/0137.rel (deflated 52%)\n","  adding: data/val/rel/0405.rel (deflated 34%)\n","  adding: data/val/rel/0157.rel (deflated 53%)\n","  adding: data/val/rel/0475.rel (deflated 73%)\n","  adding: data/val/rel/0030.rel (deflated 72%)\n","  adding: data/val/rel/0302.rel (deflated 68%)\n","  adding: data/val/rel/0017.rel (deflated 73%)\n","  adding: data/val/rel/0126.rel (deflated 76%)\n","  adding: data/val/rel/0053.rel (deflated 63%)\n","  adding: data/val/rel/0354.rel (deflated 71%)\n","  adding: data/val/rel/0018.rel (deflated 77%)\n","  adding: data/val/rel/0305.rel (stored 0%)\n","  adding: data/val/rel/0210.rel (deflated 51%)\n","  adding: data/val/rel/0385.rel (deflated 79%)\n","  adding: data/val/rel/0474.rel (deflated 77%)\n","  adding: data/val/rel/0174.rel (deflated 69%)\n","  adding: data/val/rel/0410.rel (deflated 76%)\n","  adding: data/val/rel/0325.rel (deflated 74%)\n","  adding: data/val/rel/0457.rel (deflated 73%)\n","  adding: data/val/rel/0234.rel (deflated 74%)\n","  adding: data/val/rel/0230.rel (stored 0%)\n","  adding: data/val/rel/0242.rel (deflated 37%)\n","  adding: data/val/rel/0082.rel (deflated 66%)\n","  adding: data/val/rel/0006.rel (deflated 77%)\n","  adding: data/val/rel/0222.rel (deflated 58%)\n","  adding: data/val/rel/0158.rel (deflated 65%)\n","  adding: data/val/rel/0185.rel (deflated 59%)\n","  adding: data/val/rel/0373.rel (deflated 68%)\n","  adding: data/val/rel/0162.rel (deflated 71%)\n","  adding: data/val/rel/0065.rel (deflated 73%)\n","  adding: data/val/rel/0045.rel (deflated 70%)\n","  adding: data/val/rel/0173.rel (deflated 64%)\n","  adding: data/val/rel/0390.rel (deflated 71%)\n","  adding: data/val/rel/0361.rel (deflated 69%)\n","  adding: data/val/rel/0022.rel (deflated 76%)\n","  adding: data/val/rel/0138.rel (deflated 71%)\n","  adding: data/val/rel/0394.rel (deflated 37%)\n"]}],"source":["!zip -r scibert-val-rel-3-sep.zip data/val/rel/"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"concepts_ner.ipynb","provenance":[]},"interpreter":{"hash":"e2a6cb9a7be66dff740cce83a34a934469b6132ecbf60107b2f98f6b92406a0e"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"0031c8a4147f4c5c85c425ca129a5c51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"01907e7b03da4dfdac85aa0324325c8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0618d990083e4e3eb2a7d74908128b31":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07bbc987a9b74bc584d6a57a65893a5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"125ddf0cef75420db6739a63d2f4d680":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e4d0aca5e5a465191369dc629dbe373","IPY_MODEL_aa416acfbe544e449ea909815de4b49f","IPY_MODEL_fdb76d6039dc4c69a86e966d2a058ab1"],"layout":"IPY_MODEL_e6510e86dfaa42f6886ba1400ca7ca1a"}},"12691f6dfd2a4ce8aeabe5cb9d93c8df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8cdfdd2de87468da1d381c6912e2b82","IPY_MODEL_a1efb9f259a34ec9be759135ef31fea2","IPY_MODEL_17898f9c65b54d3b9e2d7ab9210530f4"],"layout":"IPY_MODEL_f24e05a562014456b2e67f00fa6568bd"}},"17898f9c65b54d3b9e2d7ab9210530f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a33891d35724498a89d810da84749cff","placeholder":"​","style":"IPY_MODEL_ff6a44e5da994f1b9145fc69564f233f","value":" 5970/5970 [00:04&lt;00:00, 1493.50ex/s]"}},"18b75a441c5b4eb7911e4584101117a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74c397ef7ba44c0e8890373029d08bd5","placeholder":"​","style":"IPY_MODEL_ce790bd0d45d456f8c6fcd934985fea0","value":" 1/1 [00:00&lt;00:00, 25.61it/s]"}},"1da3cf5bd32e4d29a2b9f99dfe28216a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0618d990083e4e3eb2a7d74908128b31","placeholder":"​","style":"IPY_MODEL_761b526d690346f1bca69b8cfece1206","value":"100%"}},"1f93a0c9add84c7ebba08fae104b24b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"25dab51ddfd1471199bd6b32fbb6fa76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cac560d62e243dcb5e6df1498a2ef77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e4d0aca5e5a465191369dc629dbe373":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b03358ce39745fe9cae3cc2541f9296","placeholder":"​","style":"IPY_MODEL_c53842a95eab47d495f124932b11e45d","value":"Downloading: "}},"2f0879ec781e4c5bb3bcd9c542d759c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f60f22da9424936914b93bafaf1cb55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5de35062707d4d7eb09bf1884fa6c4d8","max":227845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1f93a0c9add84c7ebba08fae104b24b6","value":227845}},"312d061ff23b43f082436fe0fa977b37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"334126ec80dc48acaca342f4f5ad93cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"355f639079654aaa828992c022263d39":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36369643c8c149c7b91acceebfbc6be8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3906f4df4bf443cc8c3f3d40cb80be83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e364ad3e8146466ab156d33b3926f00c","max":1990,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1879aae6e0645b59a054afd3ce72184","value":1990}},"3ab2cf683dc349f7b56f9622c3ddd798":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3efe3b45a309437880e0a5ad9c58be68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_663f407f9f704aa1ae77f28664627b33","placeholder":"​","style":"IPY_MODEL_6f4ca626282a4fb99e36f0f891b6f179","value":" 1/1 [00:00&lt;00:00, 31.55it/s]"}},"4b03358ce39745fe9cae3cc2541f9296":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c3c8c3323ef4120b682052ab9befb10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d54f94b8d9443fd91d10bd0711b3cc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8a72fb4797c464a8a35d03f22710bb8","placeholder":"​","style":"IPY_MODEL_693fef2351804fe3891ea0b40c12c9d0","value":" 385/385 [00:00&lt;00:00, 11.5kB/s]"}},"50e5a452eb4b457f9002660afb3b34c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51908f695fc54c4287b99ad0255605ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51e017bb913047eca450f712bd3cfe66":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"544c7e05491a41bbaa8624b6fd75a415":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54af4e9de2944e038c72803865280256":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_961292b1eff14bf380638899def43491","placeholder":"​","style":"IPY_MODEL_8fe76e81d1b0457b882eb6faacce232b","value":" 223k/223k [00:00&lt;00:00, 5.57MB/s]"}},"55333aab86cb4d97989af438acc53a67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01907e7b03da4dfdac85aa0324325c8d","placeholder":"​","style":"IPY_MODEL_c14429bc51fb446bbfe06f13645dd9e6","value":"Downloading: 100%"}},"55b06b56923c403eb2e9253bd884ac73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ccdd3984ca54798807d10a6c0a66006":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5de35062707d4d7eb09bf1884fa6c4d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eb7f47673534d2a9e3fc2182e8af844":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51e017bb913047eca450f712bd3cfe66","placeholder":"​","style":"IPY_MODEL_a4d87ccb9ade497999496e6f2a7935f0","value":"Downloading: 100%"}},"663f407f9f704aa1ae77f28664627b33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"666a926bc63a425494364ae71be37139":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"693681e1cca447699d0b173687067ecf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"693fef2351804fe3891ea0b40c12c9d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ae8d63f39b147fe920f1a1c144f494b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d45f6829591c400883aae0b615846105","IPY_MODEL_d8c48cc0805c4238b4b203292230c47b","IPY_MODEL_3efe3b45a309437880e0a5ad9c58be68"],"layout":"IPY_MODEL_dfc62081f19842319c6e76d620968c58"}},"6b64b3abcc2f4310a1f5ee8e0e77ec2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a566ebf1274541659cb3d19c1b3c0a37","IPY_MODEL_2f60f22da9424936914b93bafaf1cb55","IPY_MODEL_54af4e9de2944e038c72803865280256"],"layout":"IPY_MODEL_55b06b56923c403eb2e9253bd884ac73"}},"6c43c98fef8144a8af856cc97e3645f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6dbe2689dbda41de9b0080d14d90e513":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88fb311edb584296a6a21b860ac82128","placeholder":"​","style":"IPY_MODEL_bf58e38cd92a41fcb75f6029d3968e8d","value":" 422M/422M [00:13&lt;00:00, 21.4MB/s]"}},"6f0591484ddb4454af6c7f42fa0a0279":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bec30dba6dac4986a00e4e6f7c91b394","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_334126ec80dc48acaca342f4f5ad93cc","value":1}},"6f0d92e849bc4afb8e64f84be533d343":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5eb7f47673534d2a9e3fc2182e8af844","IPY_MODEL_b3e3f68fd94c4721a7928a224e7ac33f","IPY_MODEL_6dbe2689dbda41de9b0080d14d90e513"],"layout":"IPY_MODEL_aa4694bc45484e3c9a88f1f5b9a56c5b"}},"6f4ca626282a4fb99e36f0f891b6f179":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f69233013fc49dba3750855c3f4d800":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"719a00e525dc49eead3e49b31880d692":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74c397ef7ba44c0e8890373029d08bd5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75cd97ebbba949108cd363baec7afcdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8b620c510c7483388b46339e894bc7a","IPY_MODEL_a3816c2408f0448890eece697fdbd24f","IPY_MODEL_b09ca72d7cc34d558c5f8855b0816f8f"],"layout":"IPY_MODEL_e9401f754dcd4e9fa7065397c0233f29"}},"761b526d690346f1bca69b8cfece1206":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fce5b3c31df493381489879e1b262e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85f75296cfd74088959934adf6fd4639":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88fb311edb584296a6a21b860ac82128":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e23a217b590464ea448c604c5451fc0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8fe76e81d1b0457b882eb6faacce232b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"929de88e159b43dda7c1f68fa4fd0a4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1da3cf5bd32e4d29a2b9f99dfe28216a","IPY_MODEL_6f0591484ddb4454af6c7f42fa0a0279","IPY_MODEL_18b75a441c5b4eb7911e4584101117a9"],"layout":"IPY_MODEL_85f75296cfd74088959934adf6fd4639"}},"961292b1eff14bf380638899def43491":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1efb9f259a34ec9be759135ef31fea2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_693681e1cca447699d0b173687067ecf","max":5970,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fbe3059b937544b793dde2008418cce6","value":5970}},"a33891d35724498a89d810da84749cff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3816c2408f0448890eece697fdbd24f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_355f639079654aaa828992c022263d39","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50e5a452eb4b457f9002660afb3b34c1","value":1}},"a4d87ccb9ade497999496e6f2a7935f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a566ebf1274541659cb3d19c1b3c0a37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51908f695fc54c4287b99ad0255605ce","placeholder":"​","style":"IPY_MODEL_e568daf438fe4acfaed352d9c88448b2","value":"Downloading: 100%"}},"a8a72fb4797c464a8a35d03f22710bb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8cdfdd2de87468da1d381c6912e2b82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_719a00e525dc49eead3e49b31880d692","placeholder":"​","style":"IPY_MODEL_c268420d42214c3c80309f13cf42c1cb","value":"100%"}},"aa416acfbe544e449ea909815de4b49f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d91d65fe4bcb4d12952dd11e0ff0347b","max":2482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0031c8a4147f4c5c85c425ca129a5c51","value":2482}},"aa4694bc45484e3c9a88f1f5b9a56c5b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aee2836685554158b61b6167dbff7a1f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f76c37ef6ba149cba11176bb460b6a5e","placeholder":"​","style":"IPY_MODEL_8e23a217b590464ea448c604c5451fc0","value":"100%"}},"b09ca72d7cc34d558c5f8855b0816f8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c3c8c3323ef4120b682052ab9befb10","placeholder":"​","style":"IPY_MODEL_666a926bc63a425494364ae71be37139","value":" 1/1 [00:00&lt;00:00, 14.00it/s]"}},"b3e3f68fd94c4721a7928a224e7ac33f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fce5b3c31df493381489879e1b262e9","max":442221694,"min":0,"orientation":"horizontal","style":"IPY_MODEL_312d061ff23b43f082436fe0fa977b37","value":442221694}},"b8b620c510c7483388b46339e894bc7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_544c7e05491a41bbaa8624b6fd75a415","placeholder":"​","style":"IPY_MODEL_2f0879ec781e4c5bb3bcd9c542d759c6","value":"100%"}},"bec30dba6dac4986a00e4e6f7c91b394":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf58e38cd92a41fcb75f6029d3968e8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c14429bc51fb446bbfe06f13645dd9e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1879aae6e0645b59a054afd3ce72184":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c268420d42214c3c80309f13cf42c1cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c53842a95eab47d495f124932b11e45d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca682a887b394e4293ccbf8654e12eda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aee2836685554158b61b6167dbff7a1f","IPY_MODEL_3906f4df4bf443cc8c3f3d40cb80be83","IPY_MODEL_dbda26dcf5b24e2eb3699ca0d6a86d2e"],"layout":"IPY_MODEL_2cac560d62e243dcb5e6df1498a2ef77"}},"ce790bd0d45d456f8c6fcd934985fea0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d45f6829591c400883aae0b615846105":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f08d3be1fa7b41ecb1f0fdd7ca4c3080","placeholder":"​","style":"IPY_MODEL_36369643c8c149c7b91acceebfbc6be8","value":"100%"}},"d89f9874a50a41db87a1a77a89f7d41d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8c48cc0805c4238b4b203292230c47b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d89f9874a50a41db87a1a77a89f7d41d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c43c98fef8144a8af856cc97e3645f2","value":1}},"d91d65fe4bcb4d12952dd11e0ff0347b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbda26dcf5b24e2eb3699ca0d6a86d2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07bbc987a9b74bc584d6a57a65893a5a","placeholder":"​","style":"IPY_MODEL_5ccdd3984ca54798807d10a6c0a66006","value":" 1990/1990 [00:01&lt;00:00, 1500.29ex/s]"}},"dfc62081f19842319c6e76d620968c58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e364ad3e8146466ab156d33b3926f00c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e568daf438fe4acfaed352d9c88448b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6510e86dfaa42f6886ba1400ca7ca1a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e74e3069dad646898887ab6ef29a153d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7d69a97aa9e46998bbb521d77d62f22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55333aab86cb4d97989af438acc53a67","IPY_MODEL_f2cb3d3a4b30461bb3b99bc1372a45bb","IPY_MODEL_4d54f94b8d9443fd91d10bd0711b3cc3"],"layout":"IPY_MODEL_e74e3069dad646898887ab6ef29a153d"}},"e9401f754dcd4e9fa7065397c0233f29":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea845d9491834a3a9b1ab4d0b4030fb1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f08d3be1fa7b41ecb1f0fdd7ca4c3080":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f24e05a562014456b2e67f00fa6568bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2cb3d3a4b30461bb3b99bc1372a45bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea845d9491834a3a9b1ab4d0b4030fb1","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f69233013fc49dba3750855c3f4d800","value":385}},"f76c37ef6ba149cba11176bb460b6a5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbe3059b937544b793dde2008418cce6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fdb76d6039dc4c69a86e966d2a058ab1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ab2cf683dc349f7b56f9622c3ddd798","placeholder":"​","style":"IPY_MODEL_25dab51ddfd1471199bd6b32fbb6fa76","value":" 6.34k/? [00:00&lt;00:00, 157kB/s]"}},"ff6a44e5da994f1b9145fc69564f233f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
